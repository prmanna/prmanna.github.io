[{"content":"My name is Prasenjit Manna. I am a Sr. Technical Leader of Service Provider Optical Team at Cisco Systems, Bangalore. My area of work is Routing, Optical, Manageability, Embedded Systems-Networking.\nThis is my blog. Self hosted, no sponsors, purely personal. I am speaking on this site solely for myself and not for any employers, past or present. I pay all costs, there are no ads, sponsors or paid content. Any conflicts will be disclosed in the post.\n","href":"/","title":"Home"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/blogs/","title":"Blogs"},{"content":"","href":"/blogs/","title":"Blogs"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/categories/forwarding-engine/","title":"Forwarding-Engine"},{"content":"","href":"/categories/primer/","title":"Primer"},{"content":"","href":"/authors/prmanna/","title":"Prmanna"},{"content":"Router Forwarding Engines Series by Russ White\nA Close Look at Router Forwarding Engines: Part 1 ‚Äì What are the Problems? Router Forwarding Engines: Part 2 ‚Äì OpenFlow and I2RS as Possible Solutions Router Forwarding Engines: Part 3 ‚Äì P4 and SAI Join my email list to get direct access to new blog posts.\n","href":"/blogs/2025-02-05-router-forwarding-engines-series-by-russ-white/","title":"Router Forwarding Engines Series by Russ White"},{"content":"A Primer on Communication Fundamentals Join my email list to get direct access to new blog posts.\n","href":"/blogs/2025-01-30-primers-on-optics-communication/","title":"A Primer on Communication Fundamentals"},{"content":"","href":"/categories/cloud/","title":"Cloud"},{"content":"","href":"/categories/data-center/","title":"Data-Center"},{"content":"","href":"/categories/audit/","title":"Audit"},{"content":"","href":"/categories/linux/","title":"Linux"},{"content":"Excellent Article for Linux auditd for Threat Detection by Izy\nLinux auditd for Threat Detection [Part 1] Linux auditd for Threat Detection [Part 2] Linux auditd for Threat Detection [Final] Join my email list to get direct access to new blog posts.\n","href":"/blogs/2024-12-08-linux-auditd-usecase/","title":"Linux Audit Daemon Guide"},{"content":"","href":"/tags/ai/","title":"AI"},{"content":"","href":"/tags/data-center/","title":"Data Center"},{"content":"","href":"/tags/ietf/","title":"Ietf"},{"content":"IETF - Adaptive/Perceptive Routing IETF 121 Side Meeting Join my email list to get direct access to new blog posts.\n","href":"/writing/2024-11-11-ietf-adaptive-perceptive-routing/","title":"IETF - Adaptive/Perceptive Routing"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/writing/","title":"Writings"},{"content":"","href":"/categories/podcast/","title":"Podcast"},{"content":"Podcast - Scaling Data Center Networks - Dmitry Afanasiev Summary: üéôÔ∏è Introduction to Scaling in Data Centers: Hosts introduce the topic of scaling challenges in data centers, emphasizing this as a series of topics to be discussed over multiple episodes. üíª Guest Speaker - Dmitry Afanasiev: Dmitry, a network architect from Yandex, explains the challenges Yandex faces in scaling their networks, which are similar to hyperscalers like Google or Amazon. üèóÔ∏è Challenges in Scaling: The discussion covers issues such as failure domain limitation, MPLS in data centers, and managing large data center networks with advanced technologies like RIFT. üì° Networking Technologies: Dmitry explains technologies like MPLS with label distribution via ARP and delves into Yandex‚Äôs use of RIFT (Routing In Fat Trees) to improve scaling. üîÑ Scaling Strategies: Different network architectures are discussed, such as leaf-spine topologies and the use of high-radix switches, which improve scalability without adding significant levels of complexity. üß† In-Network Compute: The potential of in-network compute, especially in machine learning, where operations like collective communication can be offloaded to the network devices, improving performance. üìä Large Data Centers \u0026amp; Metrics: Dmitry mentions Yandex\u0026rsquo;s goal of scaling to 100k endpoints in a single data center, discussing metrics like tail latency, failure tolerance, and efficient resource utilization. üîå Energy and Power Constraints: Data centers face power limitations as they grow, with Dmitry noting that Yandex‚Äôs large data centers are capped at around 50 megawatts. üöÄ Future of Data Center Networking: Discussion of advanced network topologies beyond traditional Clos networks, with interest in technologies like Dragonfly and global adaptive routing to improve performance. Insights Based on Numbers: 100k endpoints: Yandex is aiming to scale its data centers to 100,000 endpoints, a huge challenge in terms of both network design and power consumption. 50 megawatts: Power constraints limit large data centers to approximately 50 MW per facility, leading to the need for multiple distributed campuses. Latency improvements: In-network compute could significantly improve latency by reducing data transfers within the network infrastructure. Yandex\u0026rsquo;s approach to scaling data centers, in comparison to other hyperscalers like Google or Amazon, reveals some key similarities and differences: Similar Challenges: Like Google and Amazon, Yandex faces typical hyperscaler challenges such as managing massive networks, failure domains, and optimizing resource utilization. These include addressing network congestion, scaling limitations, and the complexity of distributed applications running across thousands of servers.\nUnique Innovations: Yandex, despite being smaller in scale compared to Google or Amazon, has been highly innovative. One example is their early adoption of MPLS (Multi-Protocol Label Switching) in the data center, which is not commonly used by all hyperscalers. They also experimented with distributing labels using ARP (Address Resolution Protocol), showcasing their ability to push forward network design for scaling.\nScaling Constraints: While Google and Amazon have larger global footprints, Yandex focuses on similar-sized scaling, with a target of 100k endpoints per data center, limited by energy consumption to around 50 MW. However, Yandex is expanding beyond Russia into markets in the U.S. and Europe, potentially closing the gap in terms of global presence.\nThus, Yandex aligns with other hyperscalers in terms of scaling strategies but differentiates itself with some innovative networking solutions specific to its operational needs.\nUsing high-radix switches in modern data centers emphasizes both their benefits and challenges: Benefits: Improved Scalability: High-radix switches, which have more ports per switch, allow for larger network topologies without needing to add additional layers of switches. This enables data centers to scale more efficiently, supporting more endpoints with fewer devices.\nReduced Complexity: By using fewer network levels, high-radix switches simplify the network architecture. This reduces the complexity associated with managing additional spine and leaf layers, making the overall design more efficient and easier to maintain.\nBetter Load Distribution: With more ports, high-radix switches allow for better distribution of traffic across the network. This helps prevent congestion and ensures that data moves smoothly through the system, improving network performance.\nChallenges: Cost and Power: High-radix switches are more expensive and consume more power due to the larger number of ports and the need for advanced hardware. In large-scale data centers, this can become a significant cost factor, especially when multiplied across thousands of switches.\nComplexity in Cabling: Managing the physical cabling of high-radix switches is challenging, especially in dense environments. The increased number of ports requires careful planning to avoid messy or inefficient cabling setups.\nLatency Concerns: While high-radix switches reduce the need for multiple layers in the network, they may introduce higher latency if not managed properly, particularly when dealing with longer paths or larger distances within the data center.\nOverall, high-radix switches provide significant advantages in terms of scalability and efficiency, but they also introduce challenges related to cost, power, and physical complexity.\nabout in-network compute and its impact on machine learning workloads in data centers focuses on the following points: Reduced Latency: In-network compute allows some parts of collective operations (e.g., reductions and aggregations) to be handled directly within the network hardware. This reduces the amount of data that needs to be moved between servers, cutting down on latency and improving the overall speed of machine learning tasks.\nOptimized Bandwidth Use: By performing computations like collective communication in-network, the need for back-and-forth data transfers between nodes is minimized. This leads to more efficient use of network bandwidth, especially in data-intensive operations like training AI models, where large amounts of data are typically exchanged.\nImproved Performance: Offloading tasks to network devices allows compute resources (like GPUs) to focus on core machine learning operations, while the network handles supporting tasks. This separation of concerns leads to better performance in large-scale machine learning tasks, as both the compute and network layers are optimized for their respective roles.\nIn-network compute is particularly beneficial for distributed machine learning environments, where reducing traffic and improving coordination between GPUs is crucial for scaling workloads efficiently.\nPodcast - Scaling Data Center Networks - Part 2-Dmitry Afanasiev Summary: üåç Continuation on Data Center Scaling: The session picks up where the first discussion on scaling left off, discussing the topologies and designs used in hyperscalers like Yandex, including Clos or folded-Clos topologies with three stages. üîÑ Use of Multiple Planes: Yandex uses eight planes for traffic distribution, where each plane is independent of the others, ensuring better fault tolerance and traffic management. üì° Cabling Infrastructure: Dmitry elaborates on the physical constraints of data centers, describing the need for optimized cable management to deal with the long distances between equipment. Different segments use a mix of single-mode optics, multi-mode optics, and twin-ax copper cables depending on distance. üõ†Ô∏è Challenges of Future Optics: As data speeds increase to 800Gb, multi-mode optics will become less effective, forcing a move to single-mode optics for longer links. Additionally, within racks, middle-of-the-rack switches will optimize cable lengths and efficiency. üèóÔ∏è Managing Complexity: The growing complexity of cable and switch management requires careful planning and hierarchical design to keep bundles of cables consistent and reduce physical complexity. üîß Cross-Connects and Space Utilization: Dmitry explains how optical cross-connects are used to manage cable infrastructure within data centers, providing a visual description of how physically large these setups can be. üî• Power and Cooling Requirements: Cooling and power density requirements for network devices are growing, with switches requiring separate environments from the compute infrastructure to manage airflow and heat more effectively. üîÑ Routing and Advertisement: Yandex uses a simple, prefix-based routing scheme, avoiding aggregation where possible to reduce routing complexity. IPv6 is used to allocate large, stable prefixes within the data center to minimize the need for dynamic management. üßë‚Äçüíª Weighted ECMP: Weighted Equal-Cost Multi-Path (ECMP) routing is discussed as an approach to improving traffic distribution. However, due to the limited vendor support at the time, Yandex decided against using it. Insights Based on Numbers: 8 independent planes: Yandex\u0026rsquo;s data center design utilizes eight separate planes for traffic distribution, improving redundancy and fault isolation. Up to 100 meters between switch segments: Physical distance in data centers is a key challenge, with single-mode optics becoming more common as distances exceed what multi-mode optics can handle. 16,000 prefixes: Yandex manages up to 16,000 prefixes in its routing tables, relying on IPv6 to allocate large address blocks and reduce routing overhead. Yandex\u0026rsquo;s management of cable infrastructure in large data centers highlights several key strategies for optimizing space and reducing complexity: Segmented Cabling: Yandex divides its cabling into three segments:\nThe first segment connects from the host to the top-of-rack switches. The second segment runs between the top-of-rack switches and the spine switches. The third segment links spine switches to higher layers in the topology, such as super-spines. These segments help organize the physical layout and prevent cabling issues that can arise due to long distances between components.\nCable Types: Different cable technologies are used depending on the distance:\nTwin-axial copper cables are used for short distances within racks. Multi-mode optical fibers are employed for medium distances, but as speeds increase (e.g., 800Gb), multi-mode optics are becoming less effective. Single-mode optics are used for long distances, ensuring high data transfer speeds across hundreds of meters in the data center. Hierarchical Bundling: To manage the complexity of having thousands of cables, Yandex groups cables into bundles that are easier to manage and route. These bundles connect similar endpoints and travel consistent paths, which simplifies both the physical installation and future upgrades or repairs.\nOptical Cross-Connects: To further reduce complexity, Yandex uses optical cross-connects, which act as an intermediary to manage cable bundles. This allows for better organization of the physical infrastructure and simplifies the connection between different network layers.\nIn sum, Yandex tackles the challenges of large data center infrastructure with careful planning of cabling segments, varied cable types, and hierarchical designs that reduce physical complexity and improve manageability.\nabout the challenges hyperscalers face in cooling and powering their network infrastructure as they scale reveals several key points: High-Density Power Consumption: As data centers scale, especially for network infrastructure, the power requirements become substantial. Hyperscalers like Yandex deal with high-power densities in switches and other network equipment, which can consume a large portion of the available power within a data center. This is particularly challenging because switches are often located separately from compute resources, and need their own dedicated power feeds.\nCooling Requirements: Network equipment, especially high-radix switches, generates significant heat due to the high density of transceivers and cables, blocking airflow through the devices. Unlike compute servers, switches don‚Äôt have as much open space for airflow, making effective cooling harder. Yandex uses separate rooms with enhanced cooling systems to manage this heat, isolating network equipment from other data center components to ensure adequate airflow and temperature control.\nDistributed Placement for Redundancy: To mitigate risks and ensure network reliability, Yandex places critical network equipment like super-spine switches in different locations across the data center. This separation ensures that if one power zone or cooling system fails, the other sections remain operational, improving overall network resilience.\nFree-Space Cooling: Yandex also implements free-space cooling for their servers, allowing temperatures in the data center‚Äôs cold corridor to rise as high as 35¬∞C (95¬∞F). However, the network devices require better cooling environments than what is provided for standard servers, further complicating infrastructure scaling.\nThe combination of increasing power needs, the challenge of effective cooling, and the need for physical separation to prevent cascading failures highlights the significant obstacles hyperscalers face in scaling their data center network infrastructure.\nwhy Yandex chose not to implement Weighted Equal-Cost Multi-Path (ECMP) routing highlights both the challenges and potential benefits of using this traffic distribution method: Reasons for Not Implementing Weighted ECMP: Vendor Support Limitations: At the time Yandex considered using weighted ECMP, vendor support for the feature was limited and inconsistent. Yandex‚Äôs network environment involved multiple vendors, and implementing weighted ECMP across different vendors\u0026rsquo; hardware presented technical challenges. The immature support made the feature unreliable for large-scale deployment.\nIncreased Complexity: Weighted ECMP introduces additional complexity in network routing. It requires more advanced configurations and adds a layer of traffic management that can be difficult to maintain in large, multi-vendor environments. Yandex preferred to keep their routing system simpler, avoiding unnecessary complexity.\nCoarse Granularity: Implementing weighted ECMP with available chipsets at the time resulted in coarse granularity for traffic distribution. This meant that the distribution of traffic between paths was not fine-tuned enough to provide meaningful improvements. For example, balancing traffic across eight uplinks did not offer sufficient flexibility for certain network loads, limiting the effectiveness of weighted ECMP.\nPotential Benefits of Weighted ECMP: Optimized Traffic Balancing: When properly supported and implemented, weighted ECMP allows more intelligent traffic distribution based on link capacity. This helps to avoid overloading specific links while underutilizing others, improving overall network efficiency.\nReduced Congestion: By distributing traffic based on weights, network congestion can be mitigated, especially in scenarios where certain paths have more bandwidth or lower latency than others.\nScalability: For large data centers, weighted ECMP offers the potential for more scalable traffic management by dynamically adjusting traffic flows as network conditions change.\nIn conclusion, while weighted ECMP offers clear benefits for traffic distribution, Yandex avoided implementing it due to immature vendor support, added complexity, and the coarse granularity of available implementations at the time.\nPodcast - Scaling Data Center Networks - Part 3-Dmitry Afanisiev Summary: üéôÔ∏è Introduction to Networking Discussion: Jeff Doyle and co-host Jeff Tancera introduce the topic of scaling data centers, focusing on networking technologies like ECMP and MPLS. They discuss how these technologies are evolving to handle large-scale data centers. üì° MPLS in Data Centers: The team discusses the IETF draft for Labeled ARP (LARP), which allows hosts to receive both an IP address and an MPLS label upon boot. This improves efficiency by enabling immediate traffic encapsulation in MPLS. üèóÔ∏è Scaling and ECMP: The video examines how ECMP (Equal-Cost Multi-Path Routing) works well in data centers unless flows exceed 40% of upstream bandwidth. AI and machine learning workloads, which have long-lasting flows (e.g., 100Gb or 400Gb), pose unique challenges. üîå EVPN and Control Planes: For enterprise and hyperscale environments, different control planes are used. In hyperscalers, EVPN (Ethernet VPN) isn\u0026rsquo;t widely deployed, whereas the enterprise relies on EVPN over VXLAN for network overlays. ‚ö° Traffic Engineering in AI Workloads: As data center workloads evolve, particularly in AI, traffic engineering becomes more complex. Hyperscalers face challenges as flows grow longer and require more sophisticated routing. üåê Routing and Convergence: Dmitry explains how BGP convergence in multi-plane architectures can lead to temporary routing inefficiencies, stressing the importance of large buffer spaces and careful traffic balancing to avoid congestion. üîÑ Disaggregation and Aggregation Challenges: The team discusses challenges with disaggregation, particularly in handling multi-homing and managing black holes in large, multi-path networks. They explore the difficulty of aggregating routes when failures occur. Insights Based on Numbers: 50% chance of wrong path: If MPLS routing relies on default routes during failure, there\u0026rsquo;s a 50% chance of picking the wrong path due to topology asymmetry. 1-2 millisecond RTT: For data centers interconnecting at 100 kilometers, round-trip time can range from 1 to 2 milliseconds. 100,000+ route scale: Modern BGP routing in data centers supports more than 100,000 routes, allowing extensive scaling of network fabrics. about MPLS (Multi-Protocol Label Switching) improving traffic encapsulation in large-scale data centers emphasizes several key benefits: Immediate Label Assignment: Through the Labeled ARP (LARP) draft discussed at the IETF, MPLS allows a host to receive both an IP address and an MPLS label during boot-up. This enables the host to immediately start encapsulating its traffic in MPLS, eliminating delays and increasing efficiency. It ensures that traffic is routed through MPLS from the start, without needing additional setup steps.\nEfficient Traffic Engineering: MPLS supports precise traffic engineering, which is essential in large-scale data centers where traffic flows can be complex and varied. By using MPLS labels, data centers can direct traffic along pre-defined paths, optimizing bandwidth and minimizing congestion.\nScalability and Network Segmentation: MPLS allows data centers to scale their networks more effectively. By encapsulating traffic with labels, MPLS facilitates the creation of logical network segments that can be managed independently. This improves overall network organization and makes it easier to scale without causing routing or traffic issues.\nIn summary, MPLS significantly improves how large data centers manage and route traffic, providing better scalability, efficiency, and flexibility for handling complex workloads.\nabout the challenges of ECMP (Equal-Cost Multi-Path Routing) in handling AI and machine learning traffic highlights a few critical issues: Long-Lasting Flows: In AI and machine learning workloads, data flows are not typical short bursts but rather long-lasting, high-bandwidth flows. For instance, during inference or training tasks, flows can utilize 100Gb or even 400Gb of bandwidth. ECMP, which works well in traditional traffic scenarios, struggles with these longer, heavier flows, as they can overwhelm specific paths, leading to congestion.\nFlow Size Disparities: AI workloads often generate disproportionate flows. ECMP is designed to balance traffic evenly across multiple paths. However, when a few flows consume a significant portion of the available bandwidth, ECMP may not distribute traffic effectively, resulting in underutilization of some paths while others become overloaded.\nLimited Path Diversity: In ECMP, multiple paths must be available and have the same cost to the destination. However, in AI data centers, these long flows can monopolize specific paths, reducing path diversity. As AI workloads grow, it becomes harder to find equal-cost paths, which limits ECMP\u0026rsquo;s effectiveness in large-scale environments.\nThese challenges make ECMP less effective for AI and machine learning workloads, pushing data centers to explore alternative routing methods to handle the unique demands of these high-throughput, long-duration traffic patterns.\nhow disaggregation affects network resilience and routing in data centers highlights several complexities: Black Holes and Route Failure: When a network is disaggregated, meaning individual routes or paths are handled independently, failures in one part of the network can create routing black holes. This happens when aggregated routes fail to propagate correctly, leading to destinations that become unreachable. This is a major concern when dealing with multi-path architectures, especially in large-scale data centers.\nSymmetry in Aggregated Routes: Disaggregated networks rely on route aggregation to simplify routing tables and reduce overhead. However, if the network loses its symmetry (e.g., through hardware failure or path inconsistencies), these aggregated routes may no longer be valid, forcing routers to rely on disaggregated paths that are less efficient or harder to manage.\nIncreased Routing Complexity: While disaggregation provides flexibility and allows for more granular control over network traffic, it also increases routing complexity. Disaggregating paths means routers must handle more specific routes, which can put extra strain on the network‚Äôs routing tables and make it harder to quickly recover from failures.\nOverall, disaggregation can enhance flexibility but can introduce challenges in maintaining network resilience and routing efficiency, particularly when failures or asymmetric paths are involved.\nPodcast - Scaling Data Center Networks - Part 4-Dmitry Afanisiev Summary: üìà Scaling Data Center Networks: The discussion opens with an overview of scaling issues in data center networks, focusing on advanced, non-traditional topologies and network models that could improve efficiency. üöÄ Machine Learning Workloads: Machine learning clusters are introduced as a critical area requiring new network designs, as their large, synchronized data flows demand intelligent routing and congestion control. üåê Advanced Network Topologies: Non-shortest path routing, Dragonfly Plus, and Slim Fly topologies are explored for their potential to reduce network diameter and improve efficiency for large-scale networks. üìä Data Synchronization Challenges: The importance of synchronized data transfers between GPUs in machine learning models, highlighting issues such as flow size, packet reordering, and buffer constraints. üñ•Ô∏è In-Network Compute: A key concept where data aggregation and processing occur within the network itself, reducing load on end devices and improving overall performance in machine learning tasks. Insights Based on Numbers: üí° 100 gig per flow: This statistic showcases the magnitude of data being transferred in machine learning workloads, emphasizing the need for high-capacity, low-latency networks. üí° 50 tera switches: The emergence of high-performance switches capable of handling massive data volumes (50 terabits per second) marks a significant milestone in data center evolution. üí° 800 gig interfaces: Future network advancements will include 800-gigabit interfaces, drastically increasing throughput and reducing bottlenecks in data-intensive environments. What the video says about how advanced topologies like Dragonfly Plus improve network efficiency: Dragonfly Plus is highlighted as a scalable network topology that improves efficiency by reducing the overall network diameter. In traditional Clos networks, the diameter may require multiple hops to transmit data, increasing latency. However, Dragonfly Plus reduces this diameter, meaning data travels fewer hops, speeding up communication.\nBy allowing communication within fewer network layers, Dragonfly Plus enhances performance, particularly in high-performance computing (HPC) and machine learning clusters where large volumes of synchronized data transfers occur. This improvement reduces the time spent in data synchronization between nodes, essential for large machine learning tasks requiring heavy parallelism.\nDragonfly Plus also allows the use of fewer resources (nodes and links) while maintaining large-scale network sizes, making it both cost-effective and efficient for complex network environments.\nabout the main challenges in achieving synchronized data transfers between GPUs in machine learning clusters: One of the key challenges in synchronized data transfers between GPUs is packet reordering. In machine learning tasks, particularly during model training, the data flows between GPUs are large and highly synchronized. Even slight packet reordering can trigger significant slowdowns, as GPUs rely on receiving data in a precise sequence to proceed with computations.\nAnother challenge is high sensitivity to delays. Since different GPUs must wait for others to complete their tasks before proceeding, any delay from one node can stall the entire process, leading to inefficiencies. This issue is compounded by the fact that machine learning workloads involve many interconnected processes, where a delay in any part of the network halts the whole system.\nFinally, network saturation and buffering issues arise due to the high volume of data being transferred. GPUs in modern clusters can handle data at rates of 100 gigabits per second or more, easily saturating the available network capacity and creating bottlenecks that slow down overall computation. Efficient congestion control and adaptive routing mechanisms are critical to managing this load effectively.\nabout how in-network compute can reduce the load on end devices in large-scale networks: In-network compute refers to the processing of data directly within the network devices, such as switches, instead of relying solely on end devices like servers or GPUs. This reduces the load on those end devices by offloading tasks such as data aggregation and combination to the network itself.\nFor machine learning tasks, where large volumes of data need to be exchanged and synchronized between multiple nodes, in-network compute helps by performing intermediate computations during data transfers. For example, instead of sending raw data back and forth between nodes for combination, the network switch can aggregate data in real-time. This minimizes the number of hops and reduces the time spent waiting for data synchronization, ultimately speeding up the process.\nMoreover, this approach decreases the overall demand on node-level memory and processing power, since network devices handle some of the workload. This can lead to faster and more efficient operations, particularly in high-performance computing (HPC) environments where computational intensity is high.\n","href":"/blogs/2024-09-26-podcast-summary-scaling-data-center-networks/","title":"Podcast Summary - Scaling Data Center Networks | Between 0x2 Nerds"},{"content":"Podcast - AI/ML Data Center Design - Part 1 Summary: üéØ AI Data Centers Fundamentals: Focused on AI data center design and the critical role of NVIDIA and GPUs. Discusses how the evolution of AI and ML workflows demands specialized infrastructure. üöÄ Growth in GPU-Based Networks: The shift from CPU to GPU for AI/ML tasks due to their high parallel computing capacity. Increasing use of NVIDIA GPUs across data centers. üìä Massive Scaling Requirements: AI clusters are rapidly scaling up. Meta‚Äôs Lama 2 model, for instance, uses thousands of GPUs, leading to complex networking challenges. üñ•Ô∏è Efficiency and Parallelism: NVIDIA\u0026rsquo;s approach to networking, including data parallelism, to improve model training efficiency. üîó GPU-Direct RDMA (Remote Direct Memory Access): Essential for efficient data transfers, bypassing CPUs to optimize performance in AI clusters. Insights Based on Numbers: Meta\u0026rsquo;s Lama 2: Trained using 2,000 GPUs, requiring close to a million hours of processing time. This scale indicates the intensive computational power needed for modern AI models. GPU Growth: From 4K GPU clusters just two years ago to clusters with tens of thousands of GPUs today. In the future, clusters with half a million GPUs will be commonplace. AI data center design and training time for large models like Llama 2? The video explains that AI data center design plays a pivotal role in determining the training time for large models such as Meta‚Äôs Llama 2. As models grow in size, the requirements for computational power and network efficiency rise exponentially. Llama 2, for instance, uses 2,000 GPUs and requires nearly a million GPU hours to train. The design of AI data centers is tailored to support this intense computational demand by optimizing for GPU clusters, which provide the parallelism necessary for handling large datasets and running extensive computations.\nIncreased GPU density within data centers helps reduce training times, as GPUs are specifically designed to handle the parallel processing needed for machine learning and AI tasks. AI models like Llama 2 often involve complex data parallelism techniques, where datasets are distributed across multiple GPUs. Furthermore, network latency and bandwidth directly influence how fast data can be processed and shared between GPUs, impacting training speed. The architecture must ensure high bandwidth, low-latency connections (e.g., using NVIDIA\u0026rsquo;s NVLink) to handle the heavy data exchange between GPUs efficiently.\nThe video also highlights that as AI models evolve, so does the demand for scalable hardware and improved network infrastructure. Newer generations of GPUs (like the H100) and innovative network designs help cut training times by offering faster computation and data sharing capabilities.\nWhy NVIDIA GPUs are more efficient for AI tasks compared to CPUs? The video emphasizes that NVIDIA GPUs are significantly more efficient for AI tasks compared to traditional CPUs due to their architecture and specialized design for parallel computing. Here\u0026rsquo;s why:\nParallel Processing Power: GPUs, especially those from NVIDIA, are designed with a large number of smaller cores, allowing them to execute many tasks simultaneously. This is ideal for AI tasks such as training machine learning models, which involve running massive computations in parallel. CPUs, on the other hand, have fewer cores optimized for sequential processing, making them less effective for tasks requiring large-scale parallelism.\nHandling Large Datasets: AI models often require processing enormous datasets, and NVIDIA GPUs excel at this by using parallel data processing techniques. In contrast, CPUs struggle with handling such volumes efficiently. NVIDIA GPUs can quickly train models by distributing workloads across their many cores, speeding up processes like matrix multiplication and neural network calculations.\nGPU-Specific Libraries: NVIDIA provides tools like CUDA (Compute Unified Device Architecture), which simplifies the programming of AI applications on GPUs. These libraries help optimize the performance of AI models on GPUs by allowing researchers to fully utilize the hardware. CPUs lack such specialized libraries for AI, further widening the efficiency gap.\nGPU Direct RDMA: The video mentions GPU Direct RDMA (Remote Direct Memory Access), a technology that allows GPUs to communicate directly with network adapters without the involvement of the CPU. This bypasses the CPU, reducing bottlenecks in data transfer and enhancing performance, especially in large-scale AI data centers.\nOverall, NVIDIA GPUs outperform CPUs in AI tasks due to their parallel processing capabilities, optimized software, and ability to efficiently handle large-scale machine learning workloads.\nHow network latency affects AI inference and why it is critical in data center design? The video highlights that network latency plays a crucial role in the efficiency of AI inference within data center design. Inference, the process of running trained AI models to generate predictions or outputs, requires a highly optimized network. Here\u0026rsquo;s why it is critical:\nReal-Time Responses: Inference tasks often interact with humans or applications that require real-time responses. Latency delays between processing steps can degrade the user experience, especially for applications like chatbots, autonomous systems, or recommendation engines. In AI data centers, low-latency connections are essential to provide quick responses, sometimes within milliseconds.\nMulti-GPU Collaboration: As AI models scale, inference tasks are distributed across multiple GPUs. The communication between these GPUs needs to be as fast as possible to avoid bottlenecks. Any delay in data exchange between GPUs due to network latency can drastically slow down the inference process, even if the GPUs themselves are processing data efficiently.\nMachine-to-Machine Inference: The video explains that future AI infrastructure will increasingly involve machine-to-machine inference, where multiple applications or AI models interact without human intervention. In such systems, the expectation for instantaneous data transfer becomes even more important. Latency constraints in this environment would lead to slower automation processes and inefficiencies.\nComplex AI Workloads: Many AI inference tasks are complex, involving multiple stages of data processing. Each stage requires fast and seamless data transfer between GPUs and network components. Latency impacts how quickly these stages can be completed, and in some cases, a small delay in one part of the network can slow down the entire inference pipeline.\nIn short, minimizing network latency is essential in AI data centers because it ensures fast, real-time inference responses, improves multi-GPU collaboration, and supports the future demands of machine-to-machine operations.\nPodcast - AI/ML Data Center Design - Part 2 Summary: Summary: üéØ Networking Challenges in AI Workflows: The episode delves into the complexities of networking in AI, particularly focusing on routing and congestion control in data centers that support AI/ML workloads. üñ•Ô∏è Routing and Load Balancing: Discussion about how routing, especially through protocols like BGP, is crucial for managing congestion and ensuring traffic load balancing in AI data centers. üöÄ Scaling AI Networks: AI infrastructure has rapidly scaled from 1K GPU clusters to 100K GPUs, and the importance of network flexibility and the ability to scale without frequent hardware replacements is emphasized. üîÑ Congestion Control: A focus on how congestion control is handled through various mechanisms, including ECN (Explicit Congestion Notification) and QCN (Quantized Congestion Notification), which are critical to optimizing transmission rates and avoiding bottlenecks. üì∂ Best Practices in Data Center Design: Stressed the need for best practices when designing hyperscale AI data centers to avoid packet loss, improve job completion times, and reduce the high costs of mistakes. Insights Based on Numbers: Scale of Growth: From 1,000 GPU clusters just a few years ago, data centers are now handling up to 100,000 GPUs, a clear indication of how exponentially AI infrastructure is evolving. Congestion Delay: The video explains how standard congestion control mechanisms, like QCN, can introduce round-trip delays of around 10 microseconds, showing the sensitivity of AI networks to even minimal delays. How BGP assists in load balancing traffic in large-scale AI data centers? The video emphasizes that BGP (Border Gateway Protocol) plays a significant role in load balancing within large-scale AI data centers. Here‚Äôs how:\nECMP (Equal-Cost Multi-Path Routing): BGP is used to establish multiple equal-cost paths between devices in the network. This allows traffic to be spread across various routes, improving network efficiency and reducing the risk of congestion on any single link. In AI data centers, where high-volume data is transferred between GPUs and other components, load balancing is crucial to avoid bottlenecks.\nRouting Awareness and Flexibility: BGP is traditionally focused on reachability and loop prevention. However, in AI data centers, it is often extended to provide additional metadata about the quality of the routes. This enhanced routing awareness allows AI workloads to adapt based on the network\u0026rsquo;s real-time conditions, directing traffic along paths that avoid congestion and maintain high performance.\nCongestion Signaling: While BGP typically doesn\u0026rsquo;t respond to congestion in real-time, it can be integrated with other mechanisms that detect network congestion, such as congestion control algorithms. The video mentions how newer BGP extensions allow the protocol to signal beyond just reachability, providing hints about potential congestion downstream, allowing the system to dynamically adjust the load distribution.\nAI-Specific Use Case: In AI workloads, where communication between multiple GPUs is essential, BGP-based load balancing ensures that high-bandwidth traffic can be distributed efficiently across the network, maintaining the performance needed for rapid model training and inference without hitting capacity limits on individual routes.\nOverall, BGP\u0026rsquo;s scalability and ability to balance load across multiple paths make it a foundational protocol for managing traffic in AI data centers.\nThe key strategies for scaling AI networks without hardware replacement? The video outlines several key strategies for scaling AI networks effectively without frequent hardware replacements:\nModular and Repeatable Design: One of the main strategies is to design the network infrastructure in a modular and repeatable fashion. By creating building blocks, such as pods (groups of servers and switches), the infrastructure can be easily expanded without disrupting existing systems. When more computational power or network capacity is needed, additional modules or pods can be added without replacing the entire setup.\nAbstracted Layers: To manage the growing complexity of AI networks, the video stresses the importance of abstracting network layers. This means simplifying the view of the network as you scale upward. Lower levels of the network, closer to the servers, may require detailed management, but as you scale to higher layers, the network should become abstracted, reducing the burden of managing every detail. This abstraction allows for faster scaling while keeping the network manageable and avoiding large-scale hardware changes.\nCapacity Planning: Careful capacity planning is essential to ensure that the network can scale in response to demand. The video highlights how networks must be designed with future growth in mind, ensuring that new GPUs, switches, or entire data centers can be added seamlessly. Overbuilding in terms of bandwidth and computing resources ensures that the network can handle future growth without immediate hardware upgrades.\nFlexible Network Topology: AI networks are increasingly using leaf-spine architectures and segment routing, which allow the network to grow horizontally (scaling out) rather than vertically (scaling up). This flexibility means that instead of upgrading individual components (which requires replacement), the network topology can evolve by adding new links, GPUs, or switches to spread the load.\nSeamless Integration: As AI models and workloads expand, the infrastructure must allow for seamless integration of new technologies, such as the latest generation of GPUs or new routing protocols. By adopting open standards and scalable technologies like BGP and RDMA over IP, networks can accommodate new hardware and protocols without needing to overhaul the entire system.\nIn summary, scaling AI networks without hardware replacement depends on modularity, abstraction, capacity planning, and flexible network design. These principles help hyperscale data centers expand as AI models and data demands grow.\nThe role of congestion control in improving AI job completion times? The video highlights that congestion control plays a vital role in optimizing network performance, which directly impacts AI job completion times. Here\u0026rsquo;s how:\nMaintaining High Utilization: In AI data centers, maintaining high utilization of network resources is critical. Congestion control mechanisms help manage data flow and ensure that the network operates efficiently at peak levels. Without proper congestion management, traffic bottlenecks can occur, leading to slowdowns in communication between GPUs and other hardware. This delay can significantly extend the time required to complete AI tasks.\nDynamic Rate Adjustment: Congestion control protocols like ECN (Explicit Congestion Notification) and QCN (Quantized Congestion Notification) are used to dynamically adjust the rate of data transmission based on real-time network conditions. By reducing the transmission rate when congestion is detected, these protocols prevent packet loss and ensure smooth data flow, which helps in maintaining fast processing speeds and avoids redoing tasks caused by failed transmissions.\nReal-Time Feedback Loops: Congestion control uses real-time feedback to notify devices of network congestion. This feedback allows the system to react quickly, either by rerouting traffic or slowing down the rate of data transmission. The faster the system can respond to congestion signals, the more effectively it can avoid network disruptions that lead to delayed AI jobs.\nReducing Network Latency: High-performance AI clusters rely on low-latency networks to ensure that data is transferred as quickly as possible between components. Congestion control mechanisms help keep latency low by preventing data queues from building up at switches or routers, ensuring that packets move through the network without unnecessary delays.\nMinimizing Costly Retransmissions: In AI workloads, losing packets due to congestion can be extremely costly, as AI tasks often involve processing massive datasets. Congestion control mechanisms ensure that data is not dropped, thus avoiding the need for retransmissions, which would otherwise increase job completion time and waste computational resources.\nIn conclusion, congestion control is essential for minimizing delays, optimizing resource usage, and ensuring that AI jobs are completed as efficiently as possible in large-scale data centers.\nPodcast - AI/ML Data Center Design - Part 3 Summary: üéØ Session Overview \u0026amp; Expert Insights: This episode offers a detailed discussion on networking challenges in AI/ML data centers. Key focus areas include congestion control, BGP\u0026rsquo;s role in large-scale clusters, and advanced routing techniques used by companies like Meta and Alibaba. üöÄ Scaling AI Clusters: The conversation highlights the exponential growth of AI clusters, scaling from 1,000 GPUs to 100,000 GPUs. Scaling is achieved through BGP-based routing, fat-tree architectures, and RDMA over Ethernet (Rocky) and InfiniBand. üîó Innovations in Data Transport: Emphasizes TCP offload techniques and innovations like GPU Direct to optimize data movement within GPU clusters. üñ•Ô∏è Challenges in AI Training: Addresses the increased latency sensitivity in AI training workloads, with the introduction of synchronized GPU parallelism, where delays in one GPU affect the entire workload. üì∂ Network Resilience: Discusses the need for resiliency in data center fabrics, detailing techniques for redundancy and load balancing in RDMA-based networks. Insights Based on Numbers: 100,000 GPUs: Data centers are now handling up to 100,000 GPUs, highlighting the sheer scale required for modern AI workloads. 9x Faster Bandwidth: Modern GPU networking has bandwidth nine times faster than traditional Ethernet networks, demonstrating the need for ultra-high-speed communication in AI training. How fat-tree architectures contribute to scalability in AI networks? The video explains that fat-tree architectures play a pivotal role in the scalability of AI networks, especially in large data centers. Here are the key points:\nIncreased Bandwidth and Redundancy: Fat-tree architectures provide multiple paths between any two devices in the network. This design helps avoid congestion and ensures redundancy, making the network more resilient and able to handle a greater volume of traffic. As AI models scale, the increased east-west traffic (between GPUs) requires high bandwidth and low latency, which fat-tree setups provide by distributing the load across several paths.\nEfficient Load Balancing: The architecture‚Äôs multi-path design supports efficient load balancing by allowing traffic to be spread evenly across the available network links. In AI training clusters, where large datasets need to be shared between thousands of GPUs, this ensures optimal utilization of the network resources and avoids bottlenecks.\nScalability with GPU Growth: As AI workloads grow, data centers need to scale to tens or even hundreds of thousands of GPUs. The fat-tree architecture is ideal for this because it can scale out horizontally by simply adding more layers or switches to the tree. This allows for smooth expansion without redesigning the network infrastructure, making it flexible for future growth.\nSupporting East-West Traffic: AI clusters produce a lot of east-west traffic (i.e., traffic between servers or GPUs), and fat-tree architectures are designed to handle this efficiently. The added paths and bandwidth diversity ensure that the increasing demand for inter-node communication, typical in distributed AI training, is met without sacrificing performance.\nIn summary, fat-tree architectures provide the scalability, redundancy, and load balancing necessary to support the high-speed, high-bandwidth requirements of modern AI data centers.\nThe challenges of using RDMA over Ethernet in large-scale AI deployments? The video highlights several challenges when using RDMA (Remote Direct Memory Access) over Ethernet in large-scale AI deployments:\nCongestion Management: One of the primary challenges of RDMA over Ethernet is handling congestion effectively. Since RDMA bypasses the CPU to allow for faster data transfers, it can lead to congestion in the network. In large-scale AI clusters, where multiple GPUs are transferring huge amounts of data simultaneously, this can result in performance bottlenecks unless proper congestion control mechanisms, such as ECN (Explicit Congestion Notification), are implemented.\nReliability in Multi-Hop Networks: RDMA over Ethernet, particularly in large networks with multiple switches, can suffer from reliability issues when traffic crosses multiple hops. This is often due to the congestion and flow control problems that arise when RDMA traffic must share the same network paths as other types of traffic. This can cause delays, packet loss, or inefficiencies in large AI clusters unless the network is finely tuned.\nQuality of Service (QoS) and Traffic Separation: RDMA traffic is highly sensitive to latency, making it crucial to separate it from other types of traffic, such as TCP/IP, to ensure consistent performance. The video notes that many operators choose to deploy separate fabrics for RDMA to avoid conflicts with regular network traffic. However, managing multiple network fabrics can increase complexity in the data center\u0026rsquo;s infrastructure.\nScaling Beyond One-Hop Networks: Initially, there were concerns that RDMA over Ethernet (especially Rocky v1) could not scale efficiently beyond a one-hop network. Although newer versions (like Rocky v2) and advanced implementations have addressed some of these issues, scaling RDMA networks across multiple hops without experiencing performance degradation remains a technical challenge, particularly in large AI deployments.\nHardware and Interoperability: RDMA requires specialized NICs (Network Interface Cards) that support RDMA operations. Ensuring the interoperability of these NICs with the rest of the data center hardware, especially when different vendors are involved, can be another technical challenge.\nOverall, while RDMA over Ethernet offers significant performance benefits for large-scale AI tasks, it requires careful management of congestion, traffic separation, and scalability to function effectively in multi-hop, large-scale environments.\nWhy adaptive routing is essential for AI job completion in modern data centers? The video explains that adaptive routing is crucial for ensuring efficient AI job completion in modern data centers for several reasons:\nHandling Congestion Dynamically: Adaptive routing allows the network to adjust the path that data packets take based on real-time congestion information. This is particularly important in AI workloads, where delays in data transmission between GPUs can stall the entire job. By dynamically rerouting traffic around congested areas, adaptive routing ensures that data flows continue smoothly, preventing job slowdowns or failures.\nMinimizing Latency: AI jobs, especially distributed AI training, involve synchronizing large amounts of data between multiple GPUs. Even minor delays due to network congestion can accumulate, causing significant slowdowns in job completion. Adaptive routing helps minimize these delays by directing traffic through less congested paths, ensuring that GPUs can communicate with minimal latency.\nResilience to Network Failures: In large AI data centers, network failures (like link or switch failures) can severely impact job completion times. Adaptive routing helps maintain network resilience by instantly rerouting traffic around failed components, ensuring that the AI job can continue running without major disruptions.\nImproving Resource Utilization: By leveraging adaptive routing, data centers can optimize the use of network bandwidth and hardware resources. Instead of sticking to pre-defined paths, adaptive routing makes better use of available network resources, balancing the load across multiple routes. This improves overall network efficiency, which is critical when handling the massive data transfers involved in AI training.\nReducing Job Failures: AI workloads are often highly sensitive to network issues, and a slowdown in one part of the system can cause the entire job to fail or require restarting. Adaptive routing ensures that network issues like congestion or packet loss are quickly mitigated, reducing the chances of job failures and improving the overall reliability of AI workflows.\nIn summary, adaptive routing is essential for optimizing AI job completion by reducing latency, avoiding congestion, improving resilience to network failures, and ensuring efficient resource utilization in modern data centers.\nPodcast - AI/ML Data Center Design - Part 4 Summary: üéØ Networking for AI and ML: This episode explores the critical networking infrastructure required to support AI and ML workloads at scale, with a focus on how large clusters are managed. üöÄ Scaling and Parallelism: Discussion on methods of scaling AI infrastructure, including data parallelism, model parallelism, and pipeline parallelism, and their impact on performance. üîó NVIDIA\u0026rsquo;s NICL and Data Exchange: Highlights the NVIDIA NCCL (NVIDIA Collective Communication Library), crucial for optimizing communication between multiple GPUs in AI training environments. üñ•Ô∏è Challenges of Synchronization: Emphasizes the role of synchronization between GPUs, particularly for tasks requiring strong scaling, where multiple GPUs work on different portions of the data but need to synchronize results frequently. üí° Performance Optimization: Detailed strategies on minimizing communication overhead, including overlapping computation with data exchange and handling large models across multiple GPUs. Insights Based on Numbers: 100,000 GPUs: Modern AI infrastructures are scaling to handle up to 100,000 GPUs, revealing the massive growth in computational power required for AI. 1 Billion Parameters: AI models with over 1 billion parameters require complex parallelism, distributing data and computation across many GPUs to manage both computation and memory efficiently. How NCCL improves GPU communication in large AI clusters? The video explains that NVIDIA Collective Communication Library (NCCL) plays a crucial role in optimizing GPU communication in large AI clusters by addressing several challenges:\nEfficient Data Transfer: NCCL allows GPUs to communicate directly, bypassing the CPU, which speeds up data transfers between GPUs in a cluster. This direct communication is especially important in large AI clusters, where massive data volumes need to be exchanged frequently during tasks like training neural networks.\nParallel Communication: NCCL enables collective communication across multiple GPUs simultaneously. Instead of waiting for GPUs to finish one-by-one, NCCL synchronizes all GPUs to exchange data in parallel, ensuring that every GPU can send and receive data efficiently. This approach minimizes bottlenecks and helps maintain high throughput even as the number of GPUs scales.\nSupport for Large AI Models: As AI models grow larger, spreading computations across multiple GPUs becomes necessary. NCCL supports this process by providing optimized communication protocols that handle the complexity of synchronizing updates (e.g., gradients in neural network training) across different GPUs. This ensures that even large models can be trained efficiently in parallel.\nOverlapping Communication with Computation: NCCL is designed to allow overlapping communication and computation. This means that while GPUs are performing calculations, they can simultaneously start exchanging intermediate results. This overlap reduces the overall training time because data transfers do not have to wait for computations to complete.\nIn summary, NCCL enhances GPU communication by enabling efficient, parallel data exchanges, supporting large-scale parallelism, and minimizing delays through communication-computation overlap, making it a critical tool for scaling AI workloads.\nThe key challenges when scaling AI models to fit across multiple GPUs? The video outlines several key challenges when scaling AI models across multiple GPUs:\nMemory Constraints: One of the main challenges in scaling AI models across GPUs is managing the large memory requirements of modern neural networks. Models with billions of parameters cannot fit on a single GPU\u0026rsquo;s memory. This requires model parallelism, where different parts of the model are distributed across multiple GPUs. However, coordinating memory usage across GPUs becomes complex, especially for memory-intensive tasks like training deep learning models.\nSynchronization Overhead: As the model scales across multiple GPUs, the GPUs need to frequently synchronize to ensure they are working with updated weights and gradients. This leads to communication bottlenecks, especially in large clusters, as GPUs must constantly exchange data. The need to synchronize large amounts of data increases the overhead, affecting performance and training speed.\nData Parallelism Trade-offs: While data parallelism allows different GPUs to work on different subsets of data, it introduces challenges in gradient aggregation. After each GPU processes its data, the gradients need to be averaged and synchronized across GPUs, which can slow down the training process if the communication bandwidth is limited.\nStrong Scaling Limitations: In strong scaling, where the dataset remains the same but more GPUs are added to reduce computation time, there is a point where adding more GPUs leads to diminishing returns. The reason is that the overhead of synchronization and communication grows as the number of GPUs increases, eventually outweighing the performance gains from parallel computation.\nBalancing Computation and Communication: Achieving optimal performance when scaling across multiple GPUs requires carefully balancing computation and communication. If the computation per GPU is too small, the communication overhead (such as exchanging gradients or weights) will dominate, leading to inefficiency.\nIn summary, the key challenges when scaling AI models across multiple GPUs include managing memory constraints, reducing synchronization overhead, balancing computation and communication, and dealing with the limitations of strong scaling as the model grows larger.\nHow do data and model parallelism contribute to the efficiency of large-scale AI training? Data parallelism and model parallelism are two critical techniques used to improve the efficiency of large-scale AI training, particularly when training massive models that require substantial computational power.\n1. Data Parallelism: Data parallelism focuses on distributing the dataset across multiple GPUs. Each GPU processes a different subset of the data, but they all work on the same model parameters. Here\u0026rsquo;s how it improves efficiency:\nScalability: Since each GPU handles a portion of the data, the workload is distributed, allowing for faster processing of large datasets. This is especially useful when training on massive datasets that would be too time-consuming for a single GPU to handle.\nParallel Gradient Calculation: After processing their data, each GPU computes gradients locally. These gradients are then synchronized (averaged) across GPUs to ensure that the model parameters are updated consistently. This parallelization reduces the overall training time by allowing more data to be processed simultaneously.\nBatch Processing: In data parallelism, GPUs can process mini-batches of data simultaneously, leading to faster convergence of AI models. Larger batch sizes improve efficiency, though they may also lead to reduced model accuracy if not managed correctly.\n2. Model Parallelism: Model parallelism splits the model itself across multiple GPUs, where each GPU works on a different part of the model. This is particularly effective for large models that cannot fit into the memory of a single GPU. Here‚Äôs how it improves efficiency:\nMemory Efficiency: Large AI models, like those with billions of parameters, often exceed the memory capacity of a single GPU. Model parallelism splits the model across GPUs, allowing each GPU to handle a smaller portion of the model, solving memory constraints.\nLayer Distribution: Different layers or sections of the neural network are distributed across multiple GPUs. Each GPU processes its assigned part of the model and passes the data to the next GPU in sequence. This parallel processing reduces the memory load on each individual GPU while keeping the training process moving.\nSynchronization Between Layers: While model parallelism introduces more communication overhead as GPUs need to synchronize between layers, this is mitigated by techniques such as pipeline parallelism, which overlaps computation and communication. This reduces idle time and improves overall training speed.\nCombining Data and Model Parallelism: For maximum efficiency in large-scale AI training, many systems combine data and model parallelism:\nData parallelism enables faster data processing by distributing the data across GPUs. Model parallelism allows large models to be trained on GPUs with limited memory by distributing the model itself across the hardware. This combination ensures that both memory constraints and computation limits are addressed, resulting in highly efficient large-scale training of complex AI models.\nIn summary, data parallelism enhances scalability by splitting data across GPUs, while model parallelism handles large models by splitting the model across GPUs, contributing to the overall efficiency of AI training at scale.\n","href":"/blogs/2024-09-25-podcast-summary-ai-ml-data-center-design/","title":"Podcast Summary - AI/ML Data Center Design | Between 0x2 Nerds"},{"content":"Podcast Packet Pushers How HPC \u0026amp; AI Are Changing DC Networks Summary:\nüîå Introduction to AI \u0026amp; HPC Impact on Networks: Overview of how High-Performance Computing (HPC) and Artificial Intelligence (AI) are transforming data center network design, addressing issues like cooling, power, and bandwidth. ‚ö° Challenges in AI Networking: Explains the concept of Collective Communications, a core network design problem in AI training workloads, where multiple nodes must communicate simultaneously. üñß AI Model Training Issues: The complexity of training AI models, particularly in the context of data center infrastructure, where the network architecture faces significant challenges such as handling massive amounts of data and intensive computing tasks. üî• Physical Infrastructure Constraints: Discusses the need for physically isolated networks dedicated to AI training, due to the immense strain AI workloads place on standard data center networks. üåê NIC Evolution in AI Networks: Highlights the debate between smart and dumb Network Interface Cards (NICs) and the role of Remote Direct Memory Access (RDMA) in speeding up AI computing processes. üìä Bandwidth \u0026amp; Cooling Solutions: Insights into how optical and coherent networks help reduce power consumption as AI systems push towards 400Gb and 800Gb NICs, discussing the bottlenecks and inefficiencies faced by modern AI workloads. Insights Based on Numbers: Bandwidth requirements: AI systems demand up to 6.8Tbps per server, necessitating new network designs and infrastructure. Power consumption: AI workloads on modern networks can account for up to 50% of the total power used in a data center. Example Exploratory Questions: How does Collective Communication differ from traditional unicast or multicast in AI networking?\nWhat the video says about Collective Communication in AI networks is that it fundamentally differs from traditional unicast and multicast models. Here\u0026rsquo;s how:\nUnicast: In traditional networking, unicast refers to communication between one source and one destination. This method is efficient for most web traffic, where data flows from a server to a client or between two nodes.\nMulticast: In scenarios where a single source needs to send data to multiple destinations, multicast is used. The traffic is replicated to each destination, and this helps reduce bandwidth usage when broadcasting to multiple recipients.\nHowever, AI model training introduces a unique networking challenge called Collective Communication:\nCollective Communication: Unlike unicast or multicast, where data flows from one source to multiple or single destinations, collective communication requires all nodes in the network to communicate and synchronize simultaneously. This often happens in AI model training during operations like all-reduce, where each node must send data to every other node in the network. This level of synchronized communication puts a massive load on the network, creating bursty traffic where large amounts of data are transmitted all at once. This collective mode is critical for the training phase in AI models, especially when distributed across thousands of GPUs, as it allows for parallel processing and model updates across nodes. However, this comes at the cost of high network demand, necessitating specialized architecture for efficient performance.\nIn AI networking, collective communication becomes highly disruptive for typical networks due to the lockstep nature and the large volume of data exchanged across many nodes, as opposed to unicast\u0026rsquo;s or multicast\u0026rsquo;s more manageable traffic patterns.\nWhat are the main challenges in adapting existing data centers for AI training workloads?\nWhat the video says about adapting existing data centers for AI training workloads highlights several major challenges:\nNetwork Design Limitations: Traditional data centers are designed around typical networking needs, such as handling web requests or storage traffic. However, AI workloads require high-throughput, low-latency networks, which place massive demands on network bandwidth. AI training workloads are fundamentally different due to the need for collective communication, where many nodes communicate simultaneously. This leads to an immense amount of traffic that current data center architectures struggle to handle without significant overhauls.\nPower and Cooling Requirements: AI model training consumes substantial amounts of power. Existing data centers often cannot meet these power demands, leading to the need for dedicated AI clusters. Cooling is another critical challenge, as AI workloads generate significant heat. Traditional cooling systems may not be sufficient, necessitating liquid cooling or other advanced cooling technologies.\nSeparation of Networks: Due to the intense network strain from AI workloads, the video emphasizes that it is best practice to build physically separate networks for AI training. This ensures that traditional network tasks are not disrupted by the heavy demands of AI model training. This involves installing dedicated network infrastructure, such as additional NICs (Network Interface Cards), to handle the massive data loads required for AI operations.\nIn short, data centers need to be re-engineered to support the unique demands of AI training, focusing on network capacity, energy consumption, and infrastructure isolation.\nHow does the use of RDMA in AI networks reduce computational bottlenecks?\nWhat the video says about RDMA (Remote Direct Memory Access) in AI networks is that it plays a critical role in reducing computational bottlenecks during AI model training, primarily by bypassing the CPU for data transfers between nodes.\nIn traditional networks, when data is transferred, it often passes through the CPU, which can cause significant delays and add overhead. RDMA allows data to move directly between the memory of two systems without involving the CPU. Here\u0026rsquo;s how it mitigates bottlenecks in AI networks:\nCPU Bypass: With RDMA, the CPU is not involved in data transfer, allowing it to focus on other tasks. This eliminates the bottleneck of the CPU becoming overwhelmed with processing and data-handling responsibilities.\nDirect Memory Access: RDMA transfers data directly between the memory of two machines. In AI workloads, where massive amounts of data need to be shuffled between GPUs and servers during training, RDMA speeds up the process by reducing latency in data transmission.\nOptimized Network Bandwidth: AI model training involves collective communication, where thousands of GPUs communicate simultaneously. RDMA optimizes network bandwidth by ensuring data moves quickly between nodes, reducing delays that would otherwise occur from CPU-based processing.\nThis technology is especially valuable in AI systems where high-throughput and low-latency data transmission are critical for performance, allowing for faster and more efficient model training.\n","href":"/blogs/2024-09-25-podcast-summary-how-hpc-and-ai-are-changing-dc-networks/","title":"Podcast Summary - How HPC \u0026 AI Are Changing DC Networks | Packet Pushers"},{"content":"Link -\u0026gt; A Primer on the Cloud Link - \u0026gt; A Primer on Data Centers Join my email list to get direct access to new blog posts.\n","href":"/blogs/2024-09-18-primers-on-technology/","title":"A Primer on the Cloud \u0026 Data Centers"},{"content":"","href":"/categories/ietf/","title":"Ietf"},{"content":"IETF - Yang Models IETF Yang Models.\nRouting Service YANG Models (Southbound) Routing RFC/Draft Name Title Status Publication URL Progress RFC8294 Common YANG Data Types for the Routing Area RFC https://datatracker.ietf.org/doc/html/rfc8294 RFC8022 A YANG Data Model for Routing Management RFC https://datatracker.ietf.org/doc/html/rfc8022 RFC8349 A YANG Data Model for Routing Management (NMDA Version) RFC https://datatracker.ietf.org/doc/html/rfc8349 RFC8529 YANG Data Model for Network Instances RFC https://datatracker.ietf.org/doc/html/rfc8529 Routing Policy RFC/Draft Name Title Status Publication URL Progress RFC9067 A YANG Data Model for Routing Policy RFC https://datatracker.ietf.org/doc/html/rfc9067 RIB RFC/Draft Name Title Status Publication URL Progress RFC8341 A YANG Data Model for the Routing Information Base (RIB) RFC https://datatracker.ietf.org/doc/html/rfc8431 RFC9403 A YANG Data Model for RIB Extensions RFC https://datatracker.ietf.org/doc/html/rfc9403 ACL RFC/Draft Name Title Status Publication URL Progress RFC8519 YANG Data Model for Network Access Control Lists (ACLs) RFC https://datatracker.ietf.org/doc/html/rfc8519 QoS RFC/Draft Name Title Status Publication URL Progress draft-ietf-rtgwg-qos-model YANG Models for Quality of Service (QoS) WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-rtgwg-qos-model-11 BGP RFC/Draft Name Title Status Publication URL Progress draft-ietf-idr-bgp-model BGP YANG Model for Service Provider Networks WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-idr-bgp-model-17 WG Last Call Finished PCEP RFC/Draft Name Title Status Publication URL Progress draft-ietf-pce-pcep-yang A YANG Data Model for Path Computation Element Communications Protocol (PCEP) WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-pce-pcep-yang-22 WG Last Call Finished MPLS RFC/Draft Name Title Status Publication URL Progress RFC8960 A YANG Data Model for MPLS Base RFC https://datatracker.ietf.org/doc/html/rfc8960 RFC9070 YANG Data Model for MPLS LDP RFC https://datatracker.ietf.org/doc/html/rfc9070 draft-ietf-mpls-mldp-yang YANG Data Model for MPLS mLDP WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-mpls-mldp-yang-10 Expired draft-ietf-teas-yang-rsvp A YANG Data Model for Resource Reservation Protocol (RSVP) WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-yang-rsvp-18 Expired draft-ietf-teas-yang-rsvp-te A YANG Data Model for RSVP-TE Protocol WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-yang-rsvp-te-09 Expired draft-ietf-mpls-static-yang A YANG Data Model for MPLS Static LSPs WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-mpls-static-yang-13 Expired draft-nainar-mpls-lsp-ping-yang YANG Data Model for MPLS LSP Ping Individual Draft https://datatracker.ietf.org/doc/html/draft-nainar-mpls-lsp-ping-yang-05 SR RFC/Draft Name Title Status Publication URL Progress RFC9020 YANG Data Model for Segment Routing RFC https://datatracker.ietf.org/doc/html/rfc9020 draft-ietf-isis-sr-yang YANG Data Model for IS-IS Segment Routing WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-isis-sr-yang-17 draft-ietf-ospf-sr-yang Yang Data Model for OSPF SR (Segment Routing) Protocol WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-ospf-sr-yang-23 SRv6 RFC/Draft Name Title Status Publication URL Progress draft-ietf-spring-srv6-yang YANG Data Model for SRv6 Base and Static WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-spring-srv6-yang-02 Expired draft-ietf-spring-sr-policy-yang YANG Data Model for Segment Routing Policy WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-spring-sr-policy-yang-02 Expired draft-ietf-lsr-ospf-srv6-yang YANG Data Model for OSPF SRv6 WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-lsr-ospf-srv6-yang-04 draft-ietf-lsr-isis-srv6-yang YANG Data Model for IS-IS SRv6 WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-lsr-isis-srv6-yang-04 draft-ietf-pce-pcep-srv6-yang A YANG Data Model for Segment Routing (SR) Policy and SR in IPv6 (SRv6) support in Path Computation Element Communications Protocol (PCEP) WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-pce-pcep-srv6-yang-04 draft-raza-bess-srv6-services-yang Yang Data Model for SRv6 based Services Individual Draft https://datatracker.ietf.org/doc/html/draft-raza-bess-srv6-services-yang-01 Expired VPN RFC/Draft Name Title Status Publication URL Progress draft-ietf-bess-l3vpn-yang Yang Data Model for BGP/MPLS L3 VPNs WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-bess-l3vpn-yang-05 Expired draft-ietf-bess-l2vpn-yang YANG Data Model for MPLS-based L2VPN WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-bess-l2vpn-yang-10 Expired draft-ietf-bess-evpn-yang Yang Data Model for EVPN WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-bess-evpn-yang-07 Expired draft-ietf-bess-mvpn-yang Yang Data Model for Multicast in MPLS/BGP IP VPNs WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-bess-mvpn-yang-05 Expired VXLAN | \u0026mdash; | \u0026mdash; | \u0026mdash; | \u0026mdash; | \u0026mdash; | | draft-ietf-nvo3-yang-cfg | Base YANG Data Model for NVO3 Protocols | WG Draft | https://datatracker.ietf.org/doc/html/draft-ietf-nvo3-yang-cfg-05 | Expired |\nMulticast RFC/Draft Name Title Status Publication URL Progress RFC8652 A YANG Data Model for the Internet Group Management Protocol (IGMP) RFC https://datatracker.ietf.org/doc/html/rfc8652 RFC8916 A YANG Data Model for the Multicast Source Discovery Protocol (MSDP) RFC https://datatracker.ietf.org/doc/html/rfc8916 RFC9166 A Yang Data Model for IGMP and MLD Snooping RFC https://datatracker.ietf.org/doc/html/rfc9166 draft-ietf-mboned-multicast-yang-model Multicast YANG Data Model WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-mboned-multicast-yang-model-07 RFC9128 A YANG Data Model for Protocol Independent Multicast (PIM) RFC https://datatracker.ietf.org/doc/html/rfc9128 BFD RFC/Draft Name Title Status Publication URL Progress RFC9127 YANG Data Model for Bidirectional Forwarding Detection (BFD) RFC https://datatracker.ietf.org/doc/html/rfc9127 IP Basic YANG Models (Southbound) Inventory RFC/Draft Name Title Status Publication URL Progress RFC8348 A YANG Data Model for Hardware Management RFC https://datatracker.ietf.org/doc/html/rfc8348 Interface RFC/Draft Name Title Status Publication URL Progress draft-ietf-netmod-intf-ext-yang Common Interface Extension YANG Data Models WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-netmod-intf-ext-yang-12 draft-ietf-netmod-sub-intf-vlan-model Sub-interface VLAN YANG Data Models WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-netmod-sub-intf-vlan-model-09 RFC7223 A YANG Data Model for Interface Management RFC https://datatracker.ietf.org/doc/html/rfc7223 Obsoleted by RFC8343 RFC8343 A YANG Data Model for Interface Management RFC https://datatracker.ietf.org/doc/html/rfc8343 IP RFC/Draft Name Title Status Publication URL Progress RFC7277 A YANG Data Model for IP Management RFC https://datatracker.ietf.org/doc/html/rfc7277 Obsoleted by RFC8344 RFC8344 A YANG Data Model for IP Management RFC https://datatracker.ietf.org/doc/html/rfc8344 ARP RFC/Draft Name Title Status Publication URL Progress draft-ietf-rtgwg-arp-yang-model YANG Data Model for ARP WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-rtgwg-arp-yang-model-03 Expired VRRP RFC/Draft Name Title Status Publication URL Progress RFC8347 A YANG Data Model for the Virtual Router Redundancy Protocol (VRRP) RFC https://datatracker.ietf.org/doc/html/rfc8347 Time RFC/Draft Name Title Status Publication URL Progress RFC9249 A YANG Data Model for NTP RFC https://datatracker.ietf.org/doc/html/rfc9249 RFC8575 YANG Data Model for the Precision Time Protocol (PTP) RFC https://datatracker.ietf.org/doc/html/rfc8575 NAT RFC/Draft Name Title Status Publication URL Progress RFC8512 A YANG Module for Network Address Translation (NAT) and Network Prefix Translation (NPT) RFC https://datatracker.ietf.org/doc/html/rfc8512 RFC8513 A YANG Data Model for Dual-Stack Lite (DS-Lite) RFC https://datatracker.ietf.org/doc/html/rfc8513 RFC8676 YANG Modules for IPv4-in-IPv6 Address plus Port (A+P) Softwires RFC https://datatracker.ietf.org/doc/html/rfc8676 Security RFC/Draft Name Title Status Publication URL Progress RFC8177 YANG Data Model for Key Chains RFC https://datatracker.ietf.org/doc/html/rfc8177 AAA RFC/Draft Name Title Status Publication URL Progress RFC9105 Yang data model for TACACS+ RFC https://datatracker.ietf.org/doc/html/rfc9105 Northbound YANG Models Framework RFC/Draft Name Title Status Publication URL Progress RFC8309 Service Models Explained RFC https://datatracker.ietf.org/doc/html/rfc8309 RFC8969 A Framework for Automating Service and Network Management with YANG RFC https://datatracker.ietf.org/doc/html/rfc8969 RFC9417 Service Assurance for Intent-Based Networking Architecture RFC https://datatracker.ietf.org/doc/html/rfc9417 RFC9418 A YANG Data Model for Service Assurance RFC https://datatracker.ietf.org/doc/html/rfc9418 Services RFC/Draft Name Title Status Publication URL Progress RFC8466 A YANG Data Model for Layer 2 Virtual Private Network (L2VPN) Service Delivery RFC https://datatracker.ietf.org/doc/html/rfc8466 RFC8299 YANG Data Model for L3VPN Service Delivery RFC https://datatracker.ietf.org/doc/html/rfc8299 RFC9291 A Layer 2 VPN Network Yang Model RFC https://datatracker.ietf.org/doc/html/rfc9291 RFC9182 A Layer 3 VPN Network YANG Model RFC https://datatracker.ietf.org/doc/html/rfc9182 draft-ietf-teas-te-service-mapping-yang Traffic Engineering (TE) and Service Mapping Yang Model WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-te-service-mapping-yang-14 RFC9375 A YANG Data Model for Network and VPN Service Performance Monitoring RFC https://datatracker.ietf.org/doc/html/rfc9375 Topology RFC/Draft Name Title Status Publication URL Progress RFC8345 A YANG Data Model for Network Topologies RFC https://datatracker.ietf.org/doc/html/rfc8345 RFC8346 A YANG Data Model for Layer 3 Topologies RFC https://datatracker.ietf.org/doc/html/rfc8346 RFC8944 A YANG Data Model for Layer-2 Network Topologies RFC https://datatracker.ietf.org/doc/html/rfc8944 RFC8795 YANG Data Model for Traffic Engineering (TE) Topologies RFC https://datatracker.ietf.org/doc/html/rfc8795 RFC8542 A YANG Data Model for Fabric Topology in Data-Center Networks RFC https://datatracker.ietf.org/doc/html/rfc8542 draft-ietf-teas-yang-l3-te-topo YANG Data Model for Layer 3 TE Topologies WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-yang-l3-te-topo-15 draft-ietf-teas-yang-sr-te-topo YANG Data Model for SR and SR TE Topologies on MPLS Data Plane WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-yang-sr-te-topo-18 draft-ietf-teas-sf-aware-topo-model SF Aware TE Topology YANG Model WG Draft https://datatracker.ietf.org/doc/html/draft-ietf-teas-sf-aware-topo-model-12 RFC9408 A YANG Network Model for Service Attachment Points (SAPs) RFC https://datatracker.ietf.org/doc/html/rfc9408 IVY (Network Inventory YANG) https://datatracker.ietf.org/doc/draft-ietf-ivy-network-inventory-yang/ https://datatracker.ietf.org/doc/draft-ietf-ivy-network-inventory-topology/ Rerefences\nhttps://www.ipv6plus.net/IPv6Plus/YANG-Models/ ","href":"/writing/2024-08-12-ietf-yang-models/","title":"IETF - Yang Models"},{"content":"","href":"/tags/netconf/","title":"Netconf"},{"content":"","href":"/tags/programmability/","title":"Programmability"},{"content":"","href":"/tags/yang/","title":"Yang"},{"content":"","href":"/tags/network-observability/","title":"Network Observability"},{"content":"Network Observability The Many Uses of Network Observability Connections 2024: Network Observability Slides: Slides Recording: Recording Join my email list to get direct access to new blog posts.\n","href":"/writing/2024-08-06-network-observability/","title":"Network Observability"},{"content":"Data Center Networking for AI Clusters The Evolution of Data Center Networking for AI Workloads by Kentik Evolution of Data Center Networking Designs and Systems for AI Infrastructure by Sujal Das Evolution of Data Center Networking Designs and Systems for AI Infrastructure, Part 1 Evolution of Data Center Networking Designs and Systems for AI Infrastructure, Part 2 Evolution of Data Center Networking Designs and Systems for AI Infrastructure, Part 3 Evolution of Data Center Networking Designs and Systems for AI Infrastructure, Part 4 (Final) RoCE: from humble beginnings to being a star in the world of AI networking today Excellent Articles by Sharada Yeluri Large Language Models - The Hardware Connection GPU Fabrics for GenAI Workloads LLM Inference - HW/SW Optimizations In Network Acceleration for AI/ML Workloads NETWORKING THE AI DATA CENTER by Juniper Networking for the Era of AI: The Network Defines the Data Center by Nvidia, mirror RoCE networks for distributed AI training at scale by Meta Development Prospects for AI Data Center Network Architecture and Connectivity Technology by Rosenberger Demystifying AI Networking Protocols by Randy Zhang RoCE networks for distributed AI training at scale in Meta Join my email list to get direct access to new blog posts.\n","href":"/writing/2024-08-01-evolution-data-center-networking-for-ai-clusters/","title":"The Evolution of Data Center Networking for AI Clusters"},{"content":"IETF - Applicability of AI in the Network This is one of the hot topic in networking. There are side meeting in IETF also.\nIETF 118 Side Meeting IETF 119 Side Meeting IETF 120 Side Meeting IETF 121 Side Meeting ","href":"/writing/2024-07-30-ietf-ai-for-networking/","title":"IETF - AI for Networking"},{"content":"IETF - Data Center Networking for AI Clusters There has been a significant rise in the popularity of Artificial Intelligence and Machine Learning, leading to the design of data centers specifically tailored for large-scale AI model training. These advanced networks impose unique requirements for network topologies and interconnecting technologies.\nIETF 117 AIDC Side Meeting IETF 118 AIDC Side Meeting IETF 119 AIDC Side Meeting IETF 120 AIDC Side Meeting IETF 121 AIDC Side Meeting Join my email list to get direct access to new blog posts.\n","href":"/writing/2024-07-16-ietf-data-center-networking-for-ai-clusters/","title":"IETF - Data Center Networking for AI Clusters"},{"content":"Traffic tromboning is a phenomena in data center traffic configuration or service chaining, while some service is bottleneck for rest of the flows. The service chaining can be created between leaves and spines using VXLAN and VRF way.\nThis is the article written by Mario Baldi and Silvano Gai originally posted in Traffic Tromboning Why, What, Where, How, and How Not\nWhy do we have traffic tromboning? Network and security services have traditionally been deployed in front of a group of end hosts that share some common function and are topologically close: e.g., private corporate clients, public corporate servers, the servers of a specific department, the hosts of a corporate branch. The following figure shows a most common deployment model for services such as NAT, load balancers and firewalls. With virtualization, while the same logical model still applies, the physical placement of the hosts, their physical proximity, and the natural point to deploy the service application (on the shortest path of traffic to and from the served hosts) have disappeared. Of course! It is in the very nature of virtualization (as well as its value and goal) to make all functional components (resources, topology, location) independent of the physical ones. Hosts communicating with each other can be anywhere in a data center and consequently the network design must:\nProvide equal and abundant bandwidth between any pair of hosts. Separate ‚Äúphysical‚Äù adjacency (in the IP terminology sense, namely layer two connectivity) from the actual physical topology. The Clos network topology with its fractal capability of scaling (possibly incrementally) by adding identical components (leaf and spine switches) interconnected according to the same pattern, effectively solves the first requirement.\nNetwork virtualization techniques, such as SDN (Software Defined Networking) and VXLAN (Virtual Extensible LAN), address the second requirement.\nIn this context, the remaining challenge is how to deploy the networking and security services (whether discrete/physical appliances or virtual network functions running on the same type of physical hosts as the workloads), in front of the (virtual) hosts they serve. The solution relies on the same technologies and infrastructure that make logical proximity independent of physical (topological) proximity: network virtualization.\nSpecifically, there are two main mechanisms that can be used for this purpose:\nConfigure the service (virtual) appliance as the default gateway for a particular IP subnet. This requires layer two connectivity between the source of the traffic and the appliance. All packets that need to go outside the IP subnet need to cross the appliance/default gateway by definition. The appliance injects a default route using a routing protocol like BGP or OSPF. All traffic that does not have a more specific route to the destination will follow the default route and therefore traverse the appliance. We will subsequently discuss more details and the implications of these two techniques when used in conjunction with Clos networks in data centers.\nWhat is traffic tromboning? Whichever of the above techniques is deployed, traffic between topologically close hosts might take a long detour and get back, as shown in the figure below.\nThis phenomenon is commonly known as traffic trombone or traffic tromboning, as the shape of the path taken by the traffic in the network resembles the shape of the brass musical instrument that dates back to the 15th century and whose name, trombone, derives from ‚Äútromba,‚Äù the Italian word for trumpet: a large trumpet.\nIn a data center fabric based on a Clos topology, traffic tromboning results into traffic traversing the leaf-spine fabric twice, which leads to duplication of both network load and the latency experienced by the two hosts.\nMoreover, when services are applied to traffic between most pairs of hosts, the appliance becomes a funneling point - also recalling the shape of a trombone - that must be traversed by traffic from a disparate set of source-destination pairs (see the following figure for a graphic representation). We will therefore subsequently refer to service appliances such as firewalls, IDSs (Intrusion Detection Systems), NAT (Network Address Translation), load balancers, as trombones.\nTrombones, by their very nature, create a network traffic scalability problem. Addressing this through load balancing among multiple trombones is challenging since their services are often stateful.\nLet‚Äôs consider a ubiquitous combination of a firewall and a NAT that we all have in the router that connects our house to the Internet. When a family member browses the Internet, a TCP session is created. The router needs to create a NAT rule for the outgoing traffic, but also a second one for the return traffic. These two rules need to be kept for the duration of the TCP session.\nIf, for example, in a corporate network context, two or more NAT appliances are required for scale or fault tolerance, one of the following should apply:\nDirect and reverse directions of a flow should go through the same appliance. State should be synchronized across the various appliances. Synchronizing state across appliances is at best impractical and at worst impossible.\nFor example, if a NAT function is present, the packet on the Internet will have the NAT source IP address. If multiple appliances are deployed in parallel, the reverse traffic will be explicitly addressed and routed to the NAT instance with the IP address used for the translation. Hence, when a flow is assigned to a traffic trombone, it uses that particular trombone for all its duration or until the trombone fails.\nMechanisms exist for high availability across multiple trombones in case of catastrophic failures.\nNorth-South vs East-West Traffic Not every trombone is a bottleneck. Historically tromboning was happening towards the wide area network, a typical example being the firewall connecting a corporate network to the Internet. However, usually in that scenario the capacity of the link to the service provider is the bottleneck.\nToday some high end firewalls, albeit costly, are capable of supporting up to 100 Gbps. Firewalls operating at 1 Gbps are very common and cheap, with 10 Gbps ones being considered the reasonably priced midrange. Therefore firewalls are typically not choke-points when used to connect to the Internet, a type of traffic that is also called North-South in a data center.\nDifferent considerations apply to East-West traffic, i.e., the traffic inside the data center.\nWe have seen in a previous post that the amount of East-West traffic is at least ten times larger than the North-South one.\nWhen applied to East-West traffic, the firewall trombone may become a bottleneck. In the same post, we have also discussed how placing the trombone on a Clos network disrupts optimal routing increasing latency and causing a large percentage of the network bandwidth to go wasted.\nThere are indeed enterprise deployments where this might not be a problem because the traffic is limited compared to the large capacity of the data center fabric and the specific applications utilized can tolerate latency. But, there are a significant number of cases where traffic tromboning drawbacks are critical, with obvious examples being cloud provider data centers (where the fabric bandwidth is shared by the various tenants) and market segments that deploy time sensitive applications, such as financial trading.\nHow is traffic tromboning realized? Let‚Äôs now consider the configuration complexity of traffic tromboning for East-West traffic in conjunction with Clos networks for the two approaches previously mentioned.\nLayer two traffic tromboning To explain this first configuration we will reuse a figure from the previously mentioned post.\nIn this example, the E-W Firewall is deployed in transparent mode, acting as the default gateway for the server C subnet. As previously explained, IP rules require that a host and its default gateway be in the same subnet, i.e., be able to exchange layer two frames.\nThis requires some type of encapsulation, because in typical Clos data center networks each leaf implements a layer two domain (with its own IP subnet) and uses layer three forwarding towards the spines leveraging ECMP (equal cost multi-path) routing to spread the traffic across multiple paths. Therefore the network administrator needs to deploy an encapsulation technique, like VXLAN, to create a virtual layer two domain including server C (at leaf L2) and the E-W Firewall (that is physically connected to leaf L1).\nSince the firewall needs to serve as a default gateway for servers connected to multiple leaves (namely, multiple IP subnets), VLAN trunking is typically enabled between leaf L1 and the E-W Firewall. Each VLAN contains the IP subnet of a leaf and is mapped to a VNID (Virtual Network IDentifier) of the VXLAN protocol. For example, the leaf L2 may encapsulate the native VLAN toward server C in a VXLAN tunnel with VNID = 88 and leaf L1 may remove the VXLAN encapsulation and map VNID = 88 to VLAN = 8 toward the E-W Firewall. Leaf L4 may use VNID = 44 and leaf L1 may map it to VLAN = 4. The same applies to the reverse direction.\nThis approach therefore uses a layer two network virtualization technique.\nLayer three traffic tromboning In order to use layer 3 routing to force traffic through a trombone, that trombone must inject a default route that is propagated through the network and used for all traffic that does not have a more specific route. With reference to the previous figure, a default route injected in the fabric by the E-W Firewall can be used to steer the traffic from Server C to Server Z through the firewall only if leaf L2 and the other leaf and spine switches involved do not have a more specific route to Server Z. How can this be possible, while enabling Server Z to be reached by packets once they have been processed by the firewall?\nThe solution is offered by layer three network virtualization. Virtual Routing and Forwarding (VRF) is a popular technique allowing multiple instances of a routing table to exist in a router and work simultaneously. Each physical interface (i.e., a port) or logical interface (i.e., a VLAN) is associated with one VRF instance. The BGP routing protocol is deployed to advertise routes specifying the VRF instance they belong to.\nIn the specific case of the above figure, the East-West traffic requires three VRFs: red, green, and blue. The E-W Firewall is the only point of contact in the red VRF between Server C and the rest of the network, and thus all the traffic must transit through the firewall. To achieve this the E-W Firewall injects a default route into the red VRF that is propagated through the network and used for traffic on the red VRF. The routing tables of the red VRF contain more specific routes only to destinations connected to leaf L2, including Server C; consequently, when switches route a packet addressed to Server Z, they send it towards the firewall as indicated by the default route.\nThe E-W Firewall is deployed in routed mode and participates in two VRFs: the red VRF associated with the IP subnet of Server C and the green VRF associated with the load balancer (i.e., Server Z). The firewall‚Äôs routing table includes a route to Server Z through the physical or logical interface that belongs to the green VRF; hence, the packet is forwarded back into the fabric through such interface. Leaf and spine switches apply the green VRF whose tables contain a specific route for Server Z.\nThe load balancer executes its function and determines that the packet should be delivered to physical Server E. The load balancer‚Äôs routing table has a route to the IP subnet of leaf L3 through the interface associated with the blue VRF. Hence, once the load balancer forwards the packet into the fabric, switches use the blue VRF tables that contain a route to the subnet of Server E.\nThe trombones can be connected to multiple VRFs either using multiple physical ports or multiple logical interfaces (e.g., VLANs) on the same physical port, and do not need to be VRF-aware. They just need a single routing table with routes through the proper physical or logical interfaces. In our use case, the E-W Firewall learns such routes by maintaining two routing sessions (e.g., using BGP): one on the red port (physical or logical) and the other one on the green port.\nAll the spine switches in our network participate in the three VRFs. Leaf L1 is part of the red and green VRFs, leaf L4 green and blue, leaf L2 red, and leaf L3 blue. More VRFs can be deployed on leaves to serve the remaining hosts.\nIn this case the use of VXLAN is not needed and is replaced by VRF. More complicated schemes that combine VXLAN and VRF are also possible.\nHow to avoid traffic tromboning This analysis of traffic tromboning leads to very similar conclusions to the ones of our previous post. Whenever possible, centralized appliances should be avoided for East-West traffic since they are bottlenecks, disrupt optimal routing, and introduce significant latency and jitter. Instead, the same scale out model utilized for the compute should be leveraged by naturally placing services in a distributed fashion at the edge.\nIn virtualized and containerized environments, this can be achieved by executing services in a Virtual Machine (VM) or container running on the very host where workloads receiving the services are executed. This approach still results in a small tromboning effect within hosts (as traffic flows between workloads and service VM) or between neighboring hosts if service VMs are not placed in every single host.\nSuch local tromboning can be avoided by including services in the hypervisor software stack, for example within the virtual switch.\nBoth above solutions cannot be applied to bare metal (not virtualized and containerized) environments and share the major drawback of taking CPU and memory resources away from paying workloads to implement services.\nHence, the best solution is having specialized hardware on the hosts to implement and run services. The ideal point to place such hardware is at the network attachment that is naturally on the traffic path.\nAdopting domain-specific hardware for services at the edge and installing it in conjunction with servers allows services to:\nrun at wire speed without taking away any resources from the workloads executing on the hosts scale at the same rate as compute. Join my email list to get direct access to new blog posts.\n","href":"/writing/2024-04-07-clos-ip-traffic-between-leaf-and-spine/","title":"Clos IP Traffic between Leaf and Spine \u0026 Tromboning"},{"content":"","href":"/tags/forwarding/","title":"Forwarding"},{"content":"","href":"/tags/switching/","title":"Switching"},{"content":"A History of Network Switches 1990s: Layer 2 Switches Network switches (switches for short) are the evolution of network bridges whose behavior was defined by the Institute of Electrical and Electronics Engineers (IEEE) in the standard IEEE 802.1 to connect two or more Ethernet segments. Switch and switching are terms that do not exist in standards; they were introduced to indicate a multiport bridge.\nInitially, switches were pure layer 2 devices that forwarded Ethernet frames without knowing their content and without modifying the frames, thus providing connectivity for the many layer 3 protocols deployed in those days. The forwarding model of layer 2 switches is straightforward and based on the exact lookup of the destination MAC address in a forwarding (or filtering) table; this is usually accomplished with a hashing table, which is easy to implement in hardware. Layer 2 switches do not require any configuration; the forwarding table is initially empty and is populated by associating the source MAC address of a frame being received with the port through which it is received - a technique called backward learning. When the lookup of the destination MAC address fails (i.e., the association of the MAC address with a port has not yet been done), the frame is forwarded on all ports other than the one it was received from - a technique named selective broadcast.\nThis forwarding technique works only on a tree topology because backward learning does not operate properly when frames travel along a loop and selective broadcast creates copies of frames traveling along a loop until the capacity of links and/or switches is saturated. On a mesh topology, the Spanning Tree protocol is deployed to block a set of ports that are not used to ensure that forwarding takes place on a tree. Blocking ports is inefficient from a bandwidth perspective because the corresponding links, although potentially capable of carrying traffic, are not being used. In the nineties, there was a lot of innovation around the spanning tree protocol trying to increase the overall network bandwidth, but all proposed solutions introduce significant complexity in the network design and configuration to ensure high bandwidth utilization.\n2000s: The Introduction of Layer 3 Switches In the 2000s, it became apparent that the only surviving layer 3 protocol was IPv4 (Internet Protocol version 4). Therefore, it made sense to bypass the limits of the Spanning Tree Protocol by implementing IPv4 routing into the switches. Layer 3 switches were born. IP forwarding does not require blocking any port in the network, hence it can take advantage of all the links in a network. Traditionally a Layer 3 switch forwards all traffic to a destination on the same path. However, a feature called ECMP (Equal Cost Multi Path) even enables to load balance traffic to each destination among multiple paths that are considered to have the same cost according to the routing protocol in use, like for example BGP (Border Gateway Protocol).\nAppreciating the difference between layer 2 (L2) forwarding and layer 3 (L3) forwarding is essential to understanding why Layer 3 switches are substantially more complex than Layer 2 switches. At the IP level, forwarding is done packet-by-packet by looking up the destination IP address into a forwarding table with a technique called LPM (Longest Prefix Match), which is much more complex to implement in hardware than the exact, possibly hash-based, matching required in L2 switching. Deploying LPM enables the routing table not to include all the possible IP addresses, but just prefixes, thus allowing for larger networks without proportionally larger forwarding tables. A prefix is expressed as a combination of an IP address and a netmask to identify which bits in the address represent the prefix. Being 32 bit long, a netmask is cumbersome to write and not intuitive to understand; hence often the length of the prefix - or number of most significant bits in the address that represent the prefix - is explicitly indicated (e.g., /16, /24).\nAn entry of the forwarding table can be used to determine the forwarding information (e.g., the next hop and/or output interface) for any packet with a destination address that has that prefix in its most significant bits. If multiple entries match, the switch selects the one with the longest prefix: since it refers to a smaller set of addresses, it is somewhat more specific to the destination at hand.\nThis kind of forwarding is stateless since each packet is forwarded independently of previous packets. With the growing size of ICs (Integrated Circuits), more features were added to layer 3 switches, such as ACLs (Access Control Lists) to classify and filter packets, still in a stateless fashion, with limited scalability. Access-Aggregation-Core In terms of network design, the preferred topology was a Core-Aggregation-Access Design (picture courtesy of Dinesh Dutt): a mixture of layer 2 at the periphery and layer 3 in the network‚Äôs core, to allow layer 3 subnets to span across multiple edge switches. This allows, for example, Virtual Machine mobility among the servers connected to the same access switches. This design provided more aggregate bandwidth than a pure layer 2 approach, but still had a bottleneck in the two core and aggregation switches: traffic to all destinations not attached to the same pair of access switches must travel through the access switch identified as the root of the spanning tree.\n2010: Modern L3 Switches In the next decade, with the advent of even denser integrated circuits, layer 3 switches gained other functionality that can be grouped in three categories.\nClos Network and ECMP Support Clos networks remove the bottleneck present in the core of the Core-Aggregation-Access Design. A Clos network is a multistage network that Charles Clos first formalized in 1952. It is a two-stage network in its simplest embodiment, like the one shown in the figure, but it can scale to an arbitrary number of stages.\nClos Network\nIn their current usage and terminology, two-stage Clos networks have leaves (equivalent to access switches) and spines (equivalent to aggregation switches). The spines interconnect the leaves, and the leaves connect the network users. The network users are mostly, but not only the servers and can include network appliances, wide-area routers, gateways, and so on. No network users are attached to the spines.\nThe Clos topology is widely accepted in data center and cloud networks because it scales horizontally: adding more leaves allows to connect as many network users as needed, while adding more spines enables supporting larger amounts of East-West traffic. In fact, traffic to destinations not attached to the same leaf can transit through any of the spines (and corresponding link connecting the selected spine to the source and destination leaf). In order to enable this, both leaves and spines must be layer 3 switches (as layer 2 switches would block all leaf-spine links but one) and ensure that all spines are being used when forwarding packets. This is achieved through a technique called equal-cost multipath (ECMP) that stores multiple next hops (e.g., spines) for the same destination and load-balances forwarded packets to all of them.\nNetwork Virtualization Support Network virtualization enables the definition of multiple independent and separate virtual networks on a single physical network. In other words, although there is a single set of switches and links (physical network), multiple networks can be defined: the configuration (e.g., addressing scheme) of each network is independent from the others and the traffic is completely separated from the others.\nOverlay networks are a common way to satisfy this requirement. An overlay network is a virtual network built on an underlay network, i.e., a physical infrastructure.\nThe underlay network‚Äôs primary responsibility is forwarding the overlay encapsulated packets (for example, using VXLAN encapsulation) across the underlay network efficiently using ECMP when available. The underlay provides a service to the overlay. In modern network designs, the underlay network is always an IP network (either IPv4 or IPv6) because we are interested in running over a Clos fabric that requires IP routing.\nIn overlays, it is possible to decouple the IP addresses used by the applications (overlay) from the IP addresses used by the infrastructure (underlay). The VMs that run a user application may use a few addresses from a customer IP subnet (overlay), whereas the servers that host the VMs use IP addresses that belong to the cloud provider infrastructure (underlay). VXLAN is probably the best-known encapsulation protocol for overlay networks.The points where packets are encapsulated/decapsulated are called VTEPs (VXLAN tunnel endpoints).\nAs we mentioned in the introduction, Core-Aggregation-Edge networks allowed layer 2 domains (that is, layer 2 broadcast domains) to span multiple switches. The VID (VLAN identifier) was the standard way to segregate the traffic from different users and applications. Each VLAN carried one or more IP subnets that were spanning multiple switches.\nClos network changed this by mandating routing between the leaves and the spines and limiting the layer 2 domains to the southern part of the leaves toward the hosts. The same is true for the IP subnets where the hosts are connected. VXLAN solves the problem of using Clos networks while at the same time propagating a layer 2 domain over them. In a nutshell, VXLAN carries a VLAN across a routed network. In essence, VXLAN enables seamless connectivity of servers by allowing them to continue to share the same layer 2 broadcast domain.\nVXLAN Tunnel Termination\nSDN Support At the end of the previous century, all the crucial problems in networking were solved. On my bookshelf, I still have a copy of Interconnections: Bridges and Routers by Radia Perlman dated 1993. In her book, Perlman presents the spanning tree protocol for bridged networks, distance vector routing, and link-state routing: all the essential tools that we use today to build a network. During the following two decades, there have been improvements, but they were minor, incremental. The most significant change has been the dramatic increase in link speed.\nIn 2008, Professor Nick McKeown and others published their milestone paper on OpenFlow. Everybody got excited and thought that with software-defined networking (SDN), networks would change forever. They did, but it was a transient change; many people tried OpenFlow, but few adopted it. Five years later, most of the bridges and routers were still working as described by Perlman in her book.\nBut two revolutions had started:\nA distributed control plane with open API A programmable data plane The second one was not successful because it was too simple and didn‚Äôt map well to hardware. However, it created the condition for the development of P4 architecture to address those issues. OpenFlow impacted even more host networking, where the SDN concept is used in a plethora of solutions.\nSDN enables virtualized networks with a less complex overlay not based on running control protocols like BGP, but programmed through an SDN controller.\nScalability Considerations These switches have impressive performance in terms of bandwidth and PPS (Packet Per Second) but they are substantially stateless devices that are not idoneous to implement stateful services like firewall, NAT, load balancing, etc. They also have scalability issues in the number of routes and ACLs that they can support.\nScalability is critical because each virtual network has its addressing space that cannot be aggregated with one of the others and its own separate tables for services like ACL, NAT, etc.. Hence, even though the virtual topology of virtualized networks may be simpler than the physical topology, and hence routing tables smaller than the ones of the physical network, still the amount of memory required for routing tables and data structures for other services increases the size of routing tables. Moreover, VTEP mapping requires large tables that are not present in traditional switches that do not support network virtualization.\n2020: SmartSwitches (Distributed Service Switches) SmartSwitches, a.k.a Distributed Service Switches, add to the state of the art layer 3 switches a rich collection of wire-rate stateful services, e.g., firewall, LB (Load Balancer), NAT (Network Address Translation, TAP (Test Access Point) Network (traffic observability), streaming telemetry, etc. A SmartSwitch can apply these services both to the underlay network and to the overlay network. In the second case, they are typically colocated with the VTEPs.\nTo effectively support these services, SmartSwitches have a different forwarding mechanism that is stateful: decision of what to do with the packet may depend on previous packets of the same communication. Hence, each packet between host A and host B is processed as a part of a session between A and B that comprises two unidirectional flows: one from A to B and one from B to A. The term session applies as well to connection oriented protocols like TCP or connectionless protocols like UDP.\nThe concept of session is important because in many cases reaching the forwarding decision (whether to forward the packet and if so how) requires complex processing of one or more packets and then remains the same for subsequent packets of the same communication.\nThe stateful forwarding mechanism is also called cache-based forwarding. It relies on a flow cache, a binary data structure capable of ‚Äúexact‚Äù matching packets belonging to a particular flow. The word ‚Äúexact‚Äù implies a binary match easier to implement both in hardware or software, unlike a ternary match, such as LPM. The flow cache contains an entry for each flow, i.e., two entries per session. The flow can be defined with an arbitrary number of fields, thus supporting IPv4 and IPv6 addresses, different encapsulations, policy routing, and firewalling.\nCache entry contains information needed to forward the packet (e.g., layer 2 and/or layer 3 address of the next hop, type of encapsulation, etc.).\nThe separate initial processing that leads to the forwarding decision may create new cache entries. When a packet is received by the switch, if it does not match any entry in the flow cache (a ‚Äúmiss‚Äù) it is processed according to the initial processing. Otherwise (‚Äúhit‚Äù), it is forwarded according to the information in the matching flow cache entry. The above figure shows the organization of this solution. Any packet that causes a miss in the flow cache is redirected to a software process that applies a complete decision process and forwards or drops the packet accordingly. This software process also adds two entries in the flow cache (dashed line) for the session, so that subsequent packets can be forwarded or dropped as the previous packets of the same flow.\nBecause the processing of initial packets (misses) is so clearly separated from the processing of the following packets (hits), it can be implemented independently and possibly on a separated execution engine. For example, hits could be processed by a specialized integrated circuit or an application specific processor (hardware data path), while misses could be processed by software running on a general purpose CPU (software data path).\nLet‚Äôs suppose that an average flow is composed of 500 packets: one will hit the software process, and the remaining 499 will be processed directly in hardware with a speed-up factor of 500 compared to a pure software solution. In other words, cache-based forwarding allows one to take advantage of the flexibility of software with the performance of hardware. Of course, this solution should guarantee that packets processed in software and hardware do not get reordered.\nSince in cache-based forwarding the handling of the first packet of a flow or session can be implemented completely independently from the handling of subsequent packets, when evaluating the performance of a SmartSwitch we look at two different indexes:\nConnections Per Second (CPS) offers an indication of the performance in processing the initial packet of each flow. Packet Per Second (PPS) provides a measure of the performance in processing other packets. In specific use cases, complex and time consuming processing is allowed on the first packet to decide whether and how a flow should be forwarded without affecting the throughput in terms of PPS.\nScalability Considerations This architecture requires that the SmartSwitches have a large volume of high-speed memory to store all the flow descriptors, the ACLs, and the routes. Cloud providers have a large number of tenants each with their virtual network. Even if routing and access control lists (ACLs) requirements for each tenant are moderate, when multiplied by the number of tenants, there is an explosion in the number of routes and ACLs, which can be difficult to accommodate on the host with traditional table lookups. It is not unreasonable to support millions of routes and ACLs and tens of millions of flows. If the flow descriptor is, let‚Äôs say, 256 bytes, and we have ten million, the flow cache consumes 2.5 GB of memory. When we add the size of the route table, the ACL table, intermediate data structure and code, it is not unreasonable to use 8 to 16 GB of memory. This size of memory cannot be integrated on-chip, and therefore external memory needs to be used. Multiple DDR busses are required to obtain high performance, with DDR5 at the highest possible frequency being desirable. It helps that, with the exception of the flow table, the rest of information/tables is needed only for processing misses, which is a small fraction of the overall traffic.\nCopyright: https://silvanogai.github.io/posts/history/\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2024-04-06-various_switching_techniques/","title":"Various Packet Switching Techniques"},{"content":"","href":"/tags/career-progression/","title":"Career Progression"},{"content":"Self Improvement How I optimised my life to make my job redundant - Troy Hunt\u0026rsquo;s point of view Write a brag document - Get your work recognized: write a brag document You Are Your Own Best Hype Person Brag now, remember later: Document your accomplishments Being visible - Why visibility matters Being Glue - If you stop doing those things, the team won\u0026rsquo;t be as successful. But now someone\u0026rsquo;s suggesting that you might be happier in a less technical role. If this describes you, congratulations: you\u0026rsquo;re the glue. If it\u0026rsquo;s not, have you thought about who is filling this role on your team? Three crucial skills that leaders must develop to become executives - These skills include taking (almost irrational) career risk, learning to scale by trusting your team, and developing advanced soft skills. Making the case to decision makers: the presentation format to follow - A tested template to follow when presenting to executive leadership teams. How to be a tech influencer | Will Larson - Most successful people are not well-known online Being well-known online can be bad Most successful folks are prestigious in some way Content creation is an effective way to create prestige It\u0026rsquo;s hard to measure influence Don\u0026rsquo;t assume everyone\u0026rsquo;s playing the same game The impact of less scalable work | Will Larson - \u0026ldquo;Less scalable\u0026rdquo; work is more impactful than \u0026ldquo;more scalable\u0026rdquo; work if it reaches folks who are themselves in high impact roles Hard to work with | Will Larson - I\u0026rsquo;ve seen a staggering number of folks fail in an organization primarily because they want to hold others to a higher standard than their organization‚Äôs management is willing to enforce A forty-year career | Will Larson Pace The biggest barrier to a forty year career is burnout, and preventing burnout is twofold First, work on work you find meaningful Second, manage your pace People Your current coworkers also have an outsized influence on your career long after you\u0026rsquo;ve stopped working together It requires some deliberate focus to approach each new role with the intention of building the network of folks you know and work with well, but the compounding value of doing so is huge Prestige In retrospect, many folks\u0026rsquo; prestige seems inevitable, but is the result of deliberate, intentional action over an extended period With each bit of prestige you accumulate, gathering the next bit gets easier Profit As you get deeper into your career, you\u0026rsquo;ll move into increasingly senior roles, and there are considerably fewer such roles available The process takes longer, and time depends upon money. Consequently, the best roles are only accessible if you‚Äôre already financially stable Financial security is a prerequisite to own your pace and learning Learning A forty year career has ample space and need for extended periods of both \u0026ldquo;learning deep\u0026rdquo; and \u0026ldquo;learning broad\u0026rdquo; A career ending mistake - An insightful essay to help you assess where your career is going and where you want to go Work / Life Balance Work and life distinction How to think about balance Balancing techniques Wake up and smell the roses Embrace the suck Anti-patterns Changes Expiring vs. Long-Term Knowledge - How much of what you read today will you still care about a year from now? Questions to ask\n\u0026ldquo;How am I doing compared to your expectations?\u0026rdquo; \u0026ldquo;What do you see as my strengths \u0026amp; weaknesses?\u0026rdquo; \u0026ldquo;What skills do I need to get to next level?\u0026rdquo; \u0026ldquo;Any tips on professional development?\u0026rdquo; Promotions Does the title even matter? - Why titles do matter How to get promoted - Almost everyone who does great work toils in relative obscurity. Performance reviews are social fiction. How do people really advance through the corporate hierarchy? Questions to ask\n\u0026ldquo;Can we walk through the expectations for the next level to make sure I understand them?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the most effective thing I can do to make myself a stronger candidate?\u0026rdquo; \u0026ldquo;What areas do you think I should focus on?\u0026rdquo; \u0026ldquo;What are the requirements needed to advance?\u0026rdquo; \u0026ldquo;What are the skills that I\u0026rsquo;ll need to demonstrate?\u0026rdquo; \u0026ldquo;How can I best demonstrate them?\u0026rdquo; \u0026ldquo;If I don\u0026rsquo;t get promoted this cycle, what are some of the likely causes?\u0026rdquo; Examples \u0026ldquo;I\u0026rsquo;m ready to move ahead in the organization\u0026rdquo; or \u0026ldquo;I will be ready soon\u0026rdquo; \u0026ldquo;I\u0026rsquo;m enjoying what I do and I look forward to taking on more\u0026rdquo; \u0026ldquo;I\u0026rsquo;d like to be a candidate for the manager position that\u0026rsquo;s coming up\u0026rdquo; \u0026ldquo;I\u0026rsquo;ve only been here for two years, but I\u0026rsquo;ve learned a lot and I want to keep learning and growing\u0026rdquo; ","href":"/writing/evergreen-career-progression/","title":"Career Progression"},{"content":"","href":"/tags/technical-leadership/","title":"Technical Leadership"},{"content":"In general, technical leader and staff engineer are similar roles, just named differently at various companies. Here, techinical lead(er) and staff engineer will be used interchangibly.\nGeneral Resources The Definition of a Tech Lead - Titles like Architect, Tech Lead, Team Lead and Engineering Manager provide endless confusion. This article explores the definition of the Tech Lead role The Well Rounded Architect - Acting as a Leader, Being a developer, Having a systems focus, Thinking like an entrepreneur, Balancing strategic with tactical thinking, Communicating well Thriving on the Technical Leadership Path - So what does the more strategic work of a very senior engineer look like? Engineering IC Leadership - Gitlab\u0026rsquo;s view on Technical Leadership Finding the steps on the individual contributor ladder - Some experiences, learnings and questions to help people decide if the individual contributor (IC) route is for them Becoming an Organizational Leader - Progressing from a team leader to an organizational leader as an individual contributor Your CTO Should Actually Be Technical - Why engineering leaders also need to be great engineers How do Individual Contributors Get Stuck? - A Primer by Camille Fournier Who are staff, principal, and distinguished engineers? by leaddev.com Staff Engineer StaffEng Tutorials - Guides for reaching and succeeding at Staff-plus roles Staff Engineer+ StaffEng-Plus Resources - Staff-plus resources Defining a Distinguished Engineer Build a network of peers - Guides/Build a network of peers Five must-reads for Staff+ engineers Beyond Staff Engineer by Alex Ewerl√∂f How To An incomplete list of skills senior engineers need, beyond coding - For varying levels of seniority, from senior, to staff, and beyond 10 Admirable Attributes of a Great Technical Lead - There is no single formula to be a great tech lead. It\u0026rsquo;s a demanding position. It requires both sides of one\u0026rsquo;s capability: the heart, and the mind How to Influence Without Authority - Understand first, influence later ","href":"/writing/evergreen-technical-leadership/","title":"Technical Leadership Resources"},{"content":"","href":"/categories/ipv6/","title":"IPv6"},{"content":" RIPE [Create an Addressing Plan] document contains recommendations for service providers requesting IPv6 address allocation plus links to several other documents. SURFnet [Preparing an IPv6 Addressing Plan Manual] covers most things one needs to know, including the use /64 everywhere recommendation. ISOC [IPv6 Address Planning: Guidelines and Resources] document goes even deeper. Definitely worth reading. ETSI [IPv6 Best Practices, Benefits, Transition Challenges and the Way Forward] APNIC [IPv6 Best Current Practices] covers IPv6 Addressing, transition technologies, guidelines for content, data centre, hosting, cloud service, and application service providers, enterprises. ANPIC [IPv6 architecture and subnetting guide for network engineers and operators] Join my email list to get direct access to new blog posts.\n","href":"/blogs/2023-04-30-ipv6-address-planning/","title":"IPv6 Address Planning Manual"},{"content":"Life Advices by Naval\nThese are notes to myself. Your frame of reference, and therefore your calculations, may vary. These are not definitions‚Äîthese are algorithms for success. Contributions are welcome.\nHappiness = Health + Wealth + Good Relationships Health = Exercise + Diet + Sleep Exercise = High Intensity Resistance Training + Sports + Rest Diet = Natural Foods + Intermittent Fasting + Plants Sleep = No alarms + 8‚Äì9 hours + Circadian rhythms Wealth = Income + Wealth * (Return on Investment) Income = Accountability + Leverage + Specific Knowledge Accountability = Personal Branding + Personal Platform + Taking Risk? Leverage = Capital + People + Intellectual Property Specific Knowledge = Knowing how to do something society cannot yet easily train other people to do Return on Investment = ‚ÄúBuy-and-Hold‚Äù + Valuation + Margin of Safety [72] Source: www.navalmanack.com/almanack-of-naval-ravikant/navals-writing\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2023-04-08-advices-by-naval/","title":"Best Advices by Naval"},{"content":"Internet Standard Bodies Internet Society (ISOC) - a non-profit organization founded in 1992 to provide leadership in Internet-related standards, education, and policy. ISOC is dedicated to ensuring the open development, evolution and use of the Internet and its architecture for the benefit of people throughout the world. IETF - Techincal Forum for ISOC. End Product is RFC, standards for internet protocols. Other policy and business aspects ICANN (Internet Corporation for Assigned Names and Numbers), IGF (Internet Governance Forum) and RIRs (Regional Internet Registries) like APNIC (Asia Pacific Network Information Centre), RIPE NCC (R√©seaux IP Europ√©ens Network Conrdination Centre) Network Operators NANOG - North American Network Operators\u0026rsquo; Group\nAPNOG - Asia Pacific Network Operators Group. There are two regional NOGs in the Asia Pacific Region.\nPacNOG - Pacific Islands (including American Samoa, Cook Islands, Federated States of Micronesia, Fiji, French Polynesia, Guam, Kiribati, Marshall Islands, Nauru, Niue, Northern Mariana Islands, New Caledonia, Palau, Papua New Guinea, Samoa, Solomon Islands, Tokelau, Tonga, Tuvalu, and Vanuatu) SANOG - South Asia (including Afghanistan, Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan, and Sri Lanka) Local NOGs Below is a list (with links to their websites) of the Network Operator Groups in the economies of the Asia Pacific Region.\nAusNOG - Australia bdNOG - Bangladesh btNOG - Bhutan KHNOG - Cambodia HKNOG - Hong Kong INNOG - India IDNOG - Indonesia JANOG - Japan KRNOG - Korea MyNOG - Malaysia MNNOG - Mongolia mmNOG - Myanmar npNOG - Nepal NZNOG - New Zealand PhNOG - Philippines SGNOG - Singapore LKNOG - Sri Lanka TWNOG - Taiwan ThaiNOG - Thailand VNiX-NOG - Vietnam Courtesy: https://apnog.org/ops/nogs.html India\u0026rsquo;s Initiative Indian IT Ministry (MeitY) - India‚Äôs nodal ministry for Internet Governance Indian Internet Research \u0026amp; Engineering Forum (IIREF) Indian IETF Capacity Building (IICB) National Internet Exchange of India (NIXI) India Internet Governance Forum (IIGF) - to build capacity and participate in Internet Governance organizations / fora like ICANN, IETF, APNIC, IGF etc. Internet Society Chapters - Delhi Kolkata Chennai Trivendrum Mumbai Hyderabad Fellowship Program Opportunity to participate annual events and meeting, training etc.\nInternet Society - Mid Career Fellowship ICANN Fellowship Program APNIC Fellowship Program RIPE Fellowship IIREF Fellowship - (for Indian) ","href":"/reading/internet-body-and-india/","title":"Internet Standard Body And India's Initiative"},{"content":"","href":"/tags/career/","title":"Career"},{"content":"How to Get Promoted at Work by Steve Dennis\n7 Tips\nUnderstand what you want Understand the role Understand the environment Tell impact stories Proactively seek feedback Avoid the victim trap Own it Join my email list to get direct access to new blog posts.\n","href":"/blogs/2023-03-05-how-to-get-promoted-by-steve-dennis/","title":"How to Get Promoted at Work | Steve Dennis"},{"content":"Networks, Buffers, and Drops - Deep Dive by ipspace.in\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2023-03-04-networks-buffers-drops/","title":"Networks, Buffers, and Drops"},{"content":"49 Questions to Improve Your Results by Josh Kaufman\nDo I use my body optimally? Do I know what I want? What am I afraid of? Is my mind clear and focused? Am I confident, relaxed, and productive? How do I perform best? What do I really need to be happy and fulfilled? Join my email list to get direct access to new blog posts.\n","href":"/blogs/2023-02-23-49-questions-to-improve-by-josh-kaufman/","title":"49 Questions to Improve Your Results | Josh Kaufman"},{"content":"How to create luck? by Swyx\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2023-02-23-how-to-create-luck-by-swyx/","title":"How to create luck"},{"content":"Best Articles by First Round\nHow to Become Insanely Well-Connected From BigCo to Startup: 20 Tips for Evaluating Early-Stage Companies \u0026amp; Making the Leap 40 Favorite Interview Questions from Some of the Sharpest Folks We Know The ‚ÄòAdaptable Leader‚Äô is the New Holy Grail ‚Äî Become One, Hire One The Best Interview Questions We\u0026rsquo;ve Ever Publishedhttps://review.firstround.com/the-best-interview-questions-weve-ever-published These 13 Exercises Will Prepare You for Work\u0026rsquo;s Toughest Situations The Secrets to Designing a Curiosity-Driven Career Don\u0026rsquo;t Just Network ‚Äî Build Your \u0026lsquo;Meaningful Network\u0026rsquo; to Maximize Your Impact First Round Resources\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2023-02-16-interesting-article-by-first-round-review/","title":"Best Articles by First Round Review"},{"content":"Resonating Articles by Sahil Bloom\nHow to Change Your Life in 1 Year How to Get Lucky The Power Speaking Guide The Ultimate Productivity Tool Life Lessons from 1,000 Years The Think Day: Your Secret Weapon Six thinking question* prompts I have found particularly useful (that you should steal!): What are your strongest beliefs? What would it take for you to change your mind on them? What are a few things that you know now that you wish you knew 5 years ago? How can you do less, but better? Are you hunting antelope or field mice? What actions were you engaged in 5 years ago that you cringe at today? What actions are you engaged in today that you will cringe at in 5 years? What would your 80-year-old self say about your decisions today? Career Advice That Doesn\u0026rsquo;t Suck Swallow the frog Observe your boss and figure out what they hate doing (their \u0026ldquo;frog\u0026rdquo;). Learn to do it. Take it off their plate (swallow their frog). Do the old fashioned things well Look people in the eye Do what you say you\u0026rsquo;ll do Be on time (or early!) Practice good posture Have a confident handshake Hold the door Be kind (never gossip!) Work hard first and smart later, Build storytelling skills Aggregating data, and Communicating it simply \u0026amp; effectively Build a rep for figuring it out Do some work Ask the key questions Get it done Show up early and stay late Dive through cracked doors. If someone cracks open a door that may present an opportunity, dive through it. Join my email list to get direct access to new blog posts.\n","href":"/blogs/2023-02-11-advices-by-sahil-bloom/","title":"Best Articles by Sahil Bloom"},{"content":"Introduction How to Read an RFC The Tao of IETF A Novice\u0026rsquo;s Guide to the Internet Engineering Task Force\nWriting An IETF Draft: Formatting, Authorship, And Submissions, How To Submit Your Ideas To The IETF Internet-Draft authors resource site, Getting started, Guidelines to Authors of Internet-Drafts Contributing to the IETF - How to play your part, and how RFCs are made IETF Drafting in Markdown Format Drafting in Markdown Syntax India Centric Technology Standard Drive Organization India Internet Engineering Society (IIESoc) Industry Network Technology Council (INTC) Appendix-I Internet Standard Body And India\u0026rsquo;s Initiative Appendix-II - Technology Manuals IPv6 Address Planning Manual ","href":"/reading/ietf/","title":"IETF"},{"content":"A Career Transition Article by Utkarsh Amitabh\nSummary:\nCareer transitions are like onions. They‚Äôre complex and there is usually a lot more to them than we see on the surface. Whether you are pursuing a passion or side hustle, confused about quitting your job for a new one, or just looking for a change, know that it‚Äôs not a straightforward decision. It requires careful planning and thinking through the why, the what, and the when.\nWhy do you want to change? Is it the culture of the organization, the people you work with, or is something else bogging you down? It is critical to be radically honest with yourself and think things through. What do you want to do? Conduct a self-assessment. It is impossible to know where you are going if you don‚Äôt know where you are. Then write down the steps you will need to take that will get you closer to your goals and the problems that you might encounter in doing that. When will the change happen? Be realistic about the time it can take. Some transitions are unlikely in the short-term. Don‚Äôt set yourself up for failure by setting unrealistic goals in unrealistic timeframes. \u0026ndash;\u0026gt; The Right Way to Make a Big Career Transition\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-12-28-make-a-big-career-transition-by-utkarsh-amitabh/","title":"Make A Big Career Transition by Utkarsh Amitabh"},{"content":"Resonating Articles by Benjamin Hardy\n\u0026ldquo;Impossible Goals\u0026rdquo; Are Easier and More Practical than \u0026ldquo;Possible Goals.\u0026rdquo; Here\u0026rsquo;s Why. Make Decisions Today That Will Impress Your Friends In 20 Years 10 Steps to Become a Millionaire in 5 Years (or Less) The Most Important Skill For Interacting With People 23 Smart Ways To Increase Your Confidence, Productivity, and Income How To Achieve Your 10-Year Plan In The Next 6 Months Why Most People Never Get What They Want Want To Become A Multi-Millionaire? Do These 15 Things Immediately. One Behavior Separates The Successful From The Average The Secret to Upgrading Your Entire Life (Money, Happiness, etc.) In 3-5 Years 8 Ways To Reach Your Goals Extremely Fast This Is How You Train Your Brain to Get What You Really Want How To Learn In 2 Days What Normally Takes 6 Months 21 Ways to Reach Your Dreams within 12 Months How to Turn Your Heroes Into Mentors and Make $$$ Doing What You Love One Thing Separates Creators From Consumers Tells about success Want To Become A Multi-Millionaire? Do These 14 Things Immediately. How to Train Your Brain to Get What You Want in 60 Days You Love What You Invest Yourself In 10 Steps to Become a Millionaire in 5 Years (or Less) Only 1% of Americans Do This Essential Daily Habit - Revisit Seting daily How to Become the Best in the World at What You Do You Train The World How To Treat You The 13-Minute Definitive Guide To Living Your Dreams How To 10X Your Income In 2023 How to become your \u0026ldquo;future self\u0026rdquo; Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-12-05-advices-by-benjamin-hardy/","title":"Motivating Psychology Articles by Benjamin Hardy"},{"content":"","href":"/tags/mental-model/","title":"Mental Model"},{"content":"","href":"/categories/mental-model/","title":"Mental Model"},{"content":"Mental Models These two places where you\u0026rsquo;ll find the most useful mental models.\nTools for better thinking (untools.co) Mental Model Examples (50+) That Will Make You More Successful (fronterablog.com) ","href":"/writing/evergreen-mental-models/","title":"Mental Models"},{"content":"APNIC article on SR TI-LFA by Ron Bonica.\nSR TI-LFA: Segment Routing and Topology Independent Loop-Free Alternates\nNANOG 79 Recording\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-09-25-ti-lfa/","title":"Article on SR TI-LFA"},{"content":"","href":"/categories/segment-routing/","title":"Segment Routing"},{"content":"Few career advices, I leant over the years\nBite Tips\nAlways learn Stay Humble You should be valued Be mindful to your risk tolerance Be discoverable Dont stay comfortable for too long Decisions are not permanent Run Marathon, not sprint People, not jobs, are permanent Try lots of different things Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-09-17-misc-advices/","title":"Misc Advices on Career"},{"content":" Athletes train. Musicians train. Performers train. But knowledge workers don‚Äôt. Learn Like an Athlete\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-08-20-learn-like-an-athlete/","title":"Learn Like an Athlete"},{"content":"notes to self Hard choices, easy life. Easy choices, hard life.\nThe deeper you go, the better it gets.\nYou\u0026rsquo;ll never regret a workout\nGift time, not things.\nEverything will be different in six months.\nWrite your goals down. Cross \u0026rsquo;em out. Write new ones.\nEmail the people you admire and tell them that. It\u0026rsquo;ll make their day. And it\u0026rsquo;ll make yours when they write back.\nLearning is a lifelong process.\nProductivity is 80% biology. Better health = better performance. Proper planning prevents poor performance. Use the weekend to build the life you want, not escape the one you have. Three hours of deep, focused work beats eight hours of \u0026rsquo;normal\u0026rsquo; work. Success isn\u0026rsquo;t about big, monumental actions. It\u0026rsquo;s about small daily actions that compound into big results over time.\n5 Habits highly effective people practice daily (that 99% of people don\u0026rsquo;t)\nHabit #1: Plan Your Day The Night Before Habit #2: Review Your Long-Term Goals Habit #3: Use The Morning For Deep Work Habit #4: Use The Afternoon For Managing Habit #5: Practice A 5-Minute Performance Review Radically change your life with \u0026ldquo;tiny habits\u0026rdquo; and \u0026ldquo;1% improvements\u0026rdquo; Small Actions + Consistency + Time = Significant Results For example:\nExercise 3-4x a week for six months, and you‚Äôll be in better shape Publish content daily for a year, and you‚Äôll have an online audience to monetize Save and invest $500 per month for a few years, and you‚Äôll have financial security Practice 10 minutes of daily meditation for three months, and you‚Äôll have mental clarity Write 500 words daily for six months, and you‚Äôll have an entire book to publish 4 Science-based habits to build a highly productive brain\nHabit #1: Manage Stress Habit #2: Prioritize Your Sleep Habit #3: Eat Brain-Boosting Foods Habit #4: Exercise Often The Focus Formula Summarized All in all, after years of experimenting with countless focus hacks, this is the focus formula I follow daily:\n10-14 hours of intermittent fasting 8 hours of sleep the night before Create a distraction-free environment Stay sufficiently hydrated Consume 100 - 300mg caffeine Listen to 40hz binaural beats Pick one high-priority task Work in 90-minute focus sessions 20-30 minutes break after focus session Start each day by identifying my top three priorities.\nOne task to elevate my business (write an article, create a new product, work on marketing, etc.) One task to elevate my health (lifting weights, going for a run, walking in nature, etc.) One task to elevate my finances (research potential investments, manage my finances, etc.) 12 most important principles for living a highly productive, fulfilling life.\nInaction breeds doubt and fear. Action breeds confidence and courage. Focus on being effective, not on being busy. There is nothing so useless as doing efficiently that which should not be done at all. Success isn\u0026rsquo;t about big, monumental actions. It\u0026rsquo;s about small daily actions that compound into big results over time. The morning is for making, the afternoon is for managing. Three hours of deep, focused work beats eight hours of \u0026rsquo;normal\u0026rsquo; work. Use the weekend to build the life you want, not escape the one you have. Proper planning prevents poor performance. Productivity is 80% biology. Better health = better performance. Measure your days not by the harvest you reap, but by the seeds that you plant. Don\u0026rsquo;t rely on motivation, but build self-discipline. Motivation is a temporary emotion, self-discipline is a habit. Complexity is the enemy of execution. Keep things as simple as possible. The key is not to prioritize what‚Äôs on your schedule, but to schedule your priorities. ","href":"/quotes/notes-to-self/","title":"notes-to-self"},{"content":"","href":"/tags/quotes/","title":"Quotes"},{"content":"","href":"/categories/quotes/","title":"Quotes"},{"content":"Best Podcast Episodes of all time Naval Ravikant on The Knowledge Project\nA deep dive on reading, happiness, decision making, habits, and mental models.\nElon Musk: Neuralink, AI, Autopilot, and the Pale Blue Dot\nA glimpse into what the future looks like, including Neuralink, AI, Autopilot, and the Pale Blue Dot.\nA deep dive into everything sleep by Matthew Walker\n\u0026ldquo;I think that sleep may be one of the most significant lifestyle factors that determine your risk ratio for Alzheimer\u0026rsquo;s disease.\u0026rdquo;\nBalaji Srinivasan on The Tim Ferriss Show.\nBalaji lives at least 20 years in the future‚Äîthis podcast gives you a sense of what that future looks like.\nHow to Cram 2 Months of Learning into 1 day\nJosh Waitzkin on The Tim Ferriss Show.\nKeith Rabois on the North Star Podcast. The future of education, how to find undiscovered talent in society, the power of accumulating advantages, and how to raise the level of ambition in society.\nAnnie Duke on the A16z Podcast. How to improve your long-term decision-making.\nYour body language may shape who you are | Amy Cuddy\nSocial Psychologist and ‚Äòmother of body language‚Äô Amy Cuddy teaches how the power of body language can change both self-perceived and outwardly confidence.\nHow great leaders inspire action | Simon Sinek\nSimon Sinek loves starting with the question of ‚ÄúWhy?‚Äù In his Talk, Sinek explores the model of the golden circle for inspirational and successful leadership.\nThe power of vulnerability | Bren√© Brown\nBren√© Brown‚Äôs personal story and journey of vulnerability is just the beginning. She is passionate about helping everyone discover the true meaning of what it means to belong, love and be vulnerable.\n1:40 - a piece of her research that expanded her perception 3:14 - connection 4:40 - shame 5:24 - vulnerability 6:53 - worthiness 8:05 - whole-hearted 8:45 - courage 9:43 - vulnerability (embracing it) 11:17 - a \u0026ldquo;little\u0026rdquo; breakdown 12:43 - vulnerability is the core of shame \u0026amp; fear but also the birthplace of joy/ creativity/ belonging/ love 13:51 - people embrace vulnerability, but that\u0026rsquo;s not her (lol) 14:31 - why do we struggle w/ vulnerability 14:39 - numb 16:21 - you can\u0026rsquo;t numb hard feelings w/o numbing other emotions 17:03 - making everything uncertain, certain 17:34 - perfection 18:31 - pretending How to make stress your friend | Kelly McGonigal | TED\nStress. It makes your heart pound, your breathing quicken and your forehead sweat. But while stress has been made into a public health enemy, new research suggests that stress may only be bad for you if you believe that to be the case. Psychologist Kelly McGonigal urges us to see stress as a positive, and introduces us to an unsung mechanism for stress reduction: reaching out to others.\n","href":"/reading/podcasts/","title":"Best Podcasts \u0026 TED Talks"},{"content":"","href":"/tags/podcast/","title":"Podcast"},{"content":"A Glimpse At Two Approaches To Segment Routing\nTopic Covered:\nSR-MPLS Vs. SRv6 SRv6 Adoption Challenges Hardware \u0026amp; Software Support For SRv6 Appendix:\nby Clarence Filsfils\nWhat is SRv6 network programming? SRv6: Deployed use-cases SRv6 micro-instructions Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-06-03-glimpse-of-segment-routing-approach/","title":"A Glimpse At Two Approaches To Segment Routing"},{"content":"The Joy of Small Projects\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-05-24-joy-of-small-projects/","title":"The Joy of Small Projects"},{"content":"Deliberate Practice by Jason Shen\nThe Complete Guide to Deliberate Practice\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-05-16-deliberate-practice/","title":"Deliberate Practice by Jason Shen"},{"content":"Bunch of unsolicited advices by Kevin Kelly\nAdvice Set I\nAdvice Set II\nAdvice Set III\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-05-11-advice-by-kevin-kelly/","title":"Bunch of unsolicited advices by Kevin Kelly"},{"content":"New discussion in IRTF - Introduction to Semantic Routing\nSee the recording at 47 min.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-04-18-semantic-routing/","title":"Semantic Routing - new concept"},{"content":"This is really awesome. New generation cmd line tools.\nA list of new(ish) command line tools compiled by Julia.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-04-17-list-of-newish-command-line-tools/","title":"A list of new(ish) command line tools"},{"content":"","href":"/tags/linux-tool/","title":"Linux Tool"},{"content":"In the job, if you do good work, it will be recognized. At the same time, it is important to remember, specially at the time of appraisal cycle. It will help you and your manager to present your case wisely. Instead of struggling to prepare a list of achievements when needed, it will be better to keep it updated once in every month. Julia Evans has written a good article on this.\nGet your work recognized: write a brag document\n\u0026lsquo;brag\u0026rsquo; means - \u0026rsquo;to talk too proudly about something\u0026rsquo;\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-04-04-get-your-work-recognized/","title":"Get your work recognized - write a brag document"},{"content":"Summary:\nRead the abstract (if provided)* Read the introduction. Read the conclusion. Skim the middle, looking at section titles, tables, figures, etc.‚Äîtry to get a feel for the style and flow of the article. Is it methodological, conceptual, theoretical (verbal or mathematical), empirical, or something else? Is it primarily a survey, a novel theoretical contribution, an empirical application of an existing theory or technique, a critique, or something else? Go back and read the whole thing quickly, skipping equations, most figures and tables. Go back and read the whole thing carefully, focusing on the sections or areas that seem most important. Appendix:\nHow to Read an Academic Article How to read a paper, mirror Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-03-23-how-to-read-a-paper/","title":"How to read a paper"},{"content":" In general, you might want to grow in your software engineering career, but:\nwhat does growth mean? how can you achieve it? who decides if you are ready to grow? where do you start?\nHow to achieve career growth: opportunities, skills \u0026amp; sponsors\nThis is a very insightful article. It clearly depicts the importance of self-awareness and drive the grown by yourself.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-03-19-how-to-achieve-career-growth/","title":"How to achieve career growth"},{"content":"Managing Yourself by Subbu Allamaraju\nSummary:\nYou‚Äôre the CEO of yourself ‚Äî a one-person enterprise ‚Äî act like one As a CEO, every day, you get to decide how you‚Äôre going to invest your time, which is your primary fixed capital. Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-03-16-managing-yourself/","title":"Managing Yourself"},{"content":"","href":"/tags/books/","title":"Books"},{"content":"","href":"/categories/books/","title":"Books"},{"content":"The IEEE 802.3 Ethernet standards have been evolved exponentially. This is one of the driving force for the massive scale data centres, which is driving our cloud.\nIEEE 802.3 Data Center Ethernet Optical Standards Year Ethernet Speed Designation Optical Data Rate Wavelength Fiber Count MMF(km) SMF (km) 1987 10 MbE FOIRL 12.5 M 850 2 1 - 1993 10 MbE 10BASE-FB 12.5 M 850 2 2 - 1993 10 MbE 10BASE-FL 12.5 M 850 2 2 - 1995 100 MbE 100BASE-FX 125 M 1310 2 2 - 1998 1 GbE 1000BASE-SX 1.25 G 850 2 0.550 - 1998 1 GbE 1000BASE-LX 1.25 G 1310 2 0.550 2 2002 10 GbE 10GBASE-SR 10 G 850 2 0.400 - 2002 10 GbE 10GBASE-LR 10 G 1310 2 - 10 2002 10 GbE 10GBASE-ER 10 G 1550 2 - 40 2002 10 GbE 10GBASE-LX4 3.125 G 10G-CWDM 2 0.300 10 2005 100 MbE 00BASE-BX10 125 M 1310/1550 1 - 10 2005 100 MbE 00BASE-LX10 125 M 1310 2 - 10 2005 1 GbE 1000BASE-BX10 1.25 G 1310/1550 1 - 10 2005 1 GbE 1000BASE-LX10 1.25 G 1310 2 - 10 2006 10 GbE 10GBASE-LRM 10 G 1310 2 0.220 - 2010 100 GbE 100GBASE-SR10 10 G 850 20 0.150 - 2010 40 GbE 40GBASE-SR4 10 G 850 8 0.150 - 2010 40 GbE 40GBASE-LR4 10 G CWDM 2 - 10 2010 40 GbE 40GBASE-ER4 10 G CWDM 2 - 40 2010 100 GbE 100GBASE-LR4 25 G LAN-WDM 2 - 10 2010 100 GbE 100GBASE-ER4 25 G LAN-WDM 2 - 40 2011 40 GbE 40GBASE-FR 40 G 1550 2 - 2 2015 100 GbE 100GBASE-SR4 25 G 850 8 0.100 - 2016 25 GbE 25GBASE-SR 25 G 850 2 0.100 - 2017 25 GbE 25GBASE-LR 25 G 1310 2 - 10 2017 25 GbE 25GBASE-ER 25 G 1310 2 - 40 2017 200 GbE 200GBASE-DR4 50 G 1310 8 - 0.500 2017 200 GbE 200GBASE-FR4 50 G CWDM 2 - 2 2017 200 GbE 200GBASE-LR4 50 G LAN-WDM 2 - 10 2017 400 GbE 400GBASE-FR8 50 G extLAN-WDM 2 - 2 2017 400 GbE 400GBASE-LR8 50 G extLAN-WDM 2 - 10 2017 400 GbE 400GBASE-SR16 25 G 850 32 0.100 - 2017 400 GbE 400GBASE-DR4 100 G 1310 8 - 0.500 2018 50 GbE 50GBASE-SR 50 G 850 2 0.100 - 2018 100 GbE 100GBASE-SR2 50 G 850 4 0.100 - 2018 200 GbE 200GBASE-SR4 50 G 850 8 0.100 - 2018 50 GbE 50GBASE-FR 50 G 1310 2 - 2 2018 50 GbE 50GBASE-LR 50 G 1310 2 - 10 2018 100 GbE 100GBASE-DR 100 G 1310 2 - 0.500 2020 400 GbE 400BASE-SR8 50 G 850 16 0.100 - 2020 400 GbE 400BASE-SR4.2 50 G 860/910 8 0.150 - Note: 10G-CWDM = (1275, 1300, 1325, 1350 nm); CWDM = coarse wavelength-division multiplexing (grid) = (1271, 1291, 1311, 1331 nm); LAN-WDM = local area network ‚Äì wavelength-division multiplexing (grid) = (1295.56, 1300.05, 1304.58, 1309.14 nm); ext(ended) LAN-WDM = (1273.54, 1277.89, 1282.26, 1286.66, 1295.56, 1300.05, 1304.58, 1309.14 nm)\nNon-IEEE 802.3 100 Gb Data Center Ethernet Optical Standards Designation Organization Fiber Wavelength Fiber count Max reach(km) Remarks ESR4 Consensus MMF 850 8 0.300 Extended-range SR4 SWDM4 MSA MMF SWDM 2 0.100 BI-DI Proprietary MMF 860/910 2 0.100 50G Technology PSM4 MSA SMF 1310 2 0.500 CWDM4 MSA SMF CWDM 2 2 CLR4 MSA SMF CWDM 2 2 No FEC required CWDM4/OCP MSA SMF CWDM 2 0.500 Temperature range (15‚Äì45 C) 4WDM-10 MSA SMF CWDM 2 10 4WDM-20 MSA SMF LAN-WDM 2 20 4WDM-40 MSA SMF LAN-WDM 2 40 ER4-lite Consensus SMF LAN-WDM 2 35 ColorZ Proprietary SMF DWDM 2 80 Metro data center interconnect; 50G Technology 100GBASE-FR MSA SMF 1310 2 2 100G technology 100GBASE-LR MSA SMF 1310 2 10 100G Technology Note: SWDM = short-wavelength-division multiplexing = (850, 880, 910, 940 nm). FEC = forward-error correction\nNon-IEEE 802.3 400 Gb Data Center Ethernet Optical Standards Designation Organization Fiber Wavelength (nm) Fiber count Max reach(km) Remarks 400GBASE-FR4 MSA SMF CWDM 2 2 100G Technology 400GBASE-LR4 MSA SMF CWDM 2 10 100G Technology 400G ZR OIF SMF DWDM 2 120 Metro data center interconnect; 400G coherent technology Join my email list to get direct access to new blog posts.\n","href":"/writing/2022-03-12-ethernet-optical-standards/","title":"Ethernet Optical Standards"},{"content":"Currently Reading Coming Soon. Engineering Books Computer Networks: A Systems Approach (The Morgan Kaufmann Series in Networking) | Online - It explores the key principles of computer networking, using real world examples from network and protocol design. Optical Springer Handbook of Optical Networks - This is a in-depth book on optical networking for all types of deployment. This is a bible. You have to use this as reference book. As of now, I have concentrated on the data center networking and its related chapters. Engineering Career Books Coming Soon Generic Books Coming Soon ","href":"/writing/evergreen-my-reading-list/","title":"My Reading List"},{"content":"Table of contents Indirect Topologies Tree Fat Tree Direct Topologies All-to-All HyperX Flattened Butterfly Topology Torus DCell \u0026amp; DCube The network topologies can be classified into tree and non-tree based topologies. This is very common observation in topology. There is another way to connect nodes are direct and indirect.\nIndirect Topologies Tree The tree topologies are simple and broadly used in data center networks. Tree topologies may have either two or three levels. In a two-level tree, switches at the core level connect the switches at the lower level, which are referred to as edges. Servers are located at the leaves and are connected to the switches at the edge level. In a three-level tree, an intermediate level called aggregation is added between the core and edge levels.\nTree is simple , but has multiple drawbacks, performance and reliability of switches should increase as we go higher in the topology levels, which would require different switches for different levels and, in turn, a move away from using commodity switches for such systems.\nFat Tree The fat tree is an extension to tree topologies. This is widely used in commercial data center networks. Fat trees were originally proposed by Leiserson in 1985. While there have been several attempts to formalize and use fat trees since then, we found the method used by Al-Fares et al. to be the most intuitive and comprehensive.\nSimilar to three-level tree topologies, fat trees also consist of three levels: core, aggregation, and edges. *Al-*Fares et al. adds the POD to the terminology, which consists of aggregation switches, edge switches, and servers. A k-ary fat tree contains (k/2)^2 k-port core switches and k PODs. Each core switch is connected to the aggregation layer of each POD through one port (the i-th port of any core switch is connected to POD i). Each POD has two layers of k=2 k-port switches. Switches in the upper layer (i.e., aggregation layer) are connected to core switches using k=2 of their ports. The remaining k=2 ports are used to connect to k=2 switches at the lower layer (i.e., edge layer). Each k-port switch at the edge layer uses k=2 of its ports to connect to the aggregation layer and the remaining ports to directly connect to k=2 hosts. Therefore, a k-ary fat-tree provides support for a total of k^3/4 hosts using k port switches.\nUnlike tree topologies, fat trees can use commodity off-the-shelf switches at all levels. Thus, fat trees offer more flexibility and can be more cost effective compared to tree topologies.\nDirect Topologies All-to-All The all-to-all topology is a fully mesh topology where all the TOR connected to each other. This is a simple topology, but has few limitations, like scalability of TOR switch ports. The network growth will be limited by number of ports on the TOR switch. Another limitation is cabling, high number of cabling required.\nHyperX The HyperX topology, introduced for the first time by Ahn et al., can be derived from the all-to-all topology by dividing the nodes into groups (or dimensions) and by removing some intergroup links. Within a group, all nodes are fully connected. Between the groups, each node connects to all the nodes in the other groups with the same positions or coordinates. This architecture allows us to increase the scalability of the system at the expense of a larger diameter and a higher average hop count compared to the all-to-all.\nIn the example below, the ToR switch has four ports for inter-rack communication. This would allow us to interconnect only five racks in a full-mesh topology. By using HyperX, up to eight or nine racks can be interconnected at the expense of increasing the network diameter.\nJ.H. Ahn, N. Binkert, A. Davis, M. McLaren, R.S. Schreiber: HyperX: Topology, routing, and packaging of efficient large-scale networks. In: Proc. Conf. High Perform. Comput. Netw. Storage Anal., Portland (2009)\nFlattened Butterfly Topology Another popular topology is the flattened butterfly (FB) topology [23.42, 43], typically represented in rows and columns, as shown in Fig. 23.8. Each node (or ToR) is fully connected with all the nodes in the same row and column. In practice, we can consider the rows and columns as two different dimensions of a HyperX.\nJ. Kim, W.J. Dally, D. Abts: Flattened butterfly: a cost-efficient topology for high-radix networks, SIGARCH Comput. Archit. News 35, 126‚Äì137 (2007) Local Copy\nW. Dally, B. Towles: Principles and Practices of In- terconnection Networks (Morgan Kaufmann, New York 2003)\nTorus The torus topology can also be derived from the HyperX topology when assuming that each dimension is not fully connected but simply interconnected based on a ring topology.\nW. Dally, B. Towles: Principles and Practices of In- terconnection Networks (Morgan Kaufmann, New York 2003)\nW.J. Dally: Performance analysis of k-ary n-cube interconnection networks, IEEE Trans. Comput. 39, 775‚Äì785 (1990)\nDCell \u0026amp; DCube There are another two topology for data center networks.\nDCell: C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, S. Lu: DCell: a scalable and fault-tolerant network structure for data centers, ACM SIGCOMM Comput. Commun. 38(4), 75‚Äì86 (2008) Local Copy\nDCube: G. Wang, D.G. Andersen, M. Kaminsky, M. Kozuch, T. Ng, K. Papagiannaki, M. Glick, L. Mummert: Your data center is a router: The case for reconfigurable optical circuit switched paths, ACM SIGCOMM Com- put. Commun. Rev. SIGCOMM 40(4), 327‚Äì338 (2010) Local Copy\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2022-02-26-dc-topologies/","title":"Data Center Topologies"},{"content":"Optimizing Workspace for Productivity, Focus, \u0026amp; Creativity by Juan Pablo Aranovich\nSummary:\nBright lights during phase 1 of the day. Place visual focus direct in front of you. Restricted visual window. Put screens at nose level (at least). Avoid reclining Half stand and half sited work. Avoid white noise. Pursue 40 hertz binaural beats during work bout or prior to work bout. Stationary tread mill increases alertness and focus. Cathedral effect: focused and analytical work: low ceiling space. Creative work: high ceiling environment. Limit interruptions. Changing environments might be beneficial. Join my email list to get direct access to new blog posts.\n","href":"/blogs/2022-02-25-optimizing-workspace-for-productivity-focus-creativity/","title":"Optimizing Workspace For Productivity, Focus, \u0026 Creativity"},{"content":"SCION vs. Segment Routing\nBook, papers, videos, tutorials - https://www.scion-architecture.net\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2022-02-14-scion-segment-routing/","title":"SCION vs. Segment Routing"},{"content":"","href":"/tags/blogroll/","title":"Blogroll"},{"content":"","href":"/categories/blogroll/","title":"Blogroll"},{"content":"People \u0026amp; Blogs Paul Graham - Essays Marc Andreessen - Pmarca Guide to Personal Productivity Morgan Housel Naval Ravikant Sam Altman - Idea Generation, How To Be Successful, The days are long but the decades are short Julian Shapiro - Blogs Sriram Krishnan Ryan Hoover Patrick Collison David Perell Tiago Forte Will Larson - Work on what matters, Systems Thinking, Some career advice, Resources for Staff-plus engineers, How to be a tech influencer, A forty-year career Gergely Orosz Julia Evans Elad Gil - \u0026ldquo;Career Decisions\u0026rdquo; (in Silicon Valley / tech) Ben Thompson - Every business school should have a course on Aggregation Theory. Or learn it from the master himself), the best analyst in technology. Andrew Wilkinson - ‚ÄúLazy Leadership‚Äù Dan Luu- What to learn Subbu Allamaraju - Managing Yourself, On Influencing, Mid Career Stuckness Tom Bilyeu - Impact Theory Leo Babauta - zen habits Vanessa Van Edwards - Science of People Seth Gobin - Seth\u0026rsquo;s blog Tiny Buddha by Lori Deschene James Clear - James Clear Luis M. Contreras - IETF Sahil Bloom - Best Articles by Sahil Bloom Newsletters pointer.io - What Current \u0026amp; Future Engineering Leaders Read The Pragmatic Engineer System Sunday - Build systems for health, wealth, \u0026amp; free time. The Quiet Rich - You don\u0026rsquo;t just want to be rich in money. You also want to be \u0026ldquo;rich\u0026rdquo; in time. Appendix: related posts\nBlogroll by Julia Evans Appendix: Interesting links\nA Library Of 5 Minute Reads To Grow Your Startup (marketingexamined.com) ","href":"/reading/blogroll/","title":"Blogroll"},{"content":"Books/Blogs My Reading List Technology Books Programming Blogs Mental Models Books \u0026amp; Paper Recommendation by smart folks ","href":"/reading/books-blogs/","title":"Books \u0026 Blogs"},{"content":"Interesting Papers Knowledge-Defined Networking public copy\nThe research community has considered in the past the application of Artificial Intelligence (AI) techniques to control and operate networks. A notable example is the Knowledge Plane proposed by D.Clark et al. However, such techniques have not been extensively prototyped or deployed in the held yet. In this paper, we explore the reasons for the lack of adoption and posit that the rise of two recent paradigms: Software-Defined Networking (SDN) and Network Analytics (NA), will facilitate the adoption of AI techniques in the context of network operation and control. We describe a new paradigm that accommodates and exploits SDN. NA and AI. and provide use-cases that illustrate its applicability and benefits. We also present simple experimental results that support, for some relevant use-cases, its feasibility. We refer to this new paradigm as Knowledge-Defined Networking (KDN).\nJupiter Rising: A Decade of Clos Topologies and Centralized Control in Google‚Äôs Datacenter Network public copy\nWe present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth.\nITU-T Rec. Technical Report 2020 FG NET2030 Network 2030, mirror\nThis technical specification begins describing architectural principles and overall architecture for public networks in the year 2030 and beyond, namely Network 2030.\nAppendix-I: related posts Best Paper Awards in Computer Science Papers We Love Lobste.rs tagged as PDF ACM SIGOPS Hall of Fame Award list Appendix-II: How to Read an Academic Article by Peter Klein How to Read a Paper by S. Keshav Appendix-III: Misc What Happens When A CPU Starts ","href":"/reading/engg-papers/","title":"Engg Papers"},{"content":"","href":"/tags/engg-paper/","title":"Engg-Paper"},{"content":"","href":"/categories/engg-paper/","title":"Engg-Paper"},{"content":"","href":"/tags/newsletter/","title":"Newsletter"},{"content":"","href":"/categories/newsletter/","title":"Newsletter"},{"content":"Topics in my writing backlog:\nHow internet really works? Distributed systems in networking box Why C language is still exists and its competitor How do you measure your impact on the industry? Google\u0026rsquo;s Network Overview - Data Center(Jupiter), Backbone(B4), Control Plane(Orion), SDN controller for Google‚Äôs peering edge network(Espresso), Load Balancer(Maglev) Google‚Äôs data center network infrastructure: Jupiter Fabric: This is the internal network within Google‚Äôs data centers, operating at speeds of 40 Gbps per link and capable of handling a total bandwidth of 1 Petabit per second (Pbps). It leverages Software-Defined Networking (SDN) for centralized control and management. WAN B4: This is Google‚Äôs wide-area network (WAN) that connects its data centers globally. It also utilizes SDN for efficient traffic routing and boasts high throughput in the terabit range (Tbps). B2: This network segment connects Google‚Äôs data centers to the global internet backbone with very high Service Level Agreements (SLAs) for guaranteed performance and reliability for user-facing traffic entering the Google network Espresso: This is the SDN controller for Google‚Äôs peering edge network. It dynamically selects the most efficient routes to deliver customer traffic based on real-time measurements of availability and latency. Orion: This is the SDN platform deployed across both Jupiter fabrics and the WAN B4 network for Google‚Äôs entire network, not just GCP. It acts as the central control point for provisioning, configuring, and managing network resources.it acts as the control plane, telling the network equipment (switches, routers) how to route traffic and manage overall network behavior. Think of Orion as the central traffic control system, directing data flow across the entire Google network. Andromeda: This is the SDN platform focuses on network virtualization. It‚Äôs an internal system within Google Cloud Platform (GCP) that creates and manages virtual networks on top of the physical network infrastructure. Imagine it as carving out dedicated virtual highways within a massive physical road network for GCP users. Google\u0026rsquo;s writeup: Year 2013 B4: Experience with a Globally Deployed Software Defined WAN. Read this 2013 paper for a detailed look at Google‚Äôs quest for simpler and more efficient WAN. Year 2015 Jupiter A look inside Google‚Äôs Data Center Networks by Amit Vahdat Year 2015 Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google‚Äôs Datacenter Network Year 2022 Jupiter evolving: Reflecting on Google‚Äôs data center network transformation Year 2014 Andromeda: Google Cloud Platform\u0026rsquo;s latest networking stack Year 2021 Orion: Google\u0026rsquo;s Software-Defined Networking Control Plane Year 2017 Espresso makes Google cloud faster Year 2016 Maglev: Google shares software network load balancer design powering GCP networking Year 2022 Extra: Google\u0026rsquo;s subsea fiber optics, explained Networking Slicing and IETF Initatives Description of Network Slicing Concept, An Overview of Network Slicing Efforts in The IETF ","href":"/writing/writing-backlog/","title":"Writing backlog"},{"content":"","href":"/tags/gnmi/","title":"GNMI"},{"content":"In the last article, NETCONF vs RESTCONF I have explained the difference between netconf and restconf and their use-cases. Netconf has very rich functionality, whereas restconf has matching functionality. Restconf has opened the door for the framework with REST-based system and infrastructure.\nThe gNMI (gRPC Network Management Interface) protocol comes from the openconfig consortium, a group of network operators led by Google, with the following mission - \u0026ldquo;OpenConfig is an informal working group of network operators sharing the goal of moving our networks toward a more dynamic, programmable infrastructure by adopting software-defined networking principles such as declarative configuration and model-driven management and operations\u0026rdquo;.\nThe initial focus for openconfig was to develop vendor-neutral yang models for operator needs. It started with Google\u0026rsquo;s requirements, later all the operators joined hands. Right now, this is the most active group to add new data models. The group also developed gNMI as a management protocol for configuration and streaming telemetry management. Note that gNMI is based on gRPC as a transport protocol. The gNMI supports more encoding, not just protobuf [more details below]. The protobuf is more compact on the wire, so performance-wise more advantageous. But, operationally it is more complex. Also the distribution of proto files is more challenging, especially upgrade scenarios. It adds more complexities.\nGNMI Interface: service gNMI { rpc Capabilities(CapabilityRequest) returns (CapabilityResponse); rpc Get(GetRequest) returns (GetResponse); rpc Set(SetRequest) returns (SetResponse); rpc Subscribe(stream SubscribeRequest) returns (stream SubscribeResponse); } gNMI NETCONF RESTCONF Serialization Protobuf or JSON XML XML or JSON Transport gRPC (HTTP/2.0) SSH HTTP/TLS/TCP/IP Diff oriented Yes Returns only elements of the tree that have changed from last read No Always returns entire sub-tree snapshot No streaming telemetry Yes NetConf notification No Operation semantics gNMI specific. Single-target, single-shot, sequenced transactions. NETCONF specific; networkwide transactions. RESTCONF specific, based on HTTP verbs. Single-target, single-shot transactions. ","href":"/writing/2022-01-31-netconf-vs-gnmi/","title":"NETCONF vs GNMI"},{"content":"Here are few book recommendations by smart people. Book Recommendations, Holiday Book Recommendations by Gergely Orosz\nBook Recommendations till 2018, 2019 - 2022 reading recap by Will Larson\nLeadership Library by Philip Paetz\nPaper recommendations by smart people. Papers We Love Technical Papers by Will Larson ","href":"/writing/evergreen-books-reco-by-smart-folks/","title":"Book \u0026 Paper Recommendations by smart folks"},{"content":"Programming Books Skiena; The Algorithm Design Manual The longer, more comprehensive, more practical. It\u0026rsquo;s similar in that it attempts to teach you how to identify problems, use the correct algorithm, and give a clear explanation of the algorithm. This also describess the real world programming.\nMcDowell; Cracking the Coding Interview Some problems and solutions, with explanations, matching the level of questions you see in entry-level interviews at Google, Facebook, Microsoft, etc. I usually recommend this book to people who want to pass interviews but not really learn about algorithms. It has just enough to get by, but doesn\u0026rsquo;t really teach you the why behind anything. If you want to actually learn about algorithms and data structures, see below.\nCLRS; Introduction to Algorithms This book somehow manages to make it into half of these ‚ÄúN books all programmers must read‚Äù lists despite being so comprehensive and rigorous that almost no practitioners actually read the entire thing. It\u0026rsquo;s great as a textbook for an algorithms class, where you get a selection of topics. As a class textbook, it\u0026rsquo;s nice bonus that it has exercises that are hard enough that they can be used for graduate level classes.\nDemaine; Advanced Data Structures This is a set of lectures and notes and not a book, but if you want a coherent (but not intractably comprehensive) set of material on data structures that you\u0026rsquo;re unlikely to see in most undergraduate courses, this is great. The notes aren\u0026rsquo;t designed to be standalone, so you\u0026rsquo;ll want to watch the videos if you haven\u0026rsquo;t already seen this material.\nOperating Systems Silberchatz, Galvin, and Gagne; Operating System Concepts It covers concepts at a high level and hits the major points, but it\u0026rsquo;s lacking in technical depth, details on how things work, advanced topics, and clear exposition.\nCox, Kasshoek, and Morris; book, source This book is great! It explains how you can actually implement things in a real system, and it comes with its own implementation of an OS that you can play with. By design, the authors favor simple implementations over optimized ones, so the algorithms and data structures used are often quite different than what you see in production systems.\nThis book goes well when paired with a book that talks about how more modern operating systems work, like Linux or Windows.\nNetworking Beej\u0026rsquo;s Guide to Network Programming Using Internet Sockets\nBeej\u0026rsquo;s Guide to Networking Concepts All the networking concepts are all in one place for beginner and intermediate.\nBeej\u0026rsquo;s Guide to Interprocess Communcation All IPC in Unix/Linux Environment\nBeej\u0026rsquo;s Guide to Git Beej\u0026rsquo;s Guide to Python Programming‚Äîfor Beginners! A collaborative IPv6 book by Nick Buraglio and Brian E. Carpenter, pdf copy, github Appendix: related posts If you would like to go deeper and comprehensive knowledge on programming, there are reference by\nProgramming book list by Dan Luu Book Recommendations by Gergely Orosz Freely available programming books ","href":"/writing/evergreen-programming-books/","title":"Books"},{"content":"","href":"/tags/git/","title":"GIT"},{"content":"","href":"/tags/ipv6/","title":"IPv6"},{"content":"Eli Bendersky This guy is allrounder, this is a collection of various topic. ‚Äúthe C++ blog‚Äù - very good.\nAppendix: related posts Programming Blog by Dan Luu Appendix: Programming Language Benchmarks Programming Language and compiler Benchmarks [Source Code] Which programming language is fastest? [Source Code] ","href":"/writing/evergreen-programming-blogs/","title":"Programming Blogs"},{"content":"","href":"/tags/programming-blogs/","title":"Programming-Blogs"},{"content":"","href":"/categories/programming-blogs/","title":"Programming-Blogs"},{"content":"","href":"/tags/programming-books/","title":"Programming-Books"},{"content":"","href":"/categories/programming-books/","title":"Programming-Books"},{"content":"","href":"/tags/python/","title":"Python"},{"content":"\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-12-15-excercise-desk-stretches/","title":"Exercise - Daily Desk Stretches"},{"content":"ISP Design Guide: Separation of network functions ‚Äì introduction and overview by Kevin Myers\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-11-24-isp-design-guide/","title":"ISP Design Guide"},{"content":"NETCONF vs RESTCONF\nNETCONF is a protocol that was developed to provide a standardized interface to Network Element(NE)/Device to retrieve and manipulate configuration data.\nTill now, CLI is the standard interface to interact with NE. but this is mainly for human. On the other hand, the NETCONF is mainly used for network automation.\nCLI normally defined for human in user friendly way.\nContent Config, Oper Data Operations \u0026lt;get-config\u0026gt;, \u0026lt;edit-config\u0026gt;, \u0026lt;copy-config\u0026gt;, \u0026lt;delete-config\u0026gt;, \u0026lt;lock\u0026gt;, \u0026lt;unlock\u0026gt;, \u0026lt;close-session\u0026gt;, \u0026lt;kill-session\u0026gt; Messages \u0026lt;rpc\u0026gt;, \u0026lt;rpc-reply\u0026gt;, \u0026lt;notification\u0026gt; Secure Transport SSH, TLS RESTCONF is an HTTP-based protocol that provides a programmatic interface for accessing data defined in YANG, using the datastore concepts defined in NETCONF.\nContent: {+restconf}/data Config, Oper Data; Data-combined configuration and state data resources Operations: {+restconf}/operations operations-container that provides access to the data-model- specific RPC operations supported by the server Secure Transport HTTPS Why do we need RESTCONF when we already have NETCONF?\nFirst of all, we need to think about the trends of the tech industry. It\u0026rsquo;s no secret that businesses all around the world are digitizing themselves through automation. And what is the most popular protocol used in the modern era for automation?\nThe answer to that is HTTP-based REST APIs.\nImagine a company invested a huge sum of money on automating it\u0026rsquo;s business processes through REST APIs. And once that work was complete, they decided to automate their network configuration management. They could easily hit a roadblock at this point. The software they use for automation would need to support the NETCONF protocol. Also, their software developers would need to be skilled up to learn this protocol.\nYou can see the dilemma here. Businesses want to keep their solutions as simple as possible. Ideally, they would use common applications and protocols across their organization.\nThis is where RESTCONF comes to the rescue. It allows users to automate their network infrastructure using familiar RESTful API patterns.\nOperations Similarities:\nNow that we understand the need for RESTCONF, let\u0026rsquo;s do a side-by-side comparison of some of the key lower-level differences between the protocols.\nHOW ARE CRUD (CREATE, READ, UPDATE, DELETE) OPERATIONS DIFFERENTIATED IN A REQUEST?\nRESTCONF NETCONF OPTIONS None HEAD \u0026lt;get-config\u0026gt;, \u0026lt;get\u0026gt; GET \u0026lt;get-config\u0026gt;, \u0026lt;get\u0026gt; POST \u0026lt;edit-config\u0026gt; (nc:operation=\u0026ldquo;create\u0026rdquo;) POST Invoke an RPC operation PUT \u0026lt;copy-config\u0026gt; (PUT on datastore) PUT \u0026lt;edit-config\u0026gt; (nc:operation=\u0026ldquo;create/replace\u0026rdquo;) PATCH \u0026lt;edit-config\u0026gt; (nc:operation depends on patch content) DELETE \u0026lt;edit-config\u0026gt; (nc:operation=\u0026ldquo;delete\u0026rdquo;) WHAT FORMAT DOES DATA ENCODING USE?\nThe NETCONF protocol requires RPC messages to always be encoded with XML. The RESTCONF protocol allows data to be encoded with either XML or JSON.\nHOW IS CONFIGURATION DATA AND STATE DATA EXPOSED TO CLIENTS?\nA NETCONF client will use multiple levels of depth in an XML message to specify the location of data. A RESTCONF client with use the resource path in the URL of the request to specify the location of data.\nRESTCONF is Less\nNo locking No notion of transaction No candidate config No two-phase commit No Copy-config References\nNETCONF and YANG Tutorial part 1a: NETCONF and YANG Overview NETCONF - rfc6241 RESTCONF \u0026ndash; rfc8040 Link to IETF 94 Recording: NETCONF, YANG, pyang Link to slides at IETF 94: NETCONF Slides, YANG Slides ","href":"/writing/2021-11-07-netconf-vs-restconf/","title":"NETCONF vs RESTCONF"},{"content":"","href":"/tags/restconf/","title":"Restconf"},{"content":"What is APIs?\nApplication Programming Interfaces (APIs) are module: they¬†take inputs and give you predictable outputs.¬†At its core, an API is a¬†bunch of code¬†that takes an¬†input¬†and gives you an¬†output\nSometimes, companies will make parts of their APIs¬†publicly available, like github,¬†Twitter¬†or¬†Google Maps\nIn modern era, your favourite applications are collection of APIs.\nIn WWW, the programmatic service is provided by the APIs. User/Automation access this service using the API. Whatever the way the service is implemented, it is abstracted from user. So, automation can use this API for their broader usage.\nGenerally, there are two types of APIs. Internal and Public. Internal APIs are used by companies for communicating with their internal applications. They are not open to public. Whereas, companies open ups their dataset with APIs. Interested users can do interesting things with it. A good example is twitter API, which lets people like you interact with tweet data programmatically.\nAPI implementations: SOAP, REST, GRPC\nThere are three major protocols/technologies that developers use to build APIs: SOAP, REST and GRPC.\n1. SOAP:\nSOAP (Simple Object Access Protocol) is the oldest of the API protocols. It was¬†originally released in 1998¬†by a few Microsoft engineers. It\u0026rsquo;s based on XML. Here\u0026rsquo;s an example of what kind of code you\u0026rsquo;d need to write to work with it:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;soap:Envelope xmlns:soap=\u0026#34;http://www.w3.org/2003/05/soap-envelope\u0026#34; xmlns:m=\u0026#34;http://www.example.org\u0026#34;\u0026gt; \u0026lt;soap:Header\u0026gt; \u0026lt;/soap:Header\u0026gt; \u0026lt;soap:Body\u0026gt; \u0026lt;m:GetStockPrice\u0026gt; \u0026lt;m:StockName\u0026gt;T\u0026lt;/m:StockName\u0026gt; \u0026lt;/m:GetStockPrice\u0026gt; \u0026lt;/soap:Body\u0026gt; \u0026lt;/soap:Envelope\u0026gt; This is legacy technology now. Companies are moving to new technology now.\n2. REST\nREST API stands for Representational State Transfer and is an architectural pattern for creating web services. It was developed by Roy Fielding in 2000 and has led to a growing collection of RESTful web services that follow the REST principles.\nREST is a ruleset that defines best practices for sharing data between clients and the server. It\u0026rsquo;s essentially a design style used when creating HTTP or other APIs that asks you to use CRUD(create/read/update/delete) functions only, regardless of the complexity.\nREST applications use HTTP methods like¬†GET,¬†POST,¬†DELETE, and¬†PUT. REST emphasizes the scalability of components and the simplicity of interfaces. Not all HTTP APIs are REST APIs. The API needs to meet the following architectural requirements to be considered a REST API:\nA uniform interface:¬†An API must expose specific application resources to API consumers. Client-server independence:¬†The client and the server function independently. The client will only know the URIs that point to the application\u0026rsquo;s resources. These are usually published in the API documentation. Stateless:¬†The server doesn\u0026rsquo;t save any data pertaining to the client request. The client saves this \u0026ldquo;state data\u0026rdquo; on its end (via a cache).¬†Learn more about stateful vs. stateless systems here. Cacheable:¬†Application resources exposed by the API need to be cacheable. Layered:¬†The architecture is layered, which allows different components to be maintained on different servers. Code-on-Demand (COD):¬†This is the only optional REST constraint. This allows the client to receive executable code as a response from the server. In other words, it\u0026rsquo;s the server that determines how specific things get done. 3. gRPC\ngRPC (Remote Procedure Call) is made mostly for distributed systems that optimize for scale and low latency. gRPC is an open-source RPC architecture designed by Google to achieve high-speed communication between microservices. gRPC allows developers to integrate services programmed in different languages. gRPC uses the Protobuf (protocol buffers) messaging format, which is a highly-packed, highly-efficient messaging format for serializing structured data. For a specific set of use-cases, a gRPC API can serve as a more efficient alternative to a REST API.\nBASICS OF AN REST API REQUEST: A. The endpoint\nThe endpoint¬†(or route) is the url you request for. It follows this structure:\nroot-endpoint/?\nThe¬†root-endpoint¬†is the starting point of the API you\u0026rsquo;re requesting from. The root-endpoint of¬†Github\u0026rsquo;s API¬†is¬†https://api.github.com¬†while the root-endpoint¬†Twitter\u0026rsquo;s API¬†is¬†https://api.twitter.com.\nB. The method\nThe method is the type of request you send to the server. You can choose from these five types below:\nGET This request is used to get a resource from a server. POST This request is used to create a new resource on a server. If you perform a `POST` request, the server creates a new entry in the database and tells you whether the creation is successful. In other words, a `POST` request performs an `CREATE` operation. PUT \u0026amp; PATCH These two requests are used to update a resource on a server. If you perform a `PUT` or `PATCH` request, the server updates an entry in the database and tells you whether the update is successful. In other words, a `PUT` or `PATCH` request performs an `UPDATE` operation DELETE This request is used to delete a resource from a server. C. The Headers\nHeaders are used to provide information to both the client and server. It can be used for many purposes, such as authentication and providing information about the body content. HTTP Headers are property-value pairs¬†that are separated by a colon. The example below shows a header that tells the server to expect JSON content.\n{ Content-Type: text/html; charset=UTF-8, Authorization: Basic YWxhZGRpbjpvcGVuc2VzYW1l } D. The Body\nThe data (sometimes called \u0026ldquo;body\u0026rdquo; or \u0026ldquo;message\u0026rdquo;) contains information you want to be sent to the server. This option is only used with¬†POST,¬†PUT,¬†PATCH¬†or¬†DELETE¬†requests.\n{ user_id: 34, order_date: \u0026#34;2020-01-01\u0026#34;, order_value: \u0026#34;99.99\u0026#34; } Request bodies are generally formatted as JSON (this format of¬†name:value).\nWhere and how to call APIs\na. Browsers\nThe endpoint to get a list of my repos on Github is this: https://api.github.com/users/prmanna/repos\nIf you\u0026rsquo;d like to get a list the repositories that I pushed to recently, you can set¬†sort¬†to¬†push. https://api.github.com/users/prmanna/repos?sort=pushed\nb. Command Line\ncURL¬†is a popular command line tool for making API requests: you just choose your method and endpoint, and then attach any extra data like headers or a request body.\nc. Client Libraries\nInternal and vendor APIs often ship with built in functions for specific programming languages. If your company uses mostly Python for the backend, they might create a library to interact with their APIs and build functions like¬†listOrders()¬†that you can use directly.\nd. IDEs\nLike with the rest of the programming ecosystem, there are specialized tools for querying REST endpoints. The most popular one out there is¬†Postman. it organizes your request into simple form fields, helps autocomplete your headers and body, and lets you store credentials and settings.\nThere are few more alternative for API implementations, like, GraphQL, Thrift, via queuing like RabbitMQ,\nIn the next article, I\u0026rsquo;ll cover the device management/automation using Netconf, restconf.\n","href":"/writing/2021-11-06-what-is-api/","title":"What is APIs?"},{"content":"","href":"/tags/hashing/","title":"Hashing"},{"content":"These diagrams are self sufficient to explain.\nTable of contents 1. Web Scaler 2. Consistent Hashing 3. Web Crawler 4. Unique ID Generator for Distributed Systems 5. Key Value Store 6. Rate Limiter 7. Youtube 8. Chat System 9. News Feed System 10. Google Drive 1. Web Scaler 2. Consistent Hashing 3. Web Crawler 4. Unique ID Generator for Distributed Systems 5. Key Value Store Ref: SSTable\n6. Rate Limiter 7. Youtube 8. Chat System 9. News Feed System 10. Google Drive Join my email list to get direct access to new blog posts.\n","href":"/writing/2021-10-10-web-scaler/","title":"Web Scaler - Diagrams"},{"content":"","href":"/tags/webscaler/","title":"Webscaler"},{"content":"","href":"/tags/c++/","title":"C++"},{"content":"Best Ever C++ Tutoriali for C Programmer\nCode Snip: Part1 | Part2 | Part3\nData Types | Arithmetic | If Statement | Switch Statement | Ternary Operator | Arrays | For Loop | While Loop | Do While Loop | User Input | Convert String | Strings | Vectors | Functions | Recursive Function | File I/O | Exception Handling | Pointers | Reference Operator | Classes / Objects | Private | Static Variables | Public / Encapsulation | Constructors | Static Functions | this | Inheritance | Call Superclass Constructor | Execute Static Method | Virtual Methods | Polymorphism | Abstract Data Type\n#include \u0026lt;string\u0026gt; More details Here\nOperations on strings\ngetline() :- This function is used to store a stream of characters as entered by the user in the object memory. push_back() :- This function is used to input a character at the end of the string. pop_back() :- Introduced from C++11(for strings), this function is used to delete the last character from the string. Capacity Functions\nlength():-This function finds the length of the string resize() :- This function changes the size of string, the size can be increased or decreased. capacity() :- This function returns the capacity allocated to the string, which can be equal to or more than the size of the string. Additional space is allocated so that when the new characters are added to the string, the operations can be done efficiently. shrink_to_fit() :- This function decreases the capacity of the string and makes it equal to the minimum capacity of the string. This operation is useful to save additional memory if we are sure that no further addition of characters have to be made. Iterator Functions\nbegin() :- This function returns an iterator to beginning of the string. end() :- This function returns an iterator to end of the string. rbegin() :- This function returns a reverse iterator pointing at the end of string. rend() :- This function returns a reverse iterator pointing at beginning of string. Manipulating Functions\ncopy(‚Äúchar array‚Äù, len, pos) :- This function copies the substring in target character array mentioned in its arguments. It takes 3 arguments, target char array, length to be copied and starting position in string to start copying. #include \u0026lt;vector\u0026gt; More details Here\nIterators Functions\nbegin() ‚Äì Returns an iterator pointing to the first element in the vector end() ‚Äì Returns an iterator pointing to the theoretical element that follows the last element in the vector rbegin() ‚Äì Returns a reverse iterator pointing to the last element in the vector (reverse beginning). It moves from last to first element rend() ‚Äì Returns a reverse iterator pointing to the theoretical element preceding the first element in the vector (considered as reverse end) Capacity Functions\nempty() ‚Äì Returns whether the container is empty. size() ‚Äì Returns the number of elements in the vector. capacity() ‚Äì Returns the size of the storage space currently allocated to the vector expressed as number of elements. resize(n) ‚Äì Resizes the container so that it contains ‚Äòn‚Äô elements. shrink_to_fit() ‚Äì Reduces the capacity of the container to fit its size and destroys all elements beyond the capacity. Element Access\nreference operator [g] ‚Äì Returns a reference to the element at position ‚Äòg‚Äô in the vector at(g) ‚Äì Returns a reference to the element at position ‚Äòg‚Äô in the vector front() ‚Äì Returns a reference to the first element in the vector back() ‚Äì Returns a reference to the last element in the vector Modifiers Functions\ninsert() ‚Äì It inserts new elements before the element at the specified position push_back() ‚Äì It push the elements into a vector from the back pop_back() ‚Äì It is used to pop or remove elements from a vector from the back. erase() ‚Äì It is used to remove elements from a container from the specified position or range. clear() ‚Äì It is used to remove all the elements of the vector container emplace() ‚Äì It extends the container by inserting new element at position emplace_back() ‚Äì It is used to insert a new element into the vector container, the new element is added to the end of the vector. Join my email list to get direct access to new blog posts.\n","href":"/writing/2021-10-03-minimal-cpp-prog-manual/","title":"Learn C++ Programming in 1hr + Code"},{"content":"Table of contents The Farnam Street - How to Think: The Skill You\u0026rsquo;ve Never Been Taught Zat Rana - Marcus Aurelius: Thinking Clearly Mayo Oshin - Elon Musks\u0026rsquo; \u0026ldquo;3-Step\u0026rdquo; First Principles Thinking\u0026quot; Ed Boyden - Rules for How to Think \u0026ldquo;If I had an hour to solve a problem, I\u0026rsquo;d spend 55 minutes thinking about the problem and 5 minutes thinking about solutions.\u0026rdquo; \u0026mdash; Albert Einstein\nWe are very good at solving problems. This is been taught in school. This is also important to know \u0026ndash; how to think. This will help to find a better solution to a given problem. Sometimes a badly thought decision can cost you. So, good to have a better model in place. Last few weeks, I was trying to find the better method \u0026ndash; how to think. Here are few of references, which might be helpful.\nThe Farnam Street - How to Think: The Skill You\u0026rsquo;ve Never Been Taught Thinking means concentrating on one thing long enough to develop an idea about it. Good thinkers understand a simple truth: you can\u0026rsquo;t make good decisions without good thinking and good thinking requires time.¬†If you want to think better, schedule time to think and hone your understanding of the problem. Zat Rana - Marcus Aurelius: Thinking Clearly Actively training ourselves to fight the autonomous loop The human brain is immensely efficient, and it\u0026rsquo;s very good at filtering out noise we don\u0026rsquo;t need in our lives. Sometimes, it\u0026rsquo;s too good, and it diverts our attention away from important information, too. It\u0026rsquo;s on you to fight autonomy and look for those details. Set a few times in your day to really look and to listen. Be deliberate in seeking to bypass the compromise made by the autonomous brain.\nHarnessing objectivity through a different host of eyes We mostly live our lives like we\u0026rsquo;re at the center of reality. It influences how we see, feel, and act. It shifts our perception of the world away from what it truly is. It\u0026rsquo;s important to use tactics that help us see through the clouds.\nBuild a habit of routinely decluttering your mind. The human mind is extremely noisy, but by creating a routine that allows us to clear it up, we can make it less so. By building a habit that focuses on ordering our thoughts, we can declutter the complexity that comes with living in an increasingly busy and crowded world. For example, journaling has been shown to improve health across many different areas of life.\nMayo Oshin - Elon Musks\u0026rsquo; \u0026ldquo;3-Step\u0026rdquo; First Principles Thinking How to Think and Solve Difficult Problems Like a Genius\nIdentify and define your current assumptions\nBreakdown the problem into its fundamental principles. These fundamental principles are basically the most basic truths or elements of anything. The best way to uncover these truths is to ask powerful questions that uncover these ingenious gems.\nCreate new solutions from scratch Once you\u0026rsquo;ve identified and broken down your problems or assumptions into their most basic truths, you can begin to¬†create new insightful solutions from scratch.\nEd Boyden - Rules for How to Think Managing brain resources in an age of complexity.\nSynthesize new ideas constantly. Learn how to learn (rapidly). Work backward from your goal. Always have a long-term plan. Make contingency maps. Collaborate. Make your mistakes quickly. Write up best-practices protocols. Document everything obsessively. Keep it simple. There are few mental models, which also help to think systematically especially decision making, problem solving etc. In the next article, I\u0026rsquo;ll write a brief on this.\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2021-09-25-how-to-think-part-2/","title":"How to Think? Part-II"},{"content":"","href":"/tags/wisdom/","title":"Wisdom"},{"content":"How to negotiate from blog by Sriram Krishnan.\nYou‚Äôve done well in the interview. Now you have the recruiter calling you and reading out some numbers for you. What do you do? Here are some tips I give people.\nAlways negotiate! The system expects you to. You‚Äôll never be dinged for it as long as you do it right. Also - negotiating when you get hired is 10x easier than negotiating after you start as an employee. Always negotiate in good faith. Never be deceptive. Never be rude or ghost. It‚Äôs just good karma and how you negotiate is a proxy for how you handle yourself at work (goes both ways - a company that is a deceptive negotiator might be deceptive in other ways) Take your time. Recruiters might say ‚ÄúYou need to accept this in 24 hours‚Äù. In my experience, if a company thinks you‚Äôre a valuable hire, they‚Äôll still think that in a few days. Don‚Äôt get pressured into a response Do your homework. Know your market price. Do your research on similar titles/roles and also what other peers at the company are paid. You need a sense of what the company typically offers for these roles and what their framework is (RSUs vs cash, titles, etc). This is key - don‚Äôt make a negotiation about ‚ÄúI want X, you‚Äôre offering less X‚Äù Negotiations are easier when you make it a package. Any offer has components - title, sign-on, base pay, RSUs. Offer them options on each. Gives each side leeway to get creative. Communicate what is important to you. Some folks want cash. Some folks want the title bump. Whatever it is, give the recruiter your framework (see #2 about not being deceptive). Give them multiple options. Know your BATNA. Do you have alternate offers on hand? What does your current company pay you? Arm the recruiter with all the info so they know what your options are. Again, dont be deceptive. If you lie, you‚Äôll instantly get caught out as a person of low integrity. nother key: All through the process, reinforce how much you care about this role (and hopefully you genuinely do). It‚Äôs easiest when the recruiter hears ‚ÄúI can‚Äôt wait to kill it at this role, i just need this offer to work‚Äù. Enthusiasm and passion solves many a negotiation. Your (genuine) passion also helps them sell it to other internal stake holders - comp committees, your hiring manager. People will rally for you if they feel you want to work with them. I advise people to negotiate with the recruiter and not their future manager. Sometimes these get tricky and it can taint a key relationship before it gets started. Recruiters do this 10x a day and know how the game works, managers often don‚Äôt. Gently push back if the recruiter says ‚ÄúTitles don‚Äôt matter here‚Äù or ‚ÄúReporting structure doesn‚Äôt work‚Äù. If that were really true, they wouldn‚Äôt care about making your offer work. On the other hand, in my experience, they rarely matter over long run. Finally, give yourself and the recruiter a timeline. A week is about the maximum this should go on for. Any longer and I find the process grows cold. That‚Äôs it. Always negotiate, always act with high integrity, give them options and always show your passion. And you‚Äôll surprise yourself with what can happen!\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-09-12-how-to-negotiate/","title":"How To Negotiate"},{"content":"Few interview tips for interviewer from blog by Sriram Krishnan.\nThe macro point here is making the process as pleasant and fun as possible. It is unnatural to sit in a room being judged by another human so anything we can do to make it easier goes a long way.\nShow up on time.\nNo calls/messaging/phone usage or side-conversations during an interview unless an absolute emergency.\nAlways ask about restroom/food/drink right at the beginning and the end.\nDon‚Äôt type out notes/feedback during the interview. You can remember them in your head.\nDon‚Äôt carry a printed version of that person‚Äôs LinkedIn profile or resume. Makes them feel judged and makes you look unprepared.\nAlways hand them off to the next person/recruiter in person. Don‚Äôt abandon them in a room.\nWatch the clock - always have at least 10 minutes for questions (or better - do them in the beginning). If they don‚Äôt have questions off the top of their head, I try and prompt them with something like ‚ÄúAnything about what you‚Äôve seen about Facebook/Snap/etc today surprise you?‚Äù\nPrep with the other interviewers to not repeat questions. Bonus points if you avoid repeating themes.\nSome interviewers believe in stressing interviewees to see ‚Äúhow they act under pressure.‚Äù I can‚Äôt disagree with this tactic enough. I find it both unprofessional and not predictive of how people really act under pressure.\nMy personal bias is to try and make interviews symmetric - for example, if I ask someone for their background/story, I then follow up with my background/story. If I ask them how they think about a problem, I then talk about how I would think about it. It makes it more of a conversation and less Q\u0026amp;A.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-09-11-tips-for-interviewer/","title":"Less-known interview tips for interviewer"},{"content":"Regret Minimization Framework by Jeff Bezos, CEO of Amazon\nWhen you are facing a big potential change in life, such as switching jobs, starting a caf√©, or moving to another place, don‚Äôt just look at all the small details. These details can come later. Instead, start by asking yourself: On my deathbed, will I regret not taking this chance?\nTo some, this might sound a bit morbid, but for me, it works. Therefore, next time you face a big decision, also ask yourself ‚ÄúAs I get older, will I regret not doing this?‚Äú. If the answer is Yes, then you should probably do this.\nIt was Jeff Bezos, CEO of Amazon and one of the world‚Äôs richest people, who made the Regret Minimization Framework popular. Here is a 2:38-minute clip of Jeff 10 years ago:\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-08-24-regret-minimization-framework/","title":"Regret Minimization Framework"},{"content":"Network Programmability, which is preferred? SNMP or CLI or Yang Network programmability is directly linked to Software Defined Network (SDN). So, it is driven by real benefits like time and cost savings, reduction of human¬†error, customization and innovation. The network programmability is now understood as a set of tools and best practices to deploy, manage and troubleshoot network device, but it\u0026rsquo;s way beyond that.\nNetwork programmability is not new. From the screen scrapping, to bash and expect scripts. Those are part of the automation, try to reduce as much as human error. But what is new? The scale of network is grown exponentially and glowing every day. The agile nature and dynamic requirements of our network infrastructure cannot be realistically¬†configured and managed one device at a time. Each vendor has defined and exposed API to handle their devices programmatically, specially repetitive and mundane tasks. Along with Zero Touch Provisioning (ZTP), a device can be brought up without the human intervention. Except the physical connection, rest of the reconfigurations are managed by SDN controller for the entire network.\nIn the modern internet era, the scale of the deployment is huge. Expectation of DevOps is to get the entire testing and development environments to be built and destroyed within minutes. The network should be flexible enough to adapt the new changes dynamically.\nYang vs. Other Approaches CLI scripting was the primary approach to making automated configuration changes to the network prior to Yang. CLI scripting has several limitations, including lack of transaction management, no structured error management, and ever-changing structure and syntax of commands that makes scripts fragile and costly to maintain. The CLI was designed to use by human; we used for automation, but not for programmatic use-case.\nSNMP is another approach, but, in practice, is mostly used for performance and monitoring applications. Reasons for this include the lack of a defined discovery process that makes it hard to find the correct MIB modules, limitations inherent in using the UDP protocol, and the lack of useful standard security and commit mechanisms.\nIndustry consortiums, like IETF, ONF, TIP, operator engagements, like OpenConfig, OpenRoadm etc are active working with vendors to support Yang as day-one feature. This is also important for product vendors to implement and release programmability support from the beginning of the product release itself.\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2021-08-22-network_programmability_snmp_or_cli_or_yang/","title":"Network Programmability, which is preferred? SNMP or CLI or Yang"},{"content":"Wisdom from Poor Charlie‚Äôs Almanack by Charlie Munger.\n‚ÄúMimicking the herd invited regression to the mean.‚Äù If you do the same things as all the others, you cannot expect different results. Common plagues privately and professionally are status anxiety, social proof tendency, and the keeping-up-with-the-Joneses syndrome ‚Äì doing what we can to be like the rest. It is not until you stop doing this, and instead find a unique way for yourself, that you will see other results. Stop caring so much about what others think. Really.\n‚ÄúSpend each day trying to be a little wiser than when you woke up.‚Äù This goes back to the compound effect ‚Äì if you advance as little as 1% in an area each day, you have improved dramatically in just a few years. This applies to everything.\n‚ÄúBut if you try to succeed in what you‚Äôre worst at, you‚Äôre going to have a very lousy career.‚Äù This is a typical disease that hits us when we are children and go to school. All children have different strengths that could be developed a lot if nurtured correctly. But all too often we are expected to shine in everything from math to sports and lyrics. Yes, a solid foundation in all subjects is good, but this idea of trying to even everything out can hurt your development as an adult. Therefore, find out what your true strengths are and then nourish them. Spend your time and energy going as far as you can thanks to your strengths, instead of evening out your weaker areas.\n‚ÄúIt‚Äôs not bringing in the new ideas that‚Äôs so hard. It‚Äôs getting rid of the old ones.‚Äù This is a quote from Keynes and it still rings true. Just look at most of the change management writing ‚Äì it nearly always focuses on how to bring new things to the table and have people accept them. But where are all the ideas and methods to help people get rid of old ways of working and understanding the world?\nThe Tolstoy effect: People will forever blame their upbringing, parents, schooling, spouses, you name it, for their bad luck and misfortune. It has been proven over and over and is especially true today where there are ample chances to lead a good life, that your goals and determination can become much stronger than your previous life. Self-pity is a very sad mental state, and we should change it to something a lot better.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-08-21-quotes-charlie-munger-almanack/","title":"Wisdom by Charlie Munger"},{"content":"","href":"/tags/clos/","title":"Clos"},{"content":"Table of contents Blocking Network Non-Blocking Network Two Stage Non-Blocking Network Three Stage Non-Blocking Network Clos Network 5 Stage Clos Network FAT-Tree In new era of internet, more applications are moving to cloud. Even for the standalone application, the central processing is done in cloud. Most of the cloud companies developed NextGen datacentre with CLOS architecture. This article is to demystify the Clos architecture.\nThe Clos grows in a very consistent way, thanks to having what\u0026rsquo;s called a scale-out architecture. You increase the amount of work supported by the network by adding more leaves and servers. In contrast, the traditional access-agg-core architecture, the scaling of the services are provided by beefing up the aggregation of box\u0026rsquo;s CPUs.\nThe clos network looks like this.\nhttps://en.wikipedia.org/wiki/Multistage_interconnection_networks#Clos_network\nBefore going deep into this, we need to understand few terminologies on connections.\nBlocking Network Let\u0026rsquo;s say we have A and B as source connected via a single path to destinations C and D. If A is already using the path to talk to C and at the same time if B wants to talk to D, it cannot because there is no free path available from B to D. This is an example of Blocking Network.\nNon-Blocking Network Now if we add another path between Input and Output switches, then B can talk to D at the same time as there is another Path available to route it\u0026rsquo;s request. This is an example of a Non-Blocking Network.\nhttps://en.wikipedia.org/wiki/Multistage_interconnection_networks#Multistage_Interconnect_Network\nTwo Stage Non-Blocking Network In this two stage 2x2 switch connection, if A is already connected to E, then B is blocked to reach F.\nFig-1\nIf we add more paths like below, we\u0026rsquo;ll get non-blocking network. But, the Crosspoint complexity will be worser than crossbar crosspoint complexity.\nNote: crossbar Crosspoint complexity is n^2, where for two stage non-blocking network Crosspoint complexity is is¬†(r x n x m) (n is the number of inputs, m is the number of outputs and r is the number of input switches).\nFig-2\nThree Stage Non-Blocking Network Now, with the introduction of middle staging of two 2x2 switches in Fig-1, B will be able to connect to F.\nFig-3\nThis is the idea of Charles Clos.\nWhat Clos showed that with a 3 stage network, one can build a strictly Non-Blocking network with Crosspoint complexity of¬†which is better than O(N^2).¬†Clos Network A Clos network uses 3 stages to switch from N inputs to N outputs. In the first stage, there are r= N/n crossbar switches and each switch is of size n*m. In the second stage there are m switches of size r*r and finally the last stage is mirror of first stage with r switches of size m*n.\nFig-4\nWith the above structure in mind, Clos showed that one can build a strictly Non-Blocking network if we satisfy the inequality\nm \u0026gt;= 2n-1.\nThe number of connections is much less than that of a crossbar network.\n5 Stage Clos Network Let\u0026rsquo;s construct a 5 stage clos network. The fig-3, which is 3 stage clos network, is enhanced by adding new 2 switches each for stage 1 and stage 3 layers. The network will be like below(Fig-5).\nFig-5\nThe middle switch in Fig-5 can be decomposed like Fig-6 with 3 stage clos with 2x2 switches.\nFig-6\nSo, after replacing two middle switches in Fig-5 with Fig-6 network, it will be a 5 stage clos network with 2x2 switch like below (Fig-7).\nFig-7\nFAT-Tree Fat-Trees were originally introduced by Charles Leiserson in 1985. The basic idea is to keep larger bandwidth (with additional links) towards the root. A Fat-Tree is generally represented by FT(k,n) where k is the radix of the switch and \u0026ldquo;n\u0026rdquo; is the levels of the Fat-Tree.\n[Ref: https://en.wikipedia.org/wiki/Fat_tree]\nNow, lets construct the fat-tree from clos network (fig-5). This topology is known by various terms like Folded Clos, Spine and Leaf or 3 stage clos network.\nNow lets make the 5 stage clos network like Fat-tree from Fig-7.\nFormally, we can say it\u0026rsquo;s FT(4,3) as the radix of the switches are 4 and it has 3 levels.\nCalculation:\nAssuming a switch has¬†\u0026ldquo;k\u0026rdquo;¬†ports and the number of Levels is¬†\u0026ldquo;L\u0026rdquo;, then the first column in the below table gives you the generalize way to calculate various parameters. Next three columns, shows various numbers for 2,3 and 4 levels.\nIn the next articles, will try to deconstruct the linkedin and facebook datacenter architecture.\nReference:\nhttps://en.wikipedia.org/wiki/Multistage_interconnection_networks\nA Scalable, Commodity Data Center Network Architecture\nhttps://engineering.linkedin.com/blog/2016/03/the-linkedin-data-center-100g-transformation\nFacebook DataCenter Architecture\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2021-08-15-clos_networks/","title":"Clos Networks"},{"content":"This is indeed good information by Camille Fournier blog post.\nFor varying levels of seniority, from senior, to staff, and beyond.\nHow to run a meeting, and no, being the person who talks the most in the meeting is not the same thing as running it How to write a design doc, take feedback, and drive it to resolution, in a reasonable period of time How to mentor an early-career teammate, a mid-career engineer, a new manager who needs technical advice How to indulge a senior manager who wants to talk about technical stuff that they don‚Äôt really understand, without rolling your eyes or making them feel stupid How to explain a technical concept behind closed doors to a senior person too embarrassed to openly admit that they don‚Äôt understand it How to influence another team to use your solution instead of writing their own How to get another engineer to do something for you by asking for help in a way that makes them feel appreciated How to lead a project even though you don‚Äôt manage any of the people working on the project How to get other engineers to listen to your ideas without making them feel threatened How to listen to other engineers‚Äô ideas without feeling threatened How to give up your baby, that project that you built into something great, so you can do something else How to teach another engineer to care about that thing you really care about (operations, correctness, testing, code quality, performance, simplicity, etc) How to communicate project status to stakeholders How to convince management that they need to invest in a non-trivial technical project How to build software while delivering incremental value in the process How to craft a project proposal, socialize it, and get buy-in to execute it How to repeat yourself enough that people start to listen How to pick your battles How to help someone get promoted How to get information about what‚Äôs really happening (how to gossip, how to network) How to find interesting work on your own, instead of waiting for someone to bring it to you How to tell someone they‚Äôre wrong without making them feel ashamed How to take negative feedback gracefully Join my email list to get direct access to new blog posts.\n","href":"/blogs/2021-08-03-incomplete-list-skills-for-senior-engineers/","title":"An incomplete list of skills senior engineers need, beyond coding"},{"content":"","href":"/tags/skills/","title":"Skills"},{"content":"Charlie Muner once said:\nYou have to figure out what your own aptitudes are. If you play games where other people have the aptitudes and you don‚Äôt, you‚Äôre going to lose. And that‚Äôs as close to certain as any prediction that you can make. You have to figure out where you‚Äôve got an edge. And you‚Äôve got to play within your own circle of competence.\nIf you want to be the best tennis player in the world, you may start out trying and soon find out that it‚Äôs hopeless‚Äîthat other people blow right by you. However, if you want to become the best plumbing contractor in Bemidji, that is probably doable by two-thirds of you. It takes a will. It takes the intelligence. But after a while, you‚Äôd gradually know all about the plumbing business in Bemidji and master the art. That is an attainable objective, given enough discipline. And people who could never win a chess tournament or stand in center court in a respectable tennis tournament can rise quite high in life by slowly developing a circle of competence‚Äîwhich results partly from what they were born with and partly from what they slowly develop through work.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-07-23-circle-of-competence/","title":"Circle of Competence"},{"content":"At 31 years old, Charlie Munger was divorced, broke, and burying his 9 year old son, who had died from cancer. By the time he was 69 years old, he had become one of the richest 400 people in the world, been married to his second wife for 35+ years, had eight wonderful children, countless grandchildren, and become one of the most respected business thinkers in history. He eventually achieved his dream of having a lot of money, a house full of books, and a huge family. But that doesn‚Äôt mean he didn‚Äôt face unbelievable challenges and tragedies.\nIn 1949, Charlie Munger was 25 years old. He was hired at the law firm of Wright \u0026amp; Garrett for $3,300 per year, or $29,851 in inflation-adjusted dollars as of 2010. He had $1,500 in savings, equal to $13,570 now.\nA few years later, in 1953, Charlie was 29 years old when he and his wife divorced. He had been married since he was 21. Charlie lost everything in the divorce, his wife keeping the family home in South Pasadena. Munger moved into ‚Äúdreadful‚Äù conditions at the University Club and drove a terrible yellow Pontiac, which his children said had a horrible paint job. According to the biography written by Janet Lowe, Molly Munger asked her father, ‚ÄúDaddy, this car is just awful, a mess. Why do you drive it?‚Äù The broke Munger replied: ‚ÄúTo discourage gold diggers.‚Äù\nShortly after the divorce, Charlie learned that his son, Teddy, had leukemia. In those days, there was no health insurance, you just paid everything out of pocket and the death rate was near 100% since there was nothing doctors could do. Rick Guerin, Charlie‚Äôs friend, said Munger would go into the hospital, hold his young son, and then walk the streets of Pasadena crying.\nOne year after the diagnosis, in 1955, Teddy Munger died. Charlie was 31 years old, divorced, broke, and burying his 9 year old son. Later in life, he faced a horrific operation that left him blind in one eye with pain so terrible that he eventually had his eye removed.\nIt‚Äôs a fair bet that your present troubles pale in comparison. Whatever it is, get over it. Start over. He did it. You can, too. It is The Cinderella Principle.\nReference: Link\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-07-23-the-cindrella-principle/","title":"The Cindrella Principle"},{"content":"Entrepreneur Colin Dowling on sales, via HN:\nSales is a lot like golf. You can make it so complicated as to be impossible or you can simply walk up and hit the ball. I\u0026rsquo;ve been leading and building sales orgs for almost 20 years and my advice is to walk up and hit the ball.\nSales is about people and it\u0026rsquo;s about problem solving. It is not about solutions or technology or chemicals or lines of code or artichokes. It\u0026rsquo;s about people and it\u0026rsquo;s about solving problems.\nPeople buy 4 things and 4 things only. Ever. Those 4 things are time, money, sex, and approval/peace of mind. If you try selling something other than those 4 things you will fail.\nPeople buy aspirin always. They buy vitamins only occassionally and at unpredictable times. Sell aspirin.\nI say in every talk I give: \u0026ldquo;all things being equal people buy from their friends. So make everything else equal then go make a lot of friends.\u0026rdquo;\nBeing valuable and useful is all you ever need to do to sell things. Help people out. Send interesting posts. Write birthday cards. Record videos sharing your ideas for growing their business. Introduce people who would benefit from knowing each other then get out of the way, expecting nothing in return. Do this consistently and authentically and people will find ways to give you money. I promise.\nNo one cares about your quota, your payroll, your opex, your burn rate, etc. No one. They care about the problem you are solving for them. There is more than 100 trillion dollars in the global economy just waiting for you to breathe it in. Good luck.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2021-07-18-how-to-get-good-at-sales/","title":"How to be good at sales?"},{"content":"","href":"/tags/lpm/","title":"LPM"},{"content":"","href":"/tags/routing/","title":"Routing"},{"content":"Table of contents Bridging and Routing L2 Forwarding L3 Forwarding VRF Overlays 1. IP in IP 2. GRE 3. VXLAN Segment Routing Forwarding in NIC The core routing and packet forwarding haven\u0026rsquo;t changed for decades and it is not going to change for next decade also. But, it is going to evolve due to market needs, specially data centres. At the same time, Silicon capacity, CPU power, NIC bandwidth, all have increased significantly. Applications have evolved from monoliths to multi-tiered applications, using distributed databases with distributed microservices, creating a lot more demand for East-West network capacity and, more importantly, network services. Typically, the East-West traffic is 80 percent of overall traffic till today. Till now, various number of IP routing and forwarding techniques have been deployed in Data Center and Internet Service Provider.\nBridging and Routing The Ethernet is only surviving technology at Layer 2, defined by IEEE 802.1. The packet forwarding technique at layer 2 is called Bridging, also called as Layer 2 forwarding. On the other hand, IP is only surviving technology at Layer 3, defined by IETF. The technique it uses to forward packets is called IP Routing/IP Forwarding/L3 Forwarding.\nL2 Forwarding The Ethernet packets have a straightforward structure that contains six fields: destination MAC address, source MAC address, 802.1Q tag, Ethertype (protocol inside the data field), data, and frame check sequence (FCS). The 802.1Q tag contains the VLAN identifier (VID) and the priority. Of these fields, the only ones used in L2 forwarding are the VID and the destination MAC address. These two are used as a key to search a MAC address table with an exact match technique.¬†If the key is found, the table entry indicates where to forward the frame. If the key is missing, the frame is flooded to all the ports (except the incoming port) in an attempt to make the maximum effort to deliver the frame to its final destination. To avoid loops, spanning tree protocol is used.\nL3 Forwarding Layer 3 forwarding is different from L2 forwarding: if a packet needs to be sent across subnets, the destination IP address is searched in an IP routing table using a longest prefix match (LPM) technique.\n100.1.0.0/16 \u0026ndash; port 1 100.2.0.0/16 \u0026ndash; port 2 100.1.1.0/24 \u0026ndash; port 3\nThe /n¬†indicates that only the first¬†n¬†bits from the left are significant in any matching.\nIf the LPM does not find a match in the forwarding table for the IP destination address, the packet is dropped. In L3 forwarding, time to live (TTL) is used to avoid temporary loops.\nLPM can be done in software using a variety of data structures and algorithms. Linux uses a level-compressed trie (or LPC-trie) for IPv4, providing good performance with low memory usage. For IPv6, Linux uses a more traditional Patricia trie. LPM forwarding in HW is the most common implementation of layer 3 forwarding. There are a few different ways to accomplish LPM in hardware. All of them require performing a \u0026ldquo;ternary match\u0026rdquo;; that is, a match where some of the bits are \u0026ldquo;don\u0026rsquo;t care\u0026rdquo; (represented by the letter \u0026ldquo;X\u0026rdquo;). For example, the route 10.1.1.0/24 is encoded as 00001010 00000001 00000001 XXXXXXX.\nFor example, some commercial routers use a microcode implementation of Patricia trie. Others use a hardware structure called ternary content-addressable memory (TCAM) that supports ternary matches.¬†Based on the scale requirements, TCAMs can take up significant silicon space and have high power consumption.\nVRF Virtual routing and forwarding (VRF) is a layer 3 network virtualization technology that permits multiple instances of a routing table to exist in a router and work simultaneously. This allows different kinds of traffic to be forwarded according to different routing tables. Each routing instance is independent of the others, thus supporting overlapping IP addresses without creating conflicts.\nIn VRF, each router participates in the virtual routing environment in a peer-based fashion; that is, each router selects the routing table according to some local criteria, the most common being the incoming interface (either physical or logical). The other options are VLAN ID or an MPLS label.\nOverlays An¬†overlay network¬†is a virtual network built on top of an¬†underlay network; that is, a physical infrastructure. The underlay network\u0026rsquo;s primary responsibility is forwarding the overlay encapsulated packets (for example, VXLAN) across the underlay network in an efficient way using ECMP when available. The underlay provides a service to the overlay. In modern network designs, the underlay network is always an IP network (either IPv4 or IPv6), Example: MPLS, GRE, IP in IP, L2TP, and VXLAN. Normally, the encapsulation and decapsulation needs to be done in HW for better performance.\n1. IP in IP One of the first standards that deals with encapsulation of IPv4 in IPv4 is RFC 1853. Similarly IPv4 in IPv6, IPv6 in IPv4.\n2. GRE The goal was to be able to encapsulate a wide variety of network¬†layer protocols inside virtual point-to-point links over an IP network. Although nowadays this feature is less relevant because the only surviving protocols are IPv4 and IPv6, it is still important, because with virtualization there is a need to carry layer 2 traffic inside a tunnel. Efforts such as Network Virtualization using Generic Routing Encapsulation (NVGRE/rfc7637) and L2 GRE try to satisfy this virtualization requirement.\n3. VXLAN VXLAN carries a VLAN across a routed network. It propagates l2 packets/domain over L3 network. This is required in Clos based datacentre network. Although VLANs have historically been associated with the spanning tree protocol, which provides a single path across a network, VXLAN can use the equal cost multi-path (ECMP) of the underlying network to offer more bandwidth.\nThe VXLAN standard is defined in RFC 7348 and the authors\u0026rsquo; list, showing that it is a concerted effort among router, NIC, and virtualization companies, indicating the strategic importance of this overlay.\nVXLAN uses UDP encapsulation, and the destination UDP port is set to the well-known value of 4789. The source UDP port should be randomly set, creating entropy that can be used by routers to load balance the traffic among multiple parallel links.\nIn general, the encapsulating endpoint may set the UDP source port to a hash of the five-tuple of the inner IP header. In this way, all the packets belonging to a single flow will follow the same path, preventing out-of-order packets, but different flows may follow different paths. VXLAN is also used as a technology for encapsulating layer 3 unicast communication between application layers; this is evident in the newer revision of the VXLAN specification that allows for IP encapsulation within VXLAN natively. Finally, VXLAN encapsulation adds a Virtual Network ID (VNID) to the original L2 frame, a concept similar to a VLAN-ID, but with a much broader range of values, because the VNID field is 24 bits, compared to the VLAN-ID field that is only 12 bits.\nThe VXLAN encapsulation adds 50 bytes to the original layer 2 Ethernet frame, which needs to be considered in the context of the network underlay;\nSegment Routing Source routing is a technology known for decades in which the sender of the packet decides the path the packet should take to its destination. Segment routing (SR) is a form of source routing where the source node defines the forwarding path as an ordered list of \u0026ldquo;segments.\u0026rdquo; There are two kinds of Segment Routing:\nSR-MPLS, which is based on Multiprotocol Label Switching (MPLS) SRv6, which is based on IPv6 The underlying technology used by SR-MPLS is Multiprotocol Label Switching (MPLS), a routing technique that directs data from one node to the next based on \u0026ldquo;labels\u0026rdquo; rather than network addresses. Alternatively, SR can use an IPv6 data plane, as is the case in SRv6.\nSegment routing divides the network into \u0026ldquo;segments\u0026rdquo; where each node and link could be assigned a segment identifier, or a SID, which is advertised by each node using extensions to standard routing¬†protocols like IS-IS, OSPF and BGP, eliminating the need to run additional label distribution protocols such as MPLS LDP.\nSR imposes no changes to the MPLS data plane. In SR the ordered list of segments is encoded as a stack of labels. The first segment to process is on the top of the stack. Upon completion of a segment processing, the segment is removed from the stack.\nFor example, in the absence of SR the routing between the Source and the Destination in¬†is composed of two ECMPs: A - D - F - G and A - D - E - G (assuming all links have the same cost). In the presence of SR it is possible to forward the packet across other links. For example, if the source specifies a stack E/C/A, where A is the top of the stack, the packet is forwarded to A that pops its label, resulting in a stack containing E/C. Then A sends the packet to C, C will pop its label and forward the packet to E, which delivers it to its destination.\nSegment routing offers the following benefits:\nNetwork slicing, a type of virtual networking architecture with the ability to express a forwarding policy to meet a specific application SLA (for example, latency, bandwidth) Traffic engineering Capability to define separate paths for disjoint services Better utilization of the installed infrastructure Stateless service chaining End-to-end policies Compatibility with IP and SDN Forwarding in NIC This is very new concept, adopted in cloud providers. The cloud server consist of large number of tenants, compromising large number routes and access control lists (ACL). Sometimes, this is difficult to implement this in host with traditional table lookups. Hence, some cloud providers decided to implement limited forwarding functionally in NIC. This is kind of flow based entries in NIC. The flow supports ipv4/ipv6 address, policy, filrewalls etc. With the advancement of more technologies with ipv6 addressing, it will be a challenge to maintain both features and cost. More complicated the features will be, bigger silicon and more power will be needed and cost will increase. This is one of the future trend gearing up for cloud data centers. This approach is implemented in Microsoft Azure.\nReferences:\nUnderstanding VXLAN: A Guide to Virtual Extensible LAN Technology Join my email list to get direct access to new blog posts.\n","href":"/writing/2021-05-30-various_routing_packet_forwarding_techniques/","title":"Various Routing \u0026 Packet Forwarding Techniques"},{"content":"Table of contents Do the right thing Do the right way Prioritize Organize Routine Goal Love what you do Recreation Positive thinking \u0026amp; attitude Right Values Productivity concept is always there in my mind, but never thought so deeply. Until recently, my friend Biplab asked me the questions:\n\u0026ldquo;How do you feel at the end of a hard day? Do you feel satisfied, fulfilled, or disappointed? Are you productive?\u0026rdquo;\n\u0026ldquo;may be yes!\u0026rdquo;\u0026ndash;I said with a deep sigh.\nIn general, when I think about productivity, it comes to our mind about industrial productivity, the input vs output. This is there for ages. But as human beings, living in the modern world, we should also think about our productivity. We have a limited amount of time in a day/life we should also achieve our desired productivity. We are living in a fast and advanced world, we should improve our productivity in the advanced world, we can leverage the technology in our progress.\nDoes productivity apply to only working people? No. The next question, how can productivity be judged? Here, it can be loosely tied to individual goals. It should be better to come out of our comfort zone and achieve a little more towards individual goals. To give an example, \u0026quot;A bodybuilder is not born bodybuilder\u0026quot;. He needs to work on his goal to become a bodybuilder slowly.\nIn society, there are working people, like doctors, engineers, lawyers, industrial workers, office workers, bus drivers/conductors, farmers, etc. They need to produce their output based on their job profile. Here, I am not talking about just the output of whatever they need to produce based on their work assigned. How much better are they doing? They should try to improve everyday so that the remaining time, they can use for their betterment of life in terms of personal and professional improvement.\nIn society, there are other kinds of people, homemakers. Especially in developing and underdeveloped countries, the percentage of homemakers is more. They also try to improve their productivity to a great extent. In everybody's life there is a good portion of necessary work, which cannot be neglected. Other than that, there are some tasks which can be optimised and improvised, so that it can help to get some extra amount of time, which can be used for a greater cause.\nIn Asian countries, 75% of women are homemakers. Aren\u0026rsquo;t they organized? Absolutely, they are. Managing household chores, managing kids, helping husband/partners, etc. Some of them are doing office work, helpers as well. On a working day, we have specified time to do work. but for homemakers, they don\u0026rsquo;t have defined time to do their work. They are always on. One of the challenging tasks about homemaking is feeling like we are never entirely done. It\u0026rsquo;s the same for the people who work outside of home too, they have many tasks on top of the current task they do. That\u0026rsquo;s the reason to create a to-do list. This is the accountability matrix of tasks in hand.\nWhile writing this article, I was talking to my neighbour, who is a homemaker. I asked, \u0026ldquo;How is your schedule, how productive are you?\u0026rdquo; She said, \u0026quot;Full day, I am busy cooking, preparing children for school, basic house chores and then picking them from school, helping them study.\u0026quot;\nShe was trying to explain the bunch of work she does. Then I asked her to explain the start time. As far as I know, school has a fixed time. When she explains to me the details of the schedule for a weekday. She does a lot of ads hoc work, which can be planned properly, like breakfast for the kids, that can be done in one go for all the family members, instead of just at a time for one. Life should not be spent only on household chores. We should be doing these things to get them out of our way, so that we can go on doing things which we like to do and more enjoyable. Like reading books, talking to friends, cherishing life, old memories, playing with kids, taking a walk, exercising, holidaying,\nThere are many categories to consider improving productivity\n1. Do the right thing Here, I'll take the perspective of working professionals who are appointed to a job. Working more hours, over 8 hrs in a day, makes you more productive? You might add more value, you are not more productive, but you can enjoy more value created in this extra time.\nYou have a limited amount of time allocated in a day. You can add extra hours for a period of some time, maybe a few months. I have seen people adding extra time to achieve their extra goal. This includes me. While doing my M.Tech along with my job. I was doing extra time to finish the pending study and assignment at the end of each semester. But you cannot continue doing so in the long run. It has a detrimental effect, you might add more mistakes, overloading your working memory, more stress, etc. In the long run, your productivity will decrease. You may not be successful by just adding extra time to your work. we must work smarter than harder.\nI want to introduce a term here - deep work. The time you work, it should be focused work, no fooling around. To increase your efficiency, it should be focused work.\n2. Do the right way By now, you know what is needed for you/work. Next you should decide what is required to achieve the work, instead of spending time in hours for the task, how to do. Sometimes we need to acquire or brush up the skills before jumping into the work.\nIf an athlete decides to join a marathon, he cannot start running the next day. He needs to gain knowledge about running technique, frequency, and food. Sometimes, he needs to hire a coach to guide him through the course.\nWe all need to focus on the right strategy with a long-term view in mind with the right goals. I know this is not always easy, but we must try to sit down and think on this instead of going in the flow.\nThe above activities are crucial for us to decide what is needed. But life consists of many right things to achieve in the right way. Again, the priority needs to be decided. I feel there are a few more activities that need to be done as well.\n1. Prioritize There are two ways, prioritize tasks for the day and long term. In the long term - I like to mention that as a goal.\nIn a day, we have 24 hrs, but we cannot always be productive. Right after the sleep, in the morning, everybody has more energy to concentrate on work. This will be reduced as the day passes by. I prefer to follow the table below for my work priorities.\nAll your important tasks must be finished early in the day while your body has full energy even if the task takes longer. But I used to follow the contrary, my favourite thing first. Sometimes, it used to be an important task, but not always. Not finishing important tasks will add more pressure slowly. It will create a negative outlook.\n2. Organize Let\u0026rsquo;s go to a stationery shop or cloth store, shoe store, any other departmental store. \u0026ldquo;Why are they organized?\u0026rdquo; Yes, to maximize space, easy to find structure. The storekeeper doesn\u0026rsquo;t need to use his brain to remember the location. In our life also, organization is important. The human brain should not be used for this structural work. Instead, the brain power should be used to memorise, to innovate, to be creative, to improve life. Ideally, our tasks should be organized in a structured way to make the system clutter free, stress free.\nThe point is if tasks are not organized, twice or thrice more effort is required to do the same task. So, don\u0026rsquo;t fall into this under-achiever category. Sometimes, it just needs very minimal effort to make things organised. This is just a small habit change. Over a period, it will automatically be organised.\n3. Routine In day-to-day life, many activities are set as a routine. Like getting up from bed, breakfast, starting with the office, lunch, dinner, to go to bed. This is a common routine for all of us. Along with that, based on job profile, everyone has their prescribed work routine. Again, prioritization is needed on the routine. There is a need to build a system with a flexible framework. In the system, the most important work should be at the beginning of the day, when you have high energy.\nDo you remember the story, \u0026ldquo;Putting stone and sand in a jar\u0026rdquo;? To make the most out of the day, stone should be dropped first in the jar and then sand.\nAll the activities are like a habit. Don\u0026rsquo;t believe in will power, instead build a small habit. Very few lucky people can get something from the will power. The habit is like a system for you. Like, if anyone decides to go to gym, every day at 7am, start to go to gym for 15 mins. Don\u0026rsquo;t use your brain here, just go every morning. Automatically, this will become part of your routine.\n4. Goal Everyone has their own day-to-day life. Do you want to live the same life three years from now?\nIf not, you have to set a goal for yourself and you have to define steps for it. The day, today, tomorrow, is not an isolated component of our life, of the goals. Hence, one should know the art of mastering the day as a productive one. There is a definition of weekday and weekend. Both are equally important in our life. The weekend needed to regain the energy for the coming week. So, weekends need to be enjoyed as a productive one. Don\u0026rsquo;t be frightened to think about the future, instead one must define his goal and plan it accordingly as small as possible towards a bigger goal.\nYou are reading this article, because you want to excel in your life. So, Though I had a bunch of goals and lists planned, none of them would ever see the light of the day if I changed nothing and continued to live my days exactly as I do them now.\n5. Love what you do If you love what you create, this will create enormous peace of mind. If you love, your passion will follow. You will keep innovating. In this technology driven world, if you want to achieve your goal, make use of technology in your favour for your bigger goal. One might get setbacks. The key is to turn those setbacks into success by focusing on constant and strategic innovation. How can the approach be refined? How can one change what he is doing to more swiftly meet the goal? Mistakes are normal in human life. When can you learn from your mistakes without punishing yourself and take the courage to proceed towards a life where you love what you do, then you will overcome all the odds and become a strong human being?\n6. Recreation I think we always want to do more in a short time. This is human psychology. But study reveals that if you take a short break in-between, productivity improves. So, we should make time for leisure. Don\u0026rsquo;t let your work sacrifice your family, find a balance between work and leisure. Sometimes, it's good to take short holidays to rejuvenate yourself.\nOne of my favourite short relaxations exercises is to breathe in deeply, count one-\u0026gt;ten slowly, and then exhale, doing the same for 5 mins. In fact, this is also one of the sleep exercises. Few more techniques one can try, like visualization/daydreaming, listening to music or playing musical instruments, try to keep the mind blank for some time.\nThe productivity is directly linked to routines, goals, prioritization, etc. I feel, few psychological thinking will help as well.\n7. Positive thinking \u0026amp; attitude Positive thinking is a mental attitude which focuses on the good in any situation. It can have a big impact on physical and mental health. Research reveals that this is not just about being happy or displaying an upbeat attitude, this is more, it will create value in our life, it will help to look at the world with a positive outlook. Even, the negative outlook will be accepted in a positive angle. In life, everything is not in our control. We can only control those which are under our influence. We will stop blaming those which are not in our control. Positive thinking has positive benefits in life, better stress management and handling skills, physical well-being, less depression, longer life span. Everybody has lots of abilities, the question comes, how to use those abilities in a positive angle.\nFew years back, I met one psychology trainer, Ranjit Singh, who is specialists for sports. \u0026ldquo;What is your role in sports, how do you contribute to sports activities\u0026rdquo; \u0026ldquo;I have an invisible role, but BIG role. I work on improving their attitude,\u0026rdquo; He said laughingly.\nLater I realised, it\u0026rsquo;s indeed a big role. They help to train the athlete about their attitude. So, a positive attitude is applicable to all of us. This is just a practice and will help us in building physical, mental, social, creative skills.\n8. Right Values I believe the right values are very important in our lives, this will guide and motivate us towards our action, decision, attitude. This must be there in our fundamental belief system. Values are essential to ethics; ethics are connected to human actions and how they behave. The values are learnt in our childhood, we have been taught which is right and wrong. They are very much stable over a period, minimally changed. The right values may not have a direct relation to productivity. But this will help to achieve one\u0026rsquo;s goal with the right attitude.\nLife is fun, at the time it should be organised as well. I think, with the above-mentioned principles, anyone can improve their life, for better, more creative, more expanding. It will help to lead a less stressful, more fun, happier life.\nJoin my email list to get direct access to new blog posts.\n","href":"/writing/2021-05-15-how_productive_are_you_in_life/","title":"How productive are you in life?"},{"content":"","href":"/tags/productivity/","title":"Productivity"},{"content":"Thank you for reading my blog!\nJoin my email list to get direct access to new blog posts.\nPS: I won\u0026rsquo;t spam you. When I send you an email, I promise it will be worth it.\n","href":"/subscribe/","title":"Subscribe"},{"content":"Read enough, and you become a connoisseur. Then you naturally gravitate more toward theory, concepts, nonfiction.\nBest Advices:\nThe Day You Became A Better Writer \u0026amp; Second Version by Scott Adams Even though I am a very good writer and I‚Äôve been writing a lot since I was young, I still open up that blog post and put it in the background anytime I‚Äôm writing anything important. It‚Äôs that good. I use it as my basic template for how to write well. Think about the title, ‚ÄúThe Day You Became a Better Writer.‚Äù It‚Äôs such a powerful title. He teaches you in one small blog post the importance of surprise, the importance of headlines, the importance of being brief and directed, not using some adjectives and adverbs, using active not the passive voice, etc. This one blog post right there will change your writing style forever if you put your ego down and absorb it properly. Best Collection:\nOn Various Topics Awesome CTO Worthwhile Resources by blas - podcasts, videos, blogs, articles, etc Twitter accounts:\n@AmuseChimp @mmay3r ‚Äúintellectual compounding‚Äù by @Zaoyang). Powerful Twitter Lists\nIETF Specialists All about SDN Network Programmability High-Signal People by (Aadit Sheth)[https://twitter.com/aaditsh] Newsletters (I have subscribed to)\nSoftware Lead Weekly by Oren Ellenbogen Level Up by Pat Kua Pointer.io by Suraj Kapoor TLDR Newsletter by Dan Ni Programming Digest, by Jakub Chodounsky Startup Advice\nSam Altman Startup Playbook Talks\nHarari‚Äôs Sapiens in lecture/course from on youtube. Podcasts\nThe Changelog - high-quality conversations with engineers, leaders, innovators in the tech world.\nDevPath.fm - conversations with industry veterans about their software engineering careers and learnings.\nGreater Than Code - a podcast with frequent software engineering guests from underrepresented groups. Weekly episodes on the human side of tech.\nHeavy Networking from Packet Pushers - About data networking. Continuous professional development. Architecture and design, software defined, cloud, routing, switching, security, wireless, campus, enterprise, and more. Technical discussions with vendors about their products, deep dialog with real people who make networks work.\nUnderlay - Exploring the intersection of digital infrastructure and the humans who depend on it.\nCommunity Made - Lessons, learnings, and key insights from the world‚Äôs most fascinating entrepreneurs\nPING - Podcast of APNIC\nCisco TechBeat - Cisco TechBeat features inspirational guests from around the world that drive innovation\nPacket Pushers Podcasts - The Packet Pushers Podcast Network\nThe Routing Table Podcast - The Routing Table podcast is hosted by Melchior Aelmans and Rick Mur.\nNorth Star Podcast by David Perell - A deep dive into the stories, habits, ideas, strategies and methods that drive fulfilled people and create enormous success for them. They‚Äôre guided by purpose, live with intense joy, learn passionately, and see the world with a unique lens. Each episode lets us soak in their hard-earned wisdom and apply it to our lives. Guests include Neil deGrasse Tyson, Seth Godin, and Tyler Cowen.\nAppendix 1: Collection of Tech Blogs\ngithub.com/kilimchoi/engineering-blogs tech-blogs.dev uses.tech bloggingfordevs.com High Scalability Appendix 2: Online Tech Communities\nreddit.com Hacker news by ycombinator Appendix 3: Yearly Retrospective \u0026amp; Planning Template\nYearCompass The Personal Annual Review- Sahil Bloom, The Annual Planning Guide - Sahil Bloom ","href":"/reading/","title":"Reading"},{"content":"","href":"/archive/","title":"Archive"},{"content":"Charles Darwin‚Äôs 8 Steps to Steady-State Learning\nA life-long program of self-study Keeping a diary/journal Scribbling notes immediately Keeping ‚Äúindexed portfolios‚Äù (notes/summaries of what you\u0026rsquo;ve read or heard) Testing beloved concepts ‚Äì make a note, immediately, if you come across a thought or idea that is contrary to something you currently believe. Absorb it, study it and don\u0026rsquo;t let it go until you conclude that you need to change your mind or if the thought or ideas is wrong Learning lessons by heart Seeking to earn the approval of great men (dead or alive) The humility to seek not fame (as in, wisdom flows into the humble man like water flows into a depression) Join my email list to get direct access to new blog posts.\n","href":"/blogs/2020-12-20-charles-darwin-8-steps-to-steady-state-learning/","title":"Charles Darwin‚Äôs 8 Steps to Steady-State Learning"},{"content":"Table of contents Why is this important? What do we achieve? Principles A good code must satisfy few requirements: it must produce correct results, for which it is written. Also, it should be readable and understandable by other developers. It should also follow few code principles. This is your professional responsibility that the code you write will be re-read by many other developers, who want to either understand or modify the code.\nWhy is this important? When the code is not readable, the engineer modifying the code, will spend lot of time to understand the code itself. If it is misunderstood, then they will spend multiple iteration to fix his newly written code. Sometime, it can lead to breaking functionality in somewhere else unintentionally.\nIn another scenario, engineer failing to understand the code, will rewrite the code again, deleting the old code. It has a impact on the overall schedule, customer delivery, most importantly if any corner cases missed, this will lead to more effort. Code should be written to minimize the time it would take for someone else to understand it\nAnother important aspect, incorrect code and unreadable code. The incorrect code will not be able to hide for long time. Unit-test and automation will be able to detect this very quickly unless these are dead code, while unreadable code can exist in system for long time till the time a new engineer modify this code due to bug fix or new enhancement.\nA good codebase - is the foundation of company\u0026rsquo;s growth. a good codebase consists of well tested and readable code. It helps to increase the confidence to engineer. They will be happy to enhance and extend existing codebase to add more features. It helps company to grow more business.\nWhat do we achieve? On the surface, readability may seem subjective. Something which may vary between languages, codebases, and teams. But when you look underneath, there are core elements within all code which make it readable.\nCan you follow and understand (not read) your code? Does your variable\u0026rsquo;s, function\u0026rsquo;s naming convention and functional building blocks follow proper sequence? Are your comments adequate enough for future readers? Are the code style consistent?\nMany programmers are too close to the computer. If the code runs, nothing else matters. Although a common defense it removes all of the human elements from what we do.\nThe readable code starts with you. After you finish your coding, take a break and re-read the code again with a open mind. If you find any shortcomings in your code to make it more descriptive, please add. This will help future readers. In fact, this is a general rule in code review guidelines that another engineer, who is not expert to the codebase, should review the code also. He will look into the code from a fresh perspective. If you and the other developers/reviewers (including the non-expert engineer) are happy with the code and its structures with no open questions, then that is the readable code.\nWe are in the world of agile, faster customer engagement. Non-readable code ‚Äî It will indirectly impact project schedule and customer delivery. This is one of the contributor of technical debt. Especially in agile methodology, not necessarily, the same engineer will work on the same module for future work, a different engineer should be able to enhance the code without any additional complexity.\nPrinciples Few principles of readable code:\nWell-Tested In my opinion, this is very important. well-tested code can be modified without the fear of breaking any functionality. Without tests, risk will come in future work estimation. Optimization vs Readability, Always write code that is simple to read and which will be understandable for developers. Because time and resources that will be spent on hard readable code will be much higher than what you get from optimization. Still there are use-cases where optimizston are required, then try to make it like independent module with 100% test coverage. Architecture First Writing code, without thinking of its architecture is a bad practice. Well Structured, In any codebase (big or small), well structure code is very easy to understand and browse. Keep It Simple, Don‚Äôt write complex code. Make it simpler than more bugs it may have and less time needed to debug them. Thoughtful Naming, Variables, functions, class names will help to understand the code better. This needs little cautious effort from developers to make it happen. You have to make it a practice. Comments, Again this itself explains the need of it. A \u0026lsquo;why\u0026rsquo; comment is good enough. Make sure it is precise and compact. Automation, First and foremost is manual tests. The Automation is needed to make the reduction of effort in long term. If you have the automation ready for your existing code, the effort required to test your existing feature will be very small, only additional testing required for your new functionality. Join my email list to get direct access to new blog posts.\n","href":"/writing/2020-11-18-clean-and-readable-code/","title":"Clean and Readable Code"},{"content":"","href":"/tags/readable-code/","title":"Readable-Code"},{"content":"","href":"/tags/link-state/","title":"Link-State"},{"content":"Link-State Protocols on Data-Center Fabrics ‚Äì Internet Protocol Journal¬†(PDF)\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2020-10-01-link-state-protocols-on-data-center-fabrics-internet-protocol-journal-pdf/","title":"Link-State Protocols on Data-Center Fabrics - Internet Protocol Journal (PDF)"},{"content":"A set of 10 rules as listed in Ed Boyden‚Äôs popular blog post in MIT Technology Review, Nov 2007.\nManaging brain resources in an age of complexity.\nSynthesize new ideas constantly. Never read passively. Annotate, model, think, and synthesize while you read, even when you‚Äôre reading what you conceive to be introductory stuff. That way, you will always aim towards understanding things at a resolution fine enough for you to be creative.\nLearn how to learn (rapidly). One of the most important talents for the 21st century is the ability to learn almost anything instantly, so cultivate this talent. Be able to rapidly prototype ideas. Know how your brain works. (I often need a 20-minute power nap after loading a lot into my brain, followed by half a cup of coffee. Knowing how my brain operates enables me to use it well.)\nWork backward from your goal. Or else you may never get there. If you work forward, you may invent something profound‚Äìor you might not. If you work backward, then you have at least directed your efforts at something important to you.\nAlways have a long-term plan. Even if you change it every day. The act of making the plan alone is worth it. And even if you revise it often, you‚Äôre guaranteed to be learning something.\nMake contingency maps. Draw all the things you need to do on a big piece of paper, and find out which things depend on other things. Then, find the things that are not dependent on anything but have the most dependents, and finish them first.\nCollaborate.\nMake your mistakes quickly. You may mess things up on the first try, but do it fast, and then move on. Document what led to the error so that you learn what to recognize, and then move on. Get the mistakes out of the way. As Shakespeare put it, ‚ÄúOur doubts are traitors, and make us lose the good we oft might win, by fearing to attempt.‚Äù\nAs you develop skills, write up best-practices protocols. That way, when you return to something you‚Äôve done, you can make it routine. Instinctualize conscious control.\nDocument everything obsessively. If you don‚Äôt record it, it may never have an impact on the world. Much of creativity is learning how to see things properly. Most profound scientific discoveries are surprises. But if you don‚Äôt document and digest every observation and learn to trust your eyes, then you will not know when you have seen a surprise.\nKeep it simple. If it looks like something hard to engineer, it probably is. If you can spend two days thinking of ways to make it 10 times simpler, do it. It will work better, be more reliable, and have a bigger impact on the world. And learn, if only to know what has failed before. Remember the old saying, ‚ÄúSix months in the lab can save an afternoon in the library.‚Äù\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2020-07-22-how-to-think/","title":"How to Think?"},{"content":"External:\n100 Tips for a Better Life 100 Ways To Live Better My collection:\n\u0026ldquo;The less confident you are, the more serious you have to act.\u0026rdquo; ‚Äì Tara Ploughman\n\u0026ldquo;The condition of man is already close to satiety and arrogance, and there is danger of destruction of everything in existence.\u0026rdquo; ‚Äì a Brahmin to Onesicritus, 327 BC, reported in Strabo\u0026rsquo;s¬†Geography\n\u0026ldquo;Change breaks the brittle.\u0026rdquo; ‚Äì Jan Houtema\nThe sons of Hermes love to play, And only do their best when they Are told they oughtn\u0026rsquo;t; Apollo\u0026rsquo;s children never shrink From boring jobs but have to think Their work important. ‚Äì W. H. Auden,¬†Under Which Lyre\n\u0026ldquo;Programs must be written for people to read, and only incidentally for machines to execute.\u0026rdquo; ‚Äì Abelson \u0026amp; Sussman,¬†SICP, preface to the first edition\n\u0026ldquo;That language is an instrument of human reason, and not merely a medium for the expression of thought, is a truth generally admitted.\u0026rdquo; ‚Äì George Boole, quoted in Iverson\u0026rsquo;s Turing Award Lecture\n\u0026ldquo;Short words are best and the old words when short are best of all.\u0026rdquo; ‚Äì Winston Churchill\n\u0026ldquo;Many big people were chasing me. I didn\u0026rsquo;t know what to do. So I thought I would surprise them and throw it.\u0026rdquo; ‚Äì Garo Yepremian, Miami placekicker, after a disastrous attempt to throw a pass in the Super Bowl.\n\u0026ldquo;That book is good in vain which the reader throws away. He only is the master who keeps the mind in pleasing captivity; whose pages are perused with eagerness, and in hope of new pleasure are perused again; and whose conclusion is perceived with an eye of sorrow, such as the traveller casts upon departing day.\u0026rdquo; ‚Äì Johnson,¬†Lives of the Poets: Dryden\n\u0026ldquo;Don\u0026rsquo;t worry about what anybody else is going to do. The best way to predict the future is to invent it.\u0026rdquo; ‚Äì Alan Kay\n\u0026ldquo;But the audience is right. They\u0026rsquo;re always, always right. You hear directors complain that the advertising was lousy, the distribution is no good, the date was wrong to open the film. I don\u0026rsquo;t believe that. The audience is never wrong. Never.\u0026rdquo; ‚Äì William Friedkin, in a¬†NYT¬†interview\n\u0026ldquo;Dealing with failure is easy: Work hard to improve. Success is also easy to handle: You\u0026rsquo;ve solved the wrong problem. Work hard to improve.\u0026rdquo; ‚Äì Alan Perlis\n\u0026ldquo;Frankly, I don\u0026rsquo;t think you could have driven a needle up my sphincter using a sledgehammer.\u0026rdquo; ‚Äì Col. Barry Horne, F-117 pilot, on first mission over Baghdad\n\u0026ldquo;Two centuries later a most clear-sighted historian of the Second Crusade can find space in a short narrative to record on many occasions the flattery, perjury, perfidy, blasphemy, heresy, arrogance, servility, deceit, pride, cunning and infidelity of the Greeks.\u0026rdquo; ‚Äì R. W. Southern,¬†The Making of the Middle Ages\n\u0026ldquo;The imagination of nature is far, far greater than the imagination of man.\u0026rdquo; ‚Äì Richard Feynman\n\u0026ldquo;The greatest dangers to liberty lurk in insidious encroachment by men of zeal, well-meaning but without understanding.\u0026rdquo; ‚Äì Brandeis\n\u0026ldquo;People who read¬†Cosmopolitan¬†magazine are very different from those who do not.\u0026rdquo; ‚Äì Donald Berry,¬†Statistics: A Bayesian Perspective\n\u0026ldquo;The art of handling university students is to make oneself appear, and this almost ostentatiously, to be treating them as adults\u0026hellip;.\u0026rdquo; ‚Äì Arnold Toynbee,¬†Experiences\n\u0026ldquo;Americans spend an average of four hours a day watching TV, an hour of that enduring ads. That adds up to an astounding 10% of total leisure time; at current rates, a typical viewer fritters away three years of his life getting bombarded with commercials.\u0026rdquo; ‚Äì Scott Woolley,¬†Forbes\n\u0026ldquo;The best writing is rewriting.\u0026rdquo; ‚Äì E. B. White\n\u0026ldquo;Modern invention has been a great leveller. A machine may operate far more quickly than a political or economic measure to abolish privilege and wipe out the distinctions of class or finance.\u0026rdquo; ‚Äì Ivor Brown,¬†The Heart of England\n\u0026ldquo;If our goal is to write poetry, the only way we are likely to be¬†any¬†good is to try to be as great as the best.\u0026rdquo; ‚Äì Donald Hall,¬†Poetry and Ambition\n\u0026ldquo;I am annoyed to find myself continually described by people whom I have never set eyes on as bad-tempered.\u0026rdquo; ‚Äì Evelyn Waugh, Diary (26 Dec 47)\n\u0026ldquo;Premature optimization is the root of all evil (or at least most of it) in programming.\u0026rdquo; ‚Äì Donald Knuth\n\u0026ldquo;In France those absurd perversions of the art of war which covered themselves under the name of chivalry were more omnipotent than in any other country of Europe. The strength of the armies of Philip and John of Valois was composed of a fiery and undisciplined aristocracy which imagined itself to be the most efficient military force in the world, but which was in reality little removed from an armed mob.\u0026rdquo; ‚Äì C. W. C. Oman,¬†The Art of War in the Middle Ages\n\u0026ldquo;The public should always be wondering how it is possible to give so much for the money.\u0026rdquo; ‚Äì Henry Ford\n\u0026ldquo;None ever wished it longer than it is.\u0026rdquo; ‚Äì Johnson on¬†Paradise Lost\n\u0026ldquo;Many large and high class greengrocers of my acquaintance have never heard of the Golden Wonder potato.\u0026rdquo; ‚Äì Roy Genders,¬†Vegetables for the Epicure\nThe best lack all conviction, while the worst Are full of passionate intensity. ‚Äì Yeats,¬†The Second Coming\n\u0026ldquo;We act as though comfort and luxury were the chief requirements of life, when all that we need to make us happy is something to be enthusiastic about.\u0026rdquo; ‚Äì Charles Kingsley\n\u0026ldquo;The path from good to evil goes through bogus.\u0026rdquo; ‚Äì Tara Ploughman\n\u0026ldquo;Lisp has jokingly been called \u0026ldquo;the most intelligent way to misuse a computer\u0026rdquo;. I think that description is a great compliment because it transmits the full flavor of liberation: it has assisted a number of our most gifted fellow humans in thinking previously impossible thoughts.\u0026rdquo; ‚Äì Edsger Dijkstra,¬†CACM,¬†15:10\n\u0026ldquo;Many who burnt heretics in the ordinary way of their business were otherwise excellent people.\u0026rdquo; ‚Äì G. M. Trevelyan, \u0026ldquo;Bias in History\u0026rdquo;\n\u0026ldquo;He became an object of ridicule in 1993 when a paper published an intercepted phone call in which he told his lover Camilla Parker Bowles he wanted to be reincarnated as her tampon.\u0026rdquo; ‚Äì Reuters story, on Prince Charles\n\u0026ldquo;We\u0026rsquo;re even wrong about which mistakes we\u0026rsquo;re making.\u0026rdquo; ‚Äì Carl Winfeld\n\u0026ldquo;From this place she sent into the world those novels, which by many have been placed on the same shelf as the works of a D\u0026rsquo;Arblay and an Edgeworth.\u0026rdquo; ‚Äì Henry Austen on his sister Jane, in a preface to¬†Persuasion\n\u0026ldquo;The key to performance is elegance, not battalions of special cases.\u0026rdquo; ‚Äì Jon Bentley and Doug McIlroy\n\u0026ldquo;The economic depression that struck Europe in the fourteenth century was followed ultimately by economic and technological recovery. But the depression we have moved into will have no end. We can anticipate centuries of decline and exhaustion.\u0026rdquo; ‚Äì Jean Gimpel,¬†The Medieval Machine,¬†1975\n\u0026ldquo;Politics is not the art of the possible. It consists in choosing between the disastrous and the unpalatable.\u0026rdquo; ‚Äì J. K. Galbraith, Letter to Kennedy, 1962\n\u0026ldquo;A typical dinner from the ape menu would be tofu bake with ratatouille of aubergine, onions and sweet peppers, with pearled barley and vegetable side dishes, says the team.\u0026rdquo; ‚Äì¬†New Scientist\n\u0026ldquo;Guy Steele leads a small team of researchers in Burlington, Massachusetts, who are taking on an¬†enormous challenge¬†‚Äî create a programming language better than Java.\u0026rdquo; ‚Äì Sun.Com (my italics)\n\u0026ldquo;I had my own reactions to Paul\u0026rsquo;s essay ‚Äî on the whole I liked it but when I connected some dots I found some suggestions of things I strongly disliked ‚Äî not so much in the essay as suggested by it.\u0026rdquo; ‚Äì reaction to¬†What You Can\u0026rsquo;t Say¬†in a blog\n\u0026ldquo;Your twenties are always an apprenticeship, but you don\u0026rsquo;t always know what for.\u0026rdquo; ‚Äì Jan Houtema\n\u0026ldquo;In addition, the board rewrote the definition of science, so that it is no longer limited to the search for natural explanations of phenomena.\u0026rdquo; ‚Äì AP story on Kansas Board of Education\n\u0026ldquo;A danger sign that fellow-obsessionals will at once recognize is the tendency to regard the happiest moments of your life as those that occur when someone who has an appointment to see you is prevented from coming.\u0026rdquo; ‚Äì Peter Medawar,¬†Memoirs of a Thinking Radish\n\u0026ldquo;Never offer what you\u0026rsquo;d hate someone for accepting.\u0026rdquo; ‚Äì Tara Ploughman\n\u0026ldquo;The pagans were incensed at the rashness of a recent and obscure sect, which presumed to accuse their countrymen of error, and to devote their ancestors to eternal misery.\u0026rdquo; ‚Äì Gibbon,¬†The Decline and Fall of the Roman Empire\n\u0026ldquo;Simultaneously reifying and challenging hegemonic codes of race, class, gender and regional or national identity, his characters explore the complex and changing postmodern cultural landscape.\u0026rdquo; ‚Äì Robert Bennett, English professor at Montana State, announcing a panel discussion about Brad Pitt\n\u0026ldquo;In the councils of government, we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the military-industrial complex. The potential for the disastrous rise of misplaced power exists and will persist.\u0026rdquo; ‚Äì Eisenhower, Farewell Address\n\u0026ldquo;Keep away from people who try to belittle your ambitions. Small people always do that, but the really great make you feel that you, too, can become great.\u0026rdquo; ‚Äì Mark Twain\n\u0026ldquo;However little television you watch, watch less.\u0026rdquo; ‚Äì David McCullough\n\u0026ldquo;Any word you have to hunt for in a thesaurus is the wrong word.\u0026rdquo; ‚Äì Stephen King\n\u0026ldquo;The people can always be brought to the bidding of the leaders. That is easy. All you have to do is tell them they are being attacked, and denounce the pacifists for lack of patriotism, and exposing the country to greater danger.\u0026rdquo; ‚Äì Goering at the Nuremberg Trials\n\u0026ldquo;As all these results were obtained, not by any heroic method, but by patient and detailed reasoning, I began to think it probable that philosophy had erred in adopting heroic remedies for intellectual difficulties, and that solutions were to be found merely by greater care and accuracy. This view I have come to hold more and more strongly as time went on, and it has led me to doubt whether philosophy, as a study distinct from science and possessed of a method of its own, is anything more than an unfortunate legacy from theology.\u0026rdquo; ‚Äì Bertrand Russell, \u0026ldquo;Logical Atomism\u0026rdquo;\n\u0026ldquo;Get the important things right.\u0026rdquo; ‚Äì N. P. Calderwood\n\u0026ldquo;The power of instruction is seldom of much efficacy except in those happy dispositions where it is almost superfluous.\u0026rdquo; ‚Äì Gibbon\n\u0026ldquo;I do not know what the Lord\u0026rsquo;s anointed, his Vicegerent upon earth, divinely appointed by him, and accountable to none but him for his actions, will either think or do, upon these symptoms of reason and good sense which seem to be breaking out all over France: but this I foresee, that, before the end of this century, the trade of both King and Priest will not be half so good a one as it has been.\u0026rdquo; ‚Äì Chesterfield, letter to his son, 13 Apr 1752\n\u0026ldquo;filter(P, S) is almost always written clearer as [x for x in S if P(x)]\u0026rdquo; ‚Äì Guido van Rossum on Python\n\u0026ldquo;I\u0026rsquo;m surrounded by postmodern idiots and blatherers. Your writings give me hope.\u0026rdquo; ‚Äì email from a reader\n\u0026ldquo;In the last analysis, productivity of labour is the most important, the principal thing for the victory of the new social system.\u0026rdquo; ‚Äì Lenin, quoted in¬†First Five-Year Plan for the Development of the National Economy of the People\u0026rsquo;s Republic of China in 1953-1957\n\u0026ldquo;Most interesting phenomena have multiple causes.\u0026rdquo; ‚Äì N. P. Calderwood\n\u0026ldquo;From 1911 to 1920, the mood of the city varied between utter dullness and tremendous excitement.\u0026rdquo; ‚Äì Arthur Coffman,¬†An Illustrated History of Palo Alto\n\u0026ldquo;A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\u0026rdquo; ‚Äì Max Planck\n\u0026ldquo;No man who ever held the office of President would congratulate a friend on obtaining it.\u0026rdquo; ‚Äì John Adams\n\u0026ldquo;PowerPoint makes us stupid.\u0026rdquo; ‚Äì General James N. Mattis, USMC\n\u0026ldquo;The best way to do something \u0026rsquo;lean\u0026rsquo; is to gather a tight group of people, give them very little money, and very little time.\u0026rdquo; ‚Äì Bob Klein, chief engineer of the F-14 program\n\u0026ldquo;But camels, though odious to view and endowed with the offensive spirit, did not enjoy the blessing of pachydermaty.\u0026rdquo; ‚Äì F. E. Adcock,¬†The Greek and Macedonian Art of War\n\u0026ldquo;As it turned out, the obvious clearly stated, and combined with new observations, was sometimes close to revolutionary.\u0026rdquo; ‚Äì Wallace Stegner on John Wesley Powell\n\u0026ldquo;Focusing is about saying no.\u0026rdquo; ‚Äì Steve Jobs\n\u0026ldquo;The very word \u0026ldquo;secrecy\u0026rdquo; is repugnant in a free and open society; and we are as a people inherently and historically opposed to secret societies, to secret oaths and to secret proceedings. We decided long ago that the dangers of excessive and unwarranted concealment of pertinent facts far outweighed the dangers which are cited to justify it. Even today, there is little value in opposing the threat of a closed society by imitating its arbitrary restrictions. Even today, there is little value in insuring the survival of our nation if our traditions do not survive with it.\u0026rdquo; ‚Äì John F. Kennedy\n\u0026ldquo;In the Zenith Color chassis there are no printed circuits, no production shortcuts. Every connection is carefully hand-wired with the same exacting care that makes Zenith America\u0026rsquo;s largest selling TV, and your best Color TV buy.\u0026rdquo; ‚Äì Zenith TV ad, 1964\n\u0026ldquo;Semper aut discere, aut docere, aut scribere dulce habui.\u0026rdquo; ‚Äì Bede\n\u0026ldquo;The qualities that made for success in a fighter-pilot seemed to be just those sturdy qualities that made for success in other professions; observation, initiative, determination, courage, including the courage to run away. In course of time it appeared that men who had a private axe to grind beyond the public axe of the King\u0026rsquo;s enemies were especially successful.\u0026rdquo; ‚Äì Jim Bailey,¬†The Sky Suspended\n\u0026ldquo;Everything about it was visibly mis-shapen, corrupt, crawling, verminous; for a time I could not bear to look at it, and passed with averted eyes; recovering from this weakness, I forced myself to look, and to face day by day the question: a thing so obviously, so incontrovertibly, so indefensibly bad, why had Scott done it?\u0026rdquo; ‚Äì R. G. Collingwood on the Albert Memorial\n\u0026ldquo;Anybody who cares less about wanting to be cool, I think, is more interesting.\u0026rdquo; ‚Äì Aimee Mann\n\u0026ldquo;The late Richard Feynman, a superb physicist, said once as we talked about the laser that the way to tell a great idea is that, when people hear it, they say, \u0026lsquo;Gee, I could have thought of that.\u0026rsquo;\u0026rdquo; ‚Äì Charles Townes,¬†How the Laser Happened\n\u0026ldquo;If we chose always to be wise we should rarely need to be virtuous. But inclinations which we could easily overcome irresistibly attract us. We give in to slight temptations and minimize the danger. We fall insensibly into dangerous situations, from which we could easily have safeguarded ourselves, but from which we cannot withdraw without heroic efforts which appal us. So finally, as we tumble into the abyss, we ask God why he has made us so feeble. But, in spite of ourselves, He replies through our consciences: \u0026lsquo;I have made you too feeble to climb out of the pit, because I made you strong enough not to fall in.\u0026rsquo;\u0026rdquo; ‚Äì Rousseau,¬†Confessions, Cohen trans.\n\u0026ldquo;Constantly fortified with the new blood of immigrants who saw America as a place where anything was possible, the nation had adopted an ethos that elevated problem solving to the status of religion.\u0026rdquo; ‚Äì John Hoyt Williams,¬†A Great and Shining Road\n\u0026ldquo;That 95 per cent. fail of those who start in business upon their own account seems incredible, and yet such are said to be the statistics upon the subject.\u0026rdquo; ‚Äì Andrew Carnegie, 1896\n\u0026ldquo;You should never turn a man\u0026rsquo;s generosity as a sword against him. Any virtue that a man has, even if he has many vices, should not be used as a tool against him.\u0026rdquo; ‚Äì Rabi to Feynman\n\u0026ldquo;You can\u0026rsquo;t say I didn\u0026rsquo;t try really hard, \u0026lsquo;cause I\u0026rsquo;m trying really hard to be good.\u0026rdquo; ‚Äì Tom Petty\n\u0026ldquo;He begins working calculus problems in his head as soon as he awakens. He did calculus while driving in his car, while sitting in the living room, and while lying in bed at night.\u0026rdquo; ‚Äì divorce complaint of Richard Feynman\u0026rsquo;s second wife\n\u0026ldquo;The development of this aircraft was long and complex even by Soviet standards, partially explained by the fact that its entire design team was in jail.\u0026rdquo; ‚Äì Howard Moon on the TU-2\n\u0026ldquo;It is said that there is a technical term for people who believe that little boys and little girls are born indistinguishable and are molded into their natures by parental socialization. The term is \u0026lsquo;childless.\u0026rsquo;\u0026rdquo; ‚Äì Steven Pinker\n\u0026ldquo;When you have eliminated the impossible, whatever remains, however improbable, must be the truth.\u0026rdquo; ‚Äì Sherlock Holmes\nHis notions fitted things so well, That which was which he could not tell;¬†But oftentimes mistook th\u0026rsquo; one For th\u0026rsquo; other, as great clerks have done. ‚Äì Samuel Butler,¬†Hudibras\n\u0026ldquo;A complex system that works is invariably found to have evolved from a simple system that worked. The inverse proposition also appears to be true: A complex system designed from scratch never works and cannot be made to work.\u0026rdquo; ‚Äì John Gall\n\u0026ldquo;The Muslims of al-Andalus had nothing to learn from their Christian neighbours and were incurious about them. Geographers\u0026rsquo; accounts of Christian Spain tended to be cursory in the extreme: it was cold, the inhabitants were barbarians who ate pigs, you could get slaves there ‚Äî that was about the sum of it.\u0026rdquo; ‚Äì Richard Fletcher,¬†Moorish Spain\n\u0026ldquo;When the enemy is making a false movement we must take good care not to interrupt him.\u0026rdquo; ‚Äì Napoleon\n\u0026ldquo;Leave me alone, I\u0026rsquo;m trying to build! Don\u0026rsquo;t you know how people build? By concentrating!\u0026rdquo; ‚Äì my 5 year old son, building Lego\n\u0026ldquo;An expert is a man who has made all the mistakes which can be made in a very narrow field.\u0026rdquo; ‚Äì Niels Bohr\n\u0026ldquo;Don\u0026rsquo;t fear moving slowly. Fear standing still.\u0026rdquo; ‚Äì Chinese proverb\n\u0026ldquo;I think a life properly lived is just learn, learn, learn all the time.\u0026rdquo; ‚Äì Charlie Munger\n\u0026ldquo;The handicap under which most beginning writers struggle is that they don\u0026rsquo;t know how to write.\u0026rdquo; ‚Äì Wodehouse\n\u0026ldquo;Then in 1888 came the publication of Denton\u0026rsquo;s history of England in the fifteenth century. This was a masterpiece of egregious perversity, depicting the later Middle Ages as a culminating period of ruinous taxation, iniquitous labour laws, demoralizing pestilences, and lavish dissipation of national resources upon violent and embittered domestic feuds and futile and indecisive foreign wars. The credulity of even the most undiscriminating reader was taxed to a degree by a portrayal in which horror succeeded upon cataclysm, in a thickening atmosphere of crime and terror, want, degradation, and wretchedness.\u0026rdquo; ‚Äì A. R. Bridbury\n\u0026ldquo;We are all agreed that your theory is crazy. The question that divides us is whether it is crazy enough to have a chance of being correct.\u0026rdquo; ‚Äì Neils Bohr\n\u0026ldquo;Pessimists sound smart. Optimists make money.\u0026rdquo; ‚Äì Nat Friedman\n\u0026ldquo;Immature poets imitate; mature poets steal; bad poets deface what they take, and good poets make it into something better, or at least something different.\u0026rdquo; ‚Äì T. S. Eliot\n\u0026ldquo;The most exciting phrase to hear in science, the one that heralds new discoveries, is not \u0026lsquo;Eureka\u0026rsquo; but \u0026lsquo;That\u0026rsquo;s funny\u0026hellip;\u0026rsquo; ‚Äì Isaac Asimov\n\u0026ldquo;We always want to make sure that our machines haven‚Äôt by accident learned something that isn‚Äôt what we intended.\u0026rdquo; ‚Äì Susan Wojcicki\n\u0026ldquo;The population is made up of four types of people: A small number hunt witches. A large number go along with the hunt. A larger number are silent. A tiny number oppose it. The final group ‚Äî as if by magic ‚Äî become witches.\u0026rdquo; ‚Äì Bret Weinstein\n\u0026ldquo;People who can\u0026rsquo;t laugh at themselves will always be outwitted by people who can.\u0026rdquo; ‚Äì T. K. Coleman\n\u0026ldquo;Political correctness is America\u0026rsquo;s newest form of intolerance, and it is especially pernicious because it comes disguised as tolerance. It presents itself as fairness, yet attempts to restrict and control people\u0026rsquo;s language with strict codes and rigid rules.\u0026rdquo; ‚Äì George Carlin\n","href":"/quotes/","title":"Quotes"},{"content":" My name is Prasenjit Manna. This is my personal blog. Over the years I learnt and grew my knowledge. Thanks to everyone whoever have helped me on this journey. This is my new initiative to give back to the society.\nI drink tea, coffee, play with my son, and read books. I research and use tools for that improve my thinking, productivity, and life in general. I strive to reduce friction in my life, keep things minimal, and simplify. Recently I started writing technical blogs.\nMy Bio: I am a Sr. Technical Lleader at Cisco Systems, Bangalore, India. My background is in software development, specially in embedded routing \u0026amp; optical systems. I have Bachelor‚Äôs degree in Computer Science from Haldia Institute of Technology, Vidyasagar University and Master‚Äôs degree from Birla Institute Technology, Pilani. I have prior experiences at startups like NetDevices (acquired by Alcatel-Lucent(now Nokia)), FutureSoft (now Aricent/Altran), Alumnus Software Ltd.\n","href":"/about/","title":"about"},{"content":"It\u0026rsquo;s been long time, I was keep thinking to write blog. But, always getting delayed due to fear, whether I am good at writing, or people will like it or not, etc. Finally, I overcome this fear and started writing from new year, 2020.\nJust to give little background about me, myself, Prasenjit Manna, working with networking industry as programmer, developer, architect, influencer since 2000. My area of interest is always routing, switching, manageability, optical, etc. Over the last two decades, lots of market dynamic has changed for engineers, and for the companies also. As a result, I have gained vast knowledge in different fields.\nMy interest is to write blog on technical knowledge, specially, software aspect of networking devices. Few of my initial writes will be on SDN, network manageability.\nJoin my email list to get direct access to new blog posts.\n","href":"/blogs/2020-01-01-first-post/","title":"My First Blog Post"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"}]
